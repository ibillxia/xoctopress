<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2014-01-03T23:45:33+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[如果长颈鹿哭了，它会不会要哽咽好久]]></title>
    <link href="http://ibillxia.github.io/blog/2013/12/24/if-giraffe-cried-would-it-choking-so-long/"/>
    <updated>2013-12-24T19:29:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/12/24/if-giraffe-cried-would-it-choking-so-long</id>
    <content type="html"><![CDATA[<p>一篇很唯美的文章，来自豆瓣相册：<a href="http://www.douban.com/photos/album/63673280/?start=0">如果长颈鹿哭了，它会不会要哽咽好久</a>，有微小删改。<br/>
在酷我调频的莫萱日记“<a href="http://www.kuwo.cn/yinyue/3619660/">关于我爱你这件小事</a>”这一期中听到的，分享给大家，顺祝大家圣诞快乐！</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122401.jpg"></center>


<p>长颈鹿的脖子那么长 哽咽的时候是不是很难受<br/>
我没有长长的脖子 却哽咽的说不出话<br/>
章鱼有三颗心脏 心痛的时候是不是很疼<br/>
我没有三颗心脏 体会不到无法忍受的痛再多三倍<br/>
乌鸦可以学人说话 尴尬的时候会不会装咳嗽<br/>
我假装咳嗽 假装被沙子迷了眼 你也没有看我一眼<br/>
骆驼有长长的睫毛 想哭的时候能不能说眼睛进了沙<br/>
我假装咳嗽 假装被沙子迷了眼 你也没有看我一眼<br/>
蛇没有宽宽的肩膀 她累的时候给不了能够依靠的温暖<br/>
是因为我太弱小 没有很可靠的肩膀么<br/>
小强有两个大脑 孤单的时候会不会一起想着谁<br/>
无时无刻的清澈的想念 一定比两个大脑一起想你还多吧<br/>
蜉蝣只能活很短 可能一辈子都来不及和心里珍藏的那个人说一些想说的话<br/>
我又能活多久 时间会不会给我可以开口的勇气<br/>
把人生看做是自己独一无二的创作 便不会频频回首<br/>
你会不会忽然地出现 在旧时光的风景里 成为珍藏一生的美丽</p>

<!--more-->




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122402.jpg"></center>


<p>夏天过去　你没有消息<br/>
说会再见终究我相信了你<br/>
秋天来临，我想等下去<br/>
你最爱的叶子花语是关心<br/>
咖啡色的鞋子 灰色毛衣 从以前到现在的玩具<br/>
你的表情 太没志气　说我很想你<br/>
把握太轻，却让人沉重地无法逃离<br/>
我只是觉得 这种牵挂放着无趣丢了可惜</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122403.jpg"></center>


<p>两个人一起是为了快乐，分手是为了减轻痛苦<br/>
你无法再令我快乐，我也唯有离开<br/>
我离开的时候，也很痛苦<br/>
只是，你肯定比我痛苦<br/>
因为我首先说再见，首先追求快乐的是我。<br/>
爱上一个人的时候，总会有点害怕<br/>
怕得到他，怕失掉他<br/>
有时候，我们愿意原谅一个人<br/>
并不是我们真的愿意原谅他<br/>
而是我们不想失去他<br/>
不想失去他，唯有假装原谅他<br/>
恨，也是一种爱<br/>
岁月漫长，装着装着也许真的就能够原谅与遗忘。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122404.jpg"></center>


<p>所有单向的爱情，是不是终归也会有悄然落幕的一天？<br/>
所有寂寞的影子，是不是终究走不出黑夜之后的黎明？<br/>
我爱你，可以跟你无关<br/>
可是，我希望它是跟你有关的。<br/>
单恋也是爱情吧？<br/>
只是，这种爱情与人无尤<br/>
爱你是我一个人的事<br/>
我自己回答自己，自己荣耀自己，自己呼应自己<br/>
在我悄悄单恋着的那个人身后，我多么像个寂寞的影子？<br/>
时而甜蜜，时而苦涩<br/>
那不是懦弱，那只是不被允许的爱情<br/>
于是，我只能选择沉默。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122405.jpg"></center>


<p>一段爱情，两个人成长<br/>
无论是否能够和你终老<br/>
那么投入地爱过彼此<br/>
流过那么多的眼泪，我们都长大了。<br/>
因为曾经那样喜欢一个人，<br/>
所以无法接受自己稍微不喜欢的人。</p>

<p>为什么要急着长大呢？正如你不会急着老去<br/>
总有一天，你会长大<br/>
要是可以一直不长大<br/>
不需要面对人生汹涌的波涛<br/>
不需要把棱角磨平<br/>
也是一种幸福啊！</p>

<p>我以为爱情可以克服一切<br/>
谁知道它有时毫无力量<br/>
我以为爱情可以填满人生的遗憾<br/>
然而，制造更多遗憾的，偏偏是爱情本身<br/>
阴晴圆缺，在一段爱情里不断重演<br/>
换一个人，也不会天色常蓝。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122406.jpg"></center>


<p>既然未曾拥有，又怎知道它有多好？<br/>
因为错过了<br/>
所以，当我们觉得不幸福的时候<br/>
总会为那段错过了的感情和那个错过了的人加入许多幻想和诗意<br/>
念念不忘的，不过是自己想象的画面。</p>

<p>那个长夜，漫天星宿<br/>
得睹芳容，魂摧魄折<br/>
想认识你，想爱你，想守护你<br/>
换几声欢笑，一场热泪，告别飘摇无根的生活<br/>
我不是暗影，我是归人<br/>
我，终究是爱你的</p>

<p>曾经，你苦苦以为<br/>
没有了这个人，也就活不成了<br/>
到了后来，不是活得好好的吗？<br/>
一个不爱你的人，决不会比你的生命重要<br/>
一个爱你的人，会告诉你，你的生命比你对他的爱情重要。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122407.jpg"></center>


<p>&#8220;你问我喜欢那声音不───若是现在，我一定说喜欢了。&#8221;<br/>
你说过这是你高中时最爱的新诗<br/>
岁暮总是无故想起，无由来地感动<br/>
又到岁暮了，我想把它送你<br/>
尘世的声音很多<br/>
亲人的、爱人的、知己的、幸福的、悲伤的<br/>
到后来又有哪些永留心中却或许有一天再也听不到了？<br/>
你的声音是我的天籁，会一直在我心头。</p>

<p>假如想念是一张地图<br/>
打开想念的地图<br/>
你也许会发现<br/>
不管这张地图有多么复杂<br/>
想念的起点也是终点<br/>
小孩子想念的是亲人<br/>
长大了，想念的是恋人<br/>
后来的后来，想念的对象又变成了亲人<br/>
人生最初和最后的想念几乎是一样的</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122408.jpg"></center>


<p>对男人来说<br/>
旧情人所生的女儿都是漂亮的<br/>
因为她长得像她妈妈<br/>
旧情人所生的儿子却很难看，因为他长得像他爸爸。──《想念》</p>

<p>每个人都带着童年故事和许多过去去爱上一个人<br/>
被渴求、被迁就、被照顾、被荣耀<br/>
也许都是一个过程<br/>
千回百转，直到不年轻了<br/>
才终于学会爱<br/>
却又不一定能遇到可以相爱的人<br/>
遇到了又是否可以厮守呢？无常世间没有圆满；</p>

<p>可是，幸运的话<br/>
会遇到你说的那个人<br/>
他完整了我<br/>
也是他让我看到我所有的缺失。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122409.jpg"></center>


<p>两个人最初走在一起的时候<br/>
对方为自己做一件很小的事情<br/>
我们也会感动<br/>
后来，他要做更多事情，我们才会感动<br/>
再后来，他要付出更多更多，我们才肯感动<br/>
人是多么贪婪的动物？<br/>
无论我们一生撒过多少谎<br/>
自欺终究比欺人的时候多<br/>
我对你说谎的时候<br/>
眼睛也许微笑颤抖，害怕你看出来<br/>
我对自己说谎的时候<br/>
却连眉头也不会皱一下<br/>
要面对人生种种痛苦和真相<br/>
委实太残忍了，我难道不能对自己说谎吗？<br/>
可惜，人有时还是骗不了自己<br/>
终于，我不得不苦涩地承认<br/>
你已经不爱我了。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122410.jpg"></center>


<p>他不是不爱你，也不是对你不好<br/>
他只是更爱自己，永远把自己放在第一位<br/>
他可爱的时候很可爱，自私的时候却又很自私<br/>
你恨透他的自私，心里却知道他改不了<br/>
他就是这样的一个人，天性如此<br/>
爱上一个自私的人，这样的爱是孤单还是无奈？<br/>
你终于明白，自私的人是比较快乐的<br/>
要是可以自私，那该多好。<br/>
所有处在恋爱年龄的女孩子，总是分成两派：<br/>
一派说，爱对方多一点，是幸福的；<br/>
另一派说，对方爱我多一点，才是幸福的<br/>
也许，我们都错了<br/>
爱的形式与分量从来不是设定在我们心里<br/>
你遇到一个怎样的男人<br/>
你便会谈一段怎样的恋爱。<br/>
希望有个人，在我嘴里说没事的时候，看出我不是真的没事<br/>
有个人，在我强颜欢笑的时候，知道我不是真的开心<br/>
也是这个人，在我拚了命憋住眼泪的时候<br/>
偏偏挨过来摸摸我的头，对我说：&#8221; 别哭，别哭，&#8221; <br/>
结果害我把脸埋在他身上稀哩哗啦哭得更惨，却也不会生他的气。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122411.jpg"></center>


<p>爱情是什么？不就是有个人<br/>
有些失望是不可避免的<br/>
但大部分的失望，都是因为你高估了自己<br/>
又或者是高估了你爱的那个人<br/>
无论高估的是谁，终也难免失望。<br/>
梦想有时候多么像爱情？<br/>
假若不是痴心相信，是无法坚持到底的<br/>
只是，失落了的梦想也像失落了的爱情<br/>
始于如此的兴奋与渴望，又终于如此的挫败与荒凉。<br/>
时间似乎一直在我们眼前流逝<br/>
它像无情的小鸟，拍翅高飞，永不回首<br/>
而其实，我们才是那只小小鸟，短暂勾留，倏忽飞逝<br/>
时间亘古长存，它一直都在那儿，从未飞渡<br/>
惟有我们自己与所爱的人，终会成为过去<br/>
所有的相依与相聚、所有的阴晴圆缺，也如飞似逝，永不复还。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122412.jpg"></center>


<p>爱情总是想象比现实美丽<br/>
相逢如是，告别亦如是<br/>
我们以为爱得很深很深<br/>
来日岁月，会让你知道，它不过很浅很浅<br/>
最深最重的爱，必须和时日一起成长。<br/>
“我不要你管！” <br/>
女人的这句话好像总是冲她最在乎的那个人说，<br/>
也总是言不由衷<br/>
她真正的意思也许是：<br/>
“要是你不爱我，你就别管我！”</p>

<p>想和你一起变老，<br/>
老得地老天荒，<br/>
老得这世界把我俩都忘掉了，<br/>
惟独你记得我年轻的容颜。<br/>
当所爱的人骤然离世，<br/>
那份伤痛是不会离去的，<br/>
它会沉淀、会改变你。<br/>
伤痛过后，<br/>
他会想你坚强地活下去，<br/>
想你幸福地活着。<br/>
因为他知道，<br/>
他也活在你心里，<br/>
成了你的一部分，<br/>
是你的血肉，你活得好，他才会好。<br/>
生于斯世，我们不都是旅人吗？<br/>
他不会回来了；但是，你终归会回去。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122413.jpg"></center>


<p>樱花如风转眼去，惟有拜拜是人生<br/>
曾经以为，拥有是不容易的；后来才知道，舍弃更难。<br/>
有些人，你不敢接近他，可能是因为你爱他，你害怕被拒绝，害怕受伤害<br/>
有些人，你不想接近他，是真的不想<br/>
说不上讨厌这个人，倒是很想微笑对他说：<br/>
“你走开吧你！麻烦你离我远一点。我不是傻的，我也没那么好骗。”<br/>
一天，我们不无惊讶地发现<br/>
过去所有的日子都恍若昨日<br/>
时间永远比我们以为的走得快<br/>
但是，我已经不是昨日那个年少青涩的人了<br/>
我爱过和我爱着的人<br/>
也和我一样，一点点老去<br/>
活着就是要经历这些。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122414.jpg"></center>


<p>童话故事不一定都美好，安徒生有些童话就很悲伤。<br/>
快乐王子只是子虚乌有，阿拉丁神灯不过是天方夜谭。<br/>
我们还要继续相信童话吗？<br/>
童话只属于真正的小孩而不是老小孩<br/>
可是，每个老小孩心中都有一个不朽的童话<br/>
它一直在那儿，遥远而美好，天真却也荒凉，不曾老去，从未被现实消磨。<br/>
你的那个童话又是什么<br/>
一切事物，包括感情，也有它最好的时机。<br/>
某年某天某一刻，你很想对他说：“ 我爱你 ”<br/>
可是，他脸上的神情似乎没有准备好听到这句话<br/>
一瞬间，说到唇边的话消逝了。 <br/>
以后的以后，或许再也不会说了</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122415.jpg"></center>


<p>千回百转，幸福原来一直在自己手里。<br/>
机遇降临时，努力把握；<br/>
缘分来时，好好抱住它。<br/>
再难再苦也要拍拍胸膛迎上去，<br/>
擦干眼泪，我会微笑到最后。<br/>
执子之手，红尘终老。<br/>
卑微的爱无法爱到地老天荒，它总难免会有终结的一天。<br/>
但是，战战兢兢地爱着一个人的时候，<br/>
多么像一只没有自信的丑小鸭？<br/>
害怕自己配不上对方。<br/>
直到一天，羽化成天鹅了，<br/>
也许会怀念丑小鸭的那些傻痴痴的日子<br/>
爱情是一百年的孤独，<br/>
直到遇上那个矢志不渝守护着你的人，<br/>
那一刻，<br/>
所有枯涩的孤独，<br/>
都有了归途</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122416.jpg"></center>


<p>你问：“没有爱情我们到底能不能活得很潇洒？”<br/>
人往往因为爱情而活得很不潇洒。<br/>
当你了解男人都是小孩子，<br/>
你就了解人生所有的事情<br/>
你问：“如何信任一个人呢？在有过失望之后，怎样才能好好去相信别人？”<br/>
我相信真心是会有回报的。<br/>
付出真心去相信一个人，<br/>
即使会受伤害，<br/>
那是我的劫难。<br/>
我也无愧了</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122417.jpg"></center>


<p>年轻就是会高估了爱情的寿命<br/>
年轻就是不懂说不<br/>
年轻就是会为了讨好你爱的那个人而扭曲自己<br/>
年轻就是会错爱<br/>
年轻就是会相信谎言<br/>
年轻就是会天真到笨<br/>
江湖老了每一个曾经青涩的少年，<br/>
江湖也老了每一个爱做梦的少女。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122418.jpg"></center>


<p>爱情终究是两厢情愿的事，你留不住一个不爱你的人；<br/>
你也不会愿意留住一个已经不爱你、只是可怜你的人<br/>
有这样的日子,还没起床,就知道今天会非常想念你；<br/>
也有这样的日子,一直忙到半夜,才惊觉一整天连一秒都没有想过你。<br/>
这都是由不得我的事,<br/>
在清晨并没有人会预报给我听:<br/>
今天到底下哪一种雨,飘哪一朵云。<br/>
这只是我每天要经历的,我一个人的小天气。</p>

<center><img src="http://ibillxia.github.io/images/2013/IMAG2013122419.jpg"></center>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[36氪开放日杭州站回顾与总结]]></title>
    <link href="http://ibillxia.github.io/blog/2013/11/11/36kr-open-day-hangzhou-station-review-and-summary/"/>
    <updated>2013-11-11T19:10:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/11/11/36kr-open-day-hangzhou-station-review-and-summary</id>
    <content type="html"><![CDATA[<p>11月10日，36氪开放日第二次来到杭州站，首次进入大学，进行互联网行业创业和投资的机遇、观点和经验分享。这次开放日主要包含三个部分：</br>
（1）成功创业者/投资者的主题分享；</br>
（2）初创团队的产品发布与展示；</br>
（3）观众参与: 10位观众每人1分钟demo展示。</br>
本文回顾并总结一下这次活动的主要观点。</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013111101.jpg"></center>




<p>主题分享的特邀嘉宾有来自阿里的副总裁纪纲，来自阿米巴的资本合伙人、挖财董事长李治国，来自丁香园的CTO、微信「小道消息」作者冯大辉@Fenng，
毕业于浙大的个推创始人方毅@方毅_个推，来自网易杭研院的产品经理、目前负责易信的章行，来自Nokia的体验创新中心总经理傅蕾。</p>


<p>初创团队的产品展示有7个，分别是——金亦冶-Kivvi收银平板：时尚店主的POS收银机，徐文俊-找大巴：首创BO2O大巴招投标预订直销模式、大巴预订神器，何翱翔-九漫：交互式动态漫画，
袁一伦-要出发旅行：主打周边自驾游，李军-微洽：面向企业的社交化工作平台，胡笃晟-阿姨厨房：为厨艺爱好者提供在家创业、P2P模式的餐饮服务电子商务平台，凌曙-同船渡：位置景点
创建和结伴出游的旅行产品。</p>




<!--more-->




<p>而现场观众的1分钟demo部分，有14 位观众带着他们的产品或者idea来跟大家分享：画吧、快读、37 度、比比、氢学习、Omygod、Tipix—照片编辑App、配配、
微信公众账号杭电助手、车纷享、穿戴式智能设备、享赢棋牌联盟、爱扫货、微博书等。</p>




<h2>特邀嘉宾的主题分享</h2>


<p>主题分享有6个，在正式分享开始之前，『36氪联合创始人·主编（第二帅[偷笑]）王壮@truant 分享 1.创业公司的理想和坚持：传递正能量 ，帮助创业者，“我们是不收车马费的媒体人”；
2.大学生互联网行业的就业：一要开阔视野，培养兴趣点；二要有展现自己能力的作品，掌握独特技能。』（<a href="http://e.weibo.com/2863376813/Ai52PFTkN">微博</a>）感到比较吃惊的
是36kr的创业团队都是88后的，而且能够做得如此之好。分享的第二点也很有启发意义，既要有开阔的视野又要掌握独特技能，寻找自己的兴趣和特长、培养独特的技能是重中之重。</p>




<p>第一个主题分享是阿里副总裁纪纲的分享。『嘉宾分享之 阿里巴巴集团副总裁纪纲：首先是对战略投资的理解——公司业务先遣队，而VC和创业者关系也同样很重要。他强调了生态圈的概念：互联网是
生机勃勃的生态；纪纲谈到值得关注的两个方向：垂直性入口、软硬件结合。初创团队基因很重要，更要关注解决问题和寻找机会的平衡。』（<a href="http://e.weibo.com/2863376813/Ai5a3DfMh">微博</a>）
我个人也总结了3点：『1.阿里的收购观点分享；2.移动互联网创业潮已过，但真正的浪潮还没到来；3.移动互联网的机会还很多，垂直入口还有希望。』（<a href="http://weibo.com/2704795533/Ai5cz2iCF">微博</a>）
对于创业者而言，垂直性入口和软硬件结合这两个方向确实还有很多机遇。</p>




<p>第二个主题分享来自阿米巴的资本合伙人李治国。『嘉宾分享之 阿米巴资本合伙人、挖财董事长李治国：投资和融资经验。投资不仅要有眼光，而且要有运气。当今时代，投资的心态也需转变。
被投资的团队要有承载资金的能力，双方需判断方向和坚持方向；有些时候做事情只要赌对了方向，刚性需求+好的团队就等于成功的一半。』（<a href="http://e.weibo.com/2863376813/Ai5iLvRkg">微博</a>）
主要是投资方面的观点分享：『判断和经验很重要，坚持也很重要！』（<a href="http://weibo.com/2704795533/Ai5f8kdb2">微博</a>），『赌对方向+nb团队。投过@蘑菇街 和@快的打车』
（<a href="http://weibo.com/2704795533/Ai5ixheZx">微博</a>）。</p>




<p>第三个主题分享来自个推的 @方毅_个推。『嘉宾分享之 个推创始总经理方毅：要变，还得是绝活——如何在业务迁移中保持竞争力？快到极致，爽到极致，“帅”到极致。[威武]』
（<a href="http://e.weibo.com/2863376813/Ai5Jqdcff">微博</a>）。@北隐_BEIYIN的这个总结也不错：『方总@方毅_个推 在做分享“要变还得有绝活”，满口的“苦逼”“屌丝”
“让你爽到极致”“生理反应”“就要给你想要的”…哈哈哈哈[哈哈][哈哈] 不过话糙理直 ①学会在夹缝中生存发展 ②能够把握住深层次的需求痛点 “我们就是为了方总来的” [围观][围观]』
（<a href="http://weibo.com/3410997984/Ai5JJ4AfO">微博</a>）上次在一个创业圆桌分享会上见识过了方总，这次方师兄从自己苦逼的创业历程讲如何在业务迁移中保持竞争力，
要变，还要是绝活！给挣扎在创业道路上同样苦逼奋斗的年轻创业者们一个很好的警示。
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013111102.jpg"></center>
</p>




<p>第四个分享来自Nokia的体验创新中心总经理傅蕾。『嘉宾分享之 诺基亚体验创新中心总经理傅蕾从NEIC的愿景、职能、工作模式、资源、最新进展和动态六个方面讲述
了NEIC如何帮助创业者、开发者开发Windows Phone 平台APP，提出了“3000+300+30”的目标，和“体验创新-扶持-成长-产出”的工作模式。』（<a href="http://e.weibo.com/2863376813/Ai5UHkUO8">微博</a>）。
主要介绍了Nokia的体验创新中心的一些目标和愿望，这种亡羊补牢，为时晚矣？ 还是为时未晚？拭目以待~
</p>




<p>第五个分享来自网易杭研院的产品经理、目前负责易信的章行。『嘉宾分享之 网易杭州研究院产品经理章行：移动IM社交思考。什么是社交？社交的本质——人与人的“资源”交换。
章行认为，中国人没社交，只有消遣，是移动网络降低了交换成本；而在IM下的SNS永远只是补充。社交产品的核心任务是建立健康长久的用户关系链；团队需要取得多方面平衡。[害羞]』
（<a href="http://e.weibo.com/2863376813/Ai6oppHi1">微博</a>）。很有理性很有深度的一个分享，相比于来往没节操的广告，易信还是比较脚踏实地的在做。
</p>




<p>第六个分享来自丁香园的CTO、微信「小道消息」作者冯大辉@Fenng。『嘉宾分享之 丁香园网站CTO冯大辉：问题与痛点。1.找到用户的问题：了解用户困境、理解用户行为、
理解商业逻辑；以用户思维分析问题、以互联网思维解决问题，未必需要技术。2. 用户的痛点：痛点是用户解决不了的问题、承受不了的成本、麻烦快要死的事，更可能不存在。
“发现比想更重要”。』（<a href="http://e.weibo.com/2863376813/Ai6zi2jXi">微博</a>）。辉哥@Fenng 吐槽了很多，把很多公司贬为“狗屎”，也从“问题与痛点”深入细致的
分析了在创业过程中，创业者们在解决问题和消除痛点时要思考的一些问题。
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013111103.jpg"></center>
</p>




<h2>初创团队的产品展示</h2>


<p>『演讲团队之 “水果的牌子”Kivvi收银平板：时尚店主的第一台POS收银机。将收银系统和POS机完美结合，内置磁卡条、IC卡刷卡两种支付功能并预留NFC；简约一体的外观
设计为商户提供了宽敞的收银环境。商家可直接通过Kivvi编辑商品信息，完成现金和刷卡支付，查询交易记录销售报表@Gokivvi』（<a href="http://e.weibo.com/2863376813/Ai5lGCO99">微博</a>）。
『一个包含 硬件 互联网 金融 3个关键字的很炫的创业项目，赞一个~』（<a href="http://weibo.com/2704795533/Ai5lrfl11">微博</a>）。很好的一个创业点，也是这次产品展示中我最看好的项目之一。
</p>




<p>『演讲团队之 “数字时代让用户更爽”的九漫 交互式动态漫画：以自主研发的创新型交互式阅读引擎为依托，将静态的数字漫画改编成可交互的动态数字漫画。
九漫创新开发工具和完备的制作流程确保生产效率是传统2D动画的10倍以上，所以只需要不到1/10的成本，就可达到动画60%~70%的效果。[good]』
（<a href="http://e.weibo.com/2863376813/Ai5savoGs">微博</a>）。『交互式动态漫画，demo不错的样子，不过国内动漫2C的思路确实很难盈利，国外2B的
话还是有机会的~ 』（<a href="http://weibo.com/2704795533/Ai5tjFyA2">微博</a>）。
</p>




<p>『演讲团队之 要出发旅行网：针对都市白领、年轻一族，围绕周边自驾游，提供精品特价的度假产品。精选主流城市周边的优质“酒店+吃喝玩乐”搭配。
由旅行记者实地考察亲身体验，保证酒店品质及服务的高品质，也有特派摄影师实地拍照，专业设计团队制作文案，为用户提供齐全的游玩攻略。[给力]』
（<a href="http://e.weibo.com/2863376813/Ai5vNn0HA">微博</a>）。『周边自驾游，用DS能够接受的钱享受GFS般的服务，但是价格战不是小公司的致胜之道啊~』
（<a href="http://weibo.com/2704795533/Ai5yJFySh">微博</a>）。
</p>




<p>『演讲团队之 找大巴网——首创BO2O大巴招投标预订直销模式：专注打造大巴预订直销平台和最方便好用的大巴预订神器。托酒店管理公司平台，
找大巴网拥有自己的车队和百家合作车队。提供大巴供应商直销管理系统，方便大巴车队调度人员管理车辆库存，提高出租率，增加车队收入。[给力]』
（<a href="http://e.weibo.com/2863376813/Ai5CJ2FXd">微博</a>）。『大巴预定神奇，可以比价，有评论，审核系统。。。 』（<a href="http://weibo.com/2704795533/Ai5CMg2ss">微博</a>）。
大巴版快的打车，比较看好~
</p>




<p>『演讲团队之 微洽，做工作社交阶段的引领者——一款基于企业2.0理念设计的企业社交化工作平台。对企业而言，用全新的产品体验，改变工作沟通、
协作和分享方式，促进工作目标的达成；对个人而言，通过满足基于工作的社交需求，实现个人在工作和职业上的存在感。[可爱]』
（<a href="http://e.weibo.com/2863376813/Ai5Y4xKGG">微博</a>）。『企业社交，又一个有意思的垂直入口～』（<a href="http://weibo.com/2704795533/Ai5WKmJlY">微博</a>）。
企业版的微信，但推广有难度，2B or 2C，That&#8217;s a question。
</p>




<p>『演讲团队之 阿姨厨房，“寻找您身边的私房菜”——P2P模式的餐饮创业平台。阿姨厨房整合闲置的厨房、让有手艺的家庭从事餐饮服务，为周边的白领、
邻居提供私房菜。阿姨厨房建立标准化体系、整合物流、线上营销为创业者提供用户、物流服务。明天有没有糖醋小排？[花心]』
（<a href="http://e.weibo.com/2863376813/Ai61MpZuA">微博</a>）。『很有意思，但确实推广难度很大啊，不过吃货有福利了～』
（<a href="http://weibo.com/2704795533/Ai65YpONI">微博</a>）。感觉用户群的年龄偏大，确实不太好推广，从小区和菜市场入手进行推广，很不错的切入点。
</p>




<p>『演讲团队之 同船渡——帮你来一场说走就走的旅行。同船渡通过支持用户根据爱好、玩法、自定义路线等创造新的旅行点打造“旅游wiki”；并且提供功能让
用户根据自身需求邀约旅行，让熟悉创建旅行点数据的用户成为向导，实现旅游从Do It Yourself到Design It Yourself的改变。』
（<a href="http://e.weibo.com/2863376813/Ai66Jsc4f">微博</a>）。『信息量确实很大，节操掉了一地啊～』（<a href="http://weibo.com/2704795533/Ai66S0n1S">微博</a>）。

</p>




<p>PS: 这几个创业项目在36kr上都有过报道，可以参见 <a href="http://www.36kr.com/events/oday-201311-hz#posts_list">列表</a>.</br>
相关的PPT微盘资源如下：<a href="http://vdisk.weibo.com/u/2863376813?page=1">演讲PPT合集</a></p>




<h2>观众参与的开放演讲</h2>


<p>每个人1分钟的开放演讲很短，包括如下项目：画吧、快读、37 度、比比、氢学习、Omygod、Tipix—照片编辑App、配配、微信公众账号杭电助手、车纷享、穿戴式智能设备、享赢棋牌联盟、
爱扫货、微博书等，其中一些已经有成品发布，有些则还处于开发测试阶段。这其中以下几个项目已被36kr创业数据库收录或者被报道过：</br>
<a href="http://www.36kr.net/huaba">画吧</a></br>
<a href="http://www.36kr.net/kuaiduyueduqi">快读</a></br>
<a href="http://www.36kr.net/qingxuexi">氢学习</a></br>
<a href="http://www.36kr.net/Tipix">Tipix—照片编辑App</a></br>
<a href="http://www.36kr.net/peipei">配配</a></br>
<a href="http://www.36kr.net/chefenxiang">车纷享</a></br>
可以点击以上链接获得这些项目更加详细的信息。
</p>




<p>后记：第一次参加这样高大上的创业分享盛会，还是挺激动的，非常感谢@36氪 和 @36氪开放日。创业是一件很刺激、需要激情和果敢的事，也是一件需要理性、需要毅力的事。
创业是一种追求，一种价值观，一种生活方式。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[互联网公司为什么都在玩硬件？]]></title>
    <link href="http://ibillxia.github.io/blog/2013/10/30/why-web-companies-all-are-beginning-to-play-hardware-products/"/>
    <updated>2013-10-30T21:07:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/10/30/why-web-companies-all-are-beginning-to-play-hardware-products</id>
    <content type="html"><![CDATA[<p>昨天360高调推出安全手环，延续着去年以来互联网公司进军硬件行业的风气。小米手机一炮而红后，互联网公司纷纷进军手机硬件行业，
百度、阿里、盛大、360等等都推出了自己的定制手机，不过巨头们发现原来硬件并没有想象中那般好玩。但不好玩也得玩，你不玩别人也
会玩。</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013103001.jpeg"></center>




<p>但今年，巨头们不玩手机了，改玩其他了，有的是自己独立生产，有的是与厂家合作，内嵌自己的服务。这其中的玩家包括百度、360、小米、盛大、乐视等。</p>




<!--more-->


<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;分割线&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;</p>


<p><strong>先不妨梳理一下巨头们都推出了哪些硬件.</strong></p>


<h4>1、百度</h4>


<p>咕咚手环：一款健康设备，用于检测运动和睡眠信息，了解在自身的健康状况。</br>
inWatch智能腕表：提供时间、音乐、上网、拨打电话等功能，与智能手机无异。</br>
小度WiFi：一个简易的WiFi共享装置。</br>
小度路由，小度影棒：即将推出，都是网络看片用的。</p>


<h4>2、腾讯</h4>


<p>Q影：是一款口袋式的互动投影，可以在几乎任何你需要的场景中使用，任何一个平面，Q影都可以投影能够触控的画面，让任一平面瞬间变身触控Touch Pad。</br>
小Q机器人：小Q机器人（Qrobot）是智能互联网机器人产品，是中国科学院与腾讯公司重大战略合作项目。它能够通过语音指令等多维交互方式为用户提供新闻、
天气、音乐、股票、教育、娱乐、办公等资讯和服务应用。同时，Qrobot有丰富的应用平台，通过互联网即可下载更多的功能。</p>


<h4>3、360</h4>


<p>360随身WiFi：一个简易的WiFi共享设备。</br>
360安全手环：用户孩童的安全设备，更像一个GPS+对讲机的结合。</p>


<h4>4、盛大</h4>


<p>果壳手表（Geak Watch）：同样是智能手表，其独特之处是可以独立上网，功能更全面。</br>
果壳魔戒（Geak Ring）：提供解锁、传递个人信息等功能，功能比较杂糅，定位略显模糊。</p>


<h4>5、小米</h4>


<p>小米盒子和小米电视，就不在此赘述了，大家应该比我了解。</p>


<h4>6、乐视</h4>


<p>乐视盒子和乐视电视，同样不再赘述。</p>


<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;分割线&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;</p>




<p><strong>互联网巨头们推出的硬件看似杂乱无章，其实不外乎以下几种类型：</strong></p>


<h4>1、独立的可穿戴设备</h4>


<p>顺应国外可穿戴设备的潮流，可与人体或衣物整合的互联网设备，利用人体的行为与设备产生交互。</br>
代表：百度咕咚手环，360安全手环，盛大果壳手表</p>


<h4>2、创新性的配件</h4>


<p>作为传统的硬件或传统的互联网服务的配件存在，更多的是微创新，起到细节优化的作用</br>
代表：小米盒子，小度WiFi，360随身WiFi</p>


<h4>3、传统的大众家电等</h4>


<p>互联网企业进军传统家电绝不只是贴一个自己Logo就完事儿了，而是在其中增加更多满足用户需求的互联网因素</br>
代表：小米电视，乐视TV</p>


<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;分割线&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;</p>




<p><strong>那么互联网企业进入硬件行业的目的是什么呢？</strong></p>


<h4>1、抢占入口</h4>


<p>这也是行业普遍的观点，互联网公司进军硬件行业是为了给自己的互联网服务找更多的入口。在当下，互联网服务同质化非常严重，很难在内容上吸引到更多的用户，所以互联网企业只能在渠道入口上下功夫。</p>


<p>PC上，浏览器和网址导航是入口，而用户已经在逃离PC，不仅手机，更多的电子设备以及接入了互联网，包括平板电脑、电子书等，这些设备不仅分流了用户的上网时间，
一定程度上也形成了新的入口。所以巨头的战略就是通过硬件切入，抢占入口，输出自己的互联网服务，获取利益。</p>


<p>举个例子，假设用户想看视频。在传统PC端，用户会习惯性地打开优酷土豆搜索或者去百度搜索（很可能被导入到爱奇艺或者PPS），这样优酷土豆和百度就是个强劲的入口；
在移动端，用户可能会选取App Store、豌豆荚、360手机助手、91助手或者其他应用商店下，这样控制应用分发的百度和360 很可能强推自己的百度视频（以及爱奇艺和PPS的移动APP）和360视频搜索。</p>


<p>而PC和移动端都无法满足你看视频的需求，假设你购买了小度影棒或者乐视盒子，高清大屏，更好的用户体验吸引了你，这时你浏览的视频一定来自于百度系爱奇艺、PPS或者乐视网的内容，这也就是硬件的价值所在。</p>




<h4>2、再造新的入口</h4>


<p>一切都在互联网化，人与互联网的界限也在逐渐变得模糊，在不断互联网化的过程中我们总会挖掘更多地需求，这种需求可能是原来就有的，现在只是把其互联网化了；
也可能是原来没有的，现在借用互联网将其挖掘了出来。很多互联网公司涉足这样的硬件可能并不是推广自己的服务，因为其本身的服务并不一定契合硬件产品功能，
其目的是提前布局，或许用户，建立新的互联网服务，再造一个入口，而不是抢夺原来的入口。</p>


<p>就拿昨天推出的360安全手环举例。儿童安全这个市场一定有不少产品了，但周鸿祎让这个市场及产品互联网化了。除了网络上的新闻报道，笔者对这款产品的细节及后续的更新还不是很了解，
但我认为360此举肯定不是为了推广自己的互联网服务，内置个360杀毒未免太low了吧。</p>


<p>这款产品自身在通话、录音、定位等方面有着广泛的互联网需求，而这些需求一旦被接受，就会带来价值，这才是很多互联网公司推出硬件的目的，开辟一款处女地，再重新开荒耕耘。</p>




<h4>3、获取数据</h4>


<p>很多互联网公司已经获取到足够多的数据了，但对于数据永远是嫌少不嫌多的，更何况，可穿戴设备下的数据是不同于传统的网络浏览数据的，可穿戴设备下的数据由于在交互上离人的真实更接近，
这样的数据其实更立体直观、更接近现实，也就更有价值。</p>


<p>举个例子，假设你用百度或者微博搜索一些关于运动健康的关键词，通过一定的数据处理分析，可能你以后会在百度右侧广告栏或者微博推广橱窗中看到运动健康相关的广告，
这就是一个简单的大数据分析，但这样的分析大部分情况下是不准确的。而假设你用了百度的咕咚手环，它有效记录了你的睡眠和运动情况，这样分析的结果可能更接近真实，
由于你用了同一个账户体系，以后再使用百度的时候，推广的广告可能就精准了。</p>


<p>我再做一个猜测，假如这个手环通过分析你的心跳、血液循环流动监测到了你潜在的疾病，并提醒了你，是不是这样的数据更有价值？因为这个手环离你更近，不只是空间上，
更是生理心理上。所以，可穿戴设备的数据的价值不在于更多，而在于更精确。</p>




<h4>4、硬件盈利</h4>


<p>好吧，笔者开了个玩笑。Kindle都在赔钱卖吆喝，当下用硬件盈利未免太反科学了。而周鸿祎也已经预言未来是一个硬件免费的时代，笔者希望那一天快点到来吧。</p>


<p>互联网企业进军硬件已经成为大势所趋，而进军硬件只是表象，本质原因是因为随着互联网的全面普及和深化，我们需要更多的新设备来连接或者呈现新的功能，所以就有了这一波硬件的潮流，
这有点像电灯、电话、电视那个百花齐放的时代，对应的是电的利用。</p>


<p>所以，在去年一股脑扎进手机圈后，今年互联网巨头对硬件制造更加理性了，推出的产品也更有市场针对性和前瞻性。这些硬件产品或是互联网企业传统服务的入口，
或是承担试水新领域的任务，或是大数据采集的深化，或是几者叠加重合。总之，对于硬件，我是不嫌多的。</p>




<p>转载自钛媒体：<a href="http://www.tmtpost.com/74476.html">互联网公司为什么都在玩硬件？</a>， 有删改。</p>


<p>知乎相关问答：</br>
[1] <a href="http://www.zhihu.com/question/21337805">2013年国内有哪些互联网公司在做硬件？</a></br>
[2] <a href="http://www.zhihu.com/question/21661422">怎么看待现在国内互联网公司纷纷开始发力硬件产品？</a>
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux的OSS和ALSA声音系统简介及其比较]]></title>
    <link href="http://ibillxia.github.io/blog/2013/09/08/brief-introduction-of-alsa-and-oss-and-it%27s-comparision/"/>
    <updated>2013-09-08T19:46:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/09/08/brief-introduction-of-alsa-and-oss-and-it&#8217;s-comparision</id>
    <content type="html"><![CDATA[<h2>概述</h2>


<p>昨天想在Ubuntu上用一下HTK工具包来绘制语音信号的频谱图和提取MFCC的结果，但由于前段时间把Ubuntu升级到13.04，系统的声卡驱动
是ALSA（Advanced Linux Sound Architecture，高级Linux声音体系），而不是HTK中所使用的OSS（Open Sound System，开放声音系统）。
网上查阅了大半天，按照 http://forum.ubuntu.org.cn/viewtopic.php?t=316792 中提供的方法用OSS4来替换ALSA，结果OSS4没替换成功，
而原来的ALSA也不好使了，真坑爹啊！到现在还没办法完全复原，现在只能通过alsamixer来设置音量了，系统的音量设置根本无法用，而且
声音设置中的输入设备和输出设备都是空的。（现在将系统升级到13.10版，系统的音量设置可以用了，哈哈）捣鼓了半天也没还原回来唉，
整个人都快崩溃了，都是由于对Linux不熟悉才被虐至如此地步，得恶补啊！！！下面本文就主要介绍一下OSS和ALSA，并将二者进行比较。</p>




<p>在介绍OSS和ALSA之前，先介绍一下音频设备的一些基础知识。</br>
数字音频设备，有时也称codec，PCM，DSP，ADC/DAC设备，用来播放或录制数字化的声音。它的指标主要有：采样速率（电话为8K，DVD为96K）、
channel数目（单声道，立体声）、采样分辨率（8-bit，16-bit）等。</br>
mixer（混频器）：用来控制多个输入、输出的音量，也控制输入（microphone，line-in，CD）之间的切换。</br>
synthesizer（合成器）：通过一些预先定义好的波形来合成声音，有时用在游戏中声音效果的产生。</br>
MIDI接口：MIDI接口是为了连接舞台上的synthesizer、键盘、道具、灯光控制器的一种串行接口。</p>




<!--more-->




<h2>OSS开放声音系统简介</h2>


<p>Open Sound System是一个类Unix和POSIX兼容系统上一个可选的声音架构。OSSv3是Linux下原始的声音系统并集成在内核里，但是OSSv4
在2002年OSS成为商业软件时它地位被ALSA所取代。OSSv4在2007年又成为了开源软件，4Front Technologies以GPL协议发布了它的源码。</p>




<p>OSS（Open Sound System）是unix平台上一个统一的音频接口。以前，每个Unix厂商都会提供一个自己专有的API，用来处理音频。这就
意味着为一种Unix平台编写的音频处理应用程序，在移植到另外一种Unix平台上时，必须要重写。不仅如此，在一种平台上具备的功能，
可能在另外一个平台上无法实现。但是，OSS出现以后情况就大不一样了，只要音频处理应用程序按照OSS的API来编写，那么在移植到另外
一个平台时，只需要重新编译即可。因此，OSS提供了源代码级的可移植性。</p>




<p>同时，很多的Unix工作站中，只能提供录音与放音的功能。有了OSS后，给这些工作站带来了MIDI功能，加上音频流、语音识别/生成、
计算机电话（CT）、JAVA以及其它的多媒体技术，在Unix工作站中，同样可以享受到同Windows、Macintosh环境一样的音频世界。另外，
OSS还提供了与视频和动画播放同步的音频能力，这对在Unix中实现动画、游戏提供了帮助。</p>




<p>在Unix系统中，所有的设备都被统一成文件，通过对文件的访问方式（首先open，然后read/write，同时可以使用ioctl读取/设置参数，
最后close）来访问设备.在OSS中，主要有以下的几种设备文件：</br>
/dev/mixer：访问声卡中内置的mixer，调整音量大小，选择音源。</br>
/dev/sndstat：测试声卡，执行cat /dev/sndstat会显示声卡驱动的信息。</br>
/dev/dsp、/dev/dspW、/dev/audio：读这个设备就相当于录音，写这个设备就相当于放音。/dev/dsp与/dev/audio之间的区别在于采样的编码
不同，/dev/audio使用μ律编码，/dev/dsp使用8-bit（无符号）线性编码，/dev/dspW使用16-bit（有符号）线形编码。/dev/audio主要是为了
与SunOS兼容，所以尽量不要使用。</br>
/dev/sequencer：访问声卡内置的，或者连接在MIDI接口的synthesizer。</p>




<p>OSS为音频编程提供三种设备，分别是/dev/dsp，/dev/dspW和/dev/audio，用户可以直接使用Unix的命令来放音和录音，命令cat /dev/dsp >xyz
可用来录音，录音的结果放在xyz文件中；命令cat xyz >/dev/dsp播放声音文件xyz。如果通过编程的方式来使用这些设备，那么Unix平台通过
文件系统提供了统一的访问接口。程序员可以通过文件的操作函数直接控制这些设备，这些操作函数包括：open、close、read、write、ioctl等。</p>




<h2>ALSA高级Linux声音系统简介</h2>


<p>高级Linux声音体系（英语：Advanced Linux Sound Architecture，缩写为ALSA）是Linux内核中，为声卡提供的驱动组件，以替代原先的
OSS（开放声音系统）。一部分的目的是支持声卡的自动配置，以及完美的处理系统中的多个声音设备，这些目的大多都已达到。另一个声音
框架JACK使用ALSA提供低延迟的专业级音频编辑和混音能力。</p>




<p>这个项目开始于为1998年Gravis Ultrasound所开发的驱动，它一直作为一个单独的软件包开发，直到2002年他被引进入Linux内核的开发
版本(2.5.4-2.5.5)。从2.6版本开始ALSA成为Linux内核中默认的标准音频驱动程序集，OSS则被标记为废弃。</p>




<p>ALSA由许多声卡的声卡驱动程序组成，同时它也提供一个称为libasound的API库。应用程序开发者应该使用libasound而不是内核中的ALSA接口。
因为libasound提供最高级并且编程方便的编程接口。并且提供一个设备逻辑命名功能，这样开发者甚至不需要知道类似设备文件这样的低层接口。
相反，OSS/Free驱动是在内核系统调用级上编程，它要求开发者提供设备文件名并且利用ioctrl来实现相应的功能。为了向后兼容，ALSA提供内核
模块来模拟OSS，这样之前的许多在OSS基础上开发的应用程序不需要任何改动就可以在ALSA上运行。另外，libaoss库也可以模拟OSS，而它不需要
内核模块。另外，ALSA还包含插件功能，使用插件可以扩展新的声卡驱动，包括完全用软件实现的虚拟声卡。ALSA提供一系列基于命令行的工具集，
比如混音器(mixer)，音频文件播放器(aplay)，以及控制特定声卡特定属性的工具。</p>




<p>ALSA API主要分为以下几种接口：</br>
控制接口：提供灵活的方式管理注册的声卡和对存在的声卡进行查询。</br>
PCM接口：提供管理数字音频的捕捉和回放。</br>
原始MIDI接口: 支持 MIDI (Musical Instrument Digital Interface)，一种标准电子音乐指令集。这些API提供访问声卡上的MIDI总线。
这些原始借口直接工作在 The MIDI事件上，程序员只需要管理协议和时间。</br>
记时接口: 为支持声音的同步事件提供访问声卡上的定时器。</br>
音序器接口：一个比原始MIDI接口高级的MIDI编程和声音同步高层接口。它可以处理很多的MIDI协议和定时器。</br>
混音器接口：控制发送信号和控制声音大小的声卡上的设备。</p>




<p>API库使用逻辑设备名而不是设备文件。设备名字可以是真实的硬件名字也可以是插件名字。硬件名字使用hw:i,j这样的格式。其中i是卡号，
j是这块声卡上的设备号。第一个声音设备是hw:0,0.这个别名默认引用第一块声音设备并且在本文示例中一真会被用到。插件使用另外的唯一名字。
比如plughw:,表示一个插件，这个插件不提供对硬件设备的访问，而是提供像采样率转换这样的软件特性，硬件本身并不支持这样的特性。</p>




<h2>OSS与ALSA的优缺点比较</h2>


<p>ALSA是一个完全开放源代码的音频驱动程序集，除了像OSS那样提供了一组内核驱动程序模块之外，ALSA还专门为简化应用程序的编写提供了
相应的函数库，与OSS提供的基于ioctl的原始编程接口相比，ALSA函数库使用起来要更加方便一些。利用该函数库，开发人员可以方便快捷的
开发出自己的应用程序，细节则留给函数库内部处理。当然ALSA也提供了类似于OSS的系统接口，不过ALSA的开发者建议应用程序开发者使用
音频函数库而不是驱动程序的API。Ubuntu默认使用ALSA作为底层声音驱动，程序则与PulseAudio交互，这是一个很不错的方案。</p>




<p>下面来比较一下OSS和ALSA的优缺点：</br>
<strong>(1)OSS的优点（对用户来说）</strong></br>
在内核空间（kernel space）里面包含了一个透明软件混音器(vmix)。这样多个程序就可以同时使用声音设备而且没有任何问题。</br>
这个混音器可以让你单独调节各个程序的音量。</br>
对某些老声卡有着更好的支持比如创新（Creative）的X-Fi。</br>
声音程序的初始反应时间一般更好。</br>
对使用OSS的应用程序接口（API）的程序有更好的支持，很多程序都支持OSS的API，而不需要ALSA的模拟。</br>

<strong>(2)OSS的优点（对开发者来说）</strong></br>
清晰的API文档，更易于使用。</br>
支持用户空间的声音驱动。</br>
可移植性强，OSS也可以在BSDs和Solaris下运行。</br>
本身可以跨平台，可以更方便移植到新的操作系统。</br>

<strong>(3)ALSA的优点</strong></br>
ALSA对USB音频设备支持更好，而OSS的输出还在试验中，输入还未实现。</br>
ALSA支持蓝牙声音设备。</br>
ALSA支持AC&#8217;97和HDAudio dial-up soft-modems (比如Si3055)。</br>
ALSA对MIDI支持得更好，但用OSS你只能通过软件合成器（如timidity和fluidsynth）来使用MIDI。</br>
ALSA对待机支持更好，而用OSS，你需要在待机前使用soundoff来停止OSS驱动，在恢复后使用soundon来启动OSS。</br>
OSS的jack检测目前在某些HDAudio-powered主板上不能正常工作。也就是说在某些型号的主板上，你可能需要在插入耳机的
时候手动关闭外置扬声器。而ALSA没这个问题。
</p>




<h2>参考资料</h2>


<p>[1]Archlinux上介绍OSS的Wiki：https://wiki.archlinux.org/index.php/OSS_%28%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87%29 </br>
[2]Archlinux上介绍ALSA的Wiki：https://wiki.archlinux.org/index.php/Advanced_Linux_Sound_Architecture_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) </br>
[3]OSS&#8211;跨平台的音频接口简介: http://www.ibm.com/developerworks/cn/linux/l-ossapi/ </br>
[4]Linux ALSA声卡驱动之一：ALSA架构简介: http://blog.csdn.net/droidphone/article/details/6271122 </br>
[5]Linux ALSA声卡编程简介: http://enmind.blog.163.com/blog/static/164138001201092334620355/</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PCM WAVE格式详解及用C语言实现wave文件的读取]]></title>
    <link href="http://ibillxia.github.io/blog/2013/07/20/details-of-wave-format-and-reading-wave-files-in-C-language/"/>
    <updated>2013-07-20T20:07:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/07/20/details-of-wave-format-and-reading-wave-files-in-C-language</id>
    <content type="html"><![CDATA[<h2>1.PCM Wave格式详解</h2>


<p>WAVE文件格式是微软RIFF(Resource Interchange File Format,资源交换文件标准)的一种，是针对于多媒体文件存储的一种文件格式和标准。
一般而言，RIFF文件由文件头和数据两部分组成，一个WAVE文件由一个“WAVE”数据块组成，这个“WAVE”块又由一个&#8221;fmt&#8221;子数据块和一个“data”子
数据块组成，也称这种格式为“Canonical form”（权威/牧师格式），如下图所示：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013072001.gif"></center>
</p>


<!--more-->


<p>每个字段的涵义如下：
ChunkID: 占4个字节，内容为“RIFF”的ASCII码(0x52494646)，以大端（big endian）存储。</br>
ChunkSize: 4字节，存储整个文件的字节数（不包含ChunkID和ChunkSize这8个字节），以小端（little endian）方式存储。</br>
Format: 4字节，内容为“WAVE”的ASCII码(0x57415645)，以大端存储。</br>
</p>




<p>
其中bigendian 主要有一个特征，在内存中对操作数的存储方式和从高字节到低字节。例如：0x1234，这样一个数，存储为:</br>
0x4000:   0x12</br>
0x4001:   0x34</br>
而小尾端littleendian是：</br>
0x4000:   0x34</br>
0x4001:   0x12</br>
用程序在区别的话，可以考虑：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#include &lt;stdio.h>
</span><span class='line'>#include &lt;stdlib.h>
</span><span class='line'>int main(int argc, char *argv[])
</span><span class='line'>{
</span><span class='line'>       union w
</span><span class='line'>      {
</span><span class='line'>       short int a;
</span><span class='line'>       char b;
</span><span class='line'>      }c;
</span><span class='line'>      c.a=1;
</span><span class='line'>      if( c.b==1 )  printf("little endian\n");
</span><span class='line'>      else printf("big endian\n");
</span><span class='line'>      system("PAUSE"); 
</span><span class='line'>      return 0;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<p>&#8220;WAVE&#8221;格式由两个子数据块构成：“fmt”块和“data”块，其中“fmt”块的详细解释如下：
Subchunk1ID: 占4个字节，内容为“fmt ”的ASCII码(0x666d7420)，以大端存储。</br>
Subchunk1Size: 占4个字节，存储该子块的字节数（不含前面的Subchunk1ID和Subchunk1Size这8个字节），以小端方式存储。</br>
AudioFormat：占2个字节，以小端方式存储，存储音频文件的编码格式，例如若为PCM则其存储值为1，若为其他非PCM格式的则有一定的压缩。</br>
NumChannels: 占2个字节，以小端方式存储，通道数，单通道(Mono)值为1，双通道(Stereo)值为2，等等。</br>
SampleRate: 占4个字节，以小端方式存储，采样率，如8k，44.1k等。</br>
ByteRate: 占4个字节，以小端方式存储，每秒存储的bit数，其值=SampleRate * NumChannels * BitsPerSample/8</br>
BlockAlign: 占2个字节，以小端方式存储，块对齐大小，其值=NumChannels * BitsPerSample/8</br>
BitsPerSample: 占2个字节，以小端方式存储，每个采样点的bit数，一般为8,16,32等。</br>
接下来是两个可选的扩展参数：</br>
ExtraParamSize: 占2个字节，表示扩展段的大小。</br>
ExtraParams: 扩展段其他自定义的一些参数的具体内容，大小由前一个字段给定。
</p>




<p>其中，对于每个采样点的bit数，不同的bit数读取数据的方式不同：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// data 为读取到的采样点的值，speech为原始数据流，
</span><span class='line'>//对应于下面的"WAVE"格式文件的第二个子数据块“data”块的“Data”部分。
</span><span class='line'>for(i=0;i&lt;NumSample;i++){
</span><span class='line'>  if(BitsPerSample==8)
</span><span class='line'>      data[i] = (int)*((char*)speech+i);
</span><span class='line'>  else if(BitsPerSample==16)
</span><span class='line'>      data[i] = (int)*((short*)speech+i);
</span><span class='line'>  else if(BitsPerSample==32)
</span><span class='line'>      data[i] = (int)*((int*)speech+i);
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>

</p>




<p>&#8220;WAVE&#8221;格式文件的第二个子数据块是“data”，其个字段的详细解释如下：</br>
Subchunk2ID: 占4个字节，内容为“data”的ASCII码(0x64617461)，以大端存储。</br>
Subchunk2Size: 占4个字节，内容为接下来的正式的数据部分的字节数，其值=NumSamples * NumChannels * BitsPerSample/8</br>
Data: 真正的语音数据部分。</br>
</p>




<h2>一个Wave文件头的实例</h2>


<p>设一个wave文件的前72个字节的十六进制内容如下(可以使用Ultra Edit等工具查看wave文件头)：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>52 49 46 46 24 08 00 00 57 41 56 45 66 6d 74 20 10 00 00 00 01 00 02 00 
</span><span class='line'>22 56 00 00 88 58 01 00 04 00 10 00 64 61 74 61 00 08 00 00 00 00 00 00 
</span><span class='line'>24 17 1e f3 3c 13 3c 14 16 f9 18 f9 34 e7 23 a6 3c f2 24 f2 11 ce 1a 0d</span></code></pre></td></tr></table></div></figure>

则其个字段的解析如下图：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013072002.gif"></center>
</p>




<h2>C语言实现wave文件的读取</h2>


<p>这里给出一个用基本的C语言文件操作库函数实现的Wave文件读取的实例代码，可以跨Windows和Linux平台。</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#include &lt;stdio.h>
</span><span class='line'>#include &lt;stdlib.h>
</span><span class='line'>#include &lt;string.h>
</span><span class='line'>
</span><span class='line'>// define Wave format structure
</span><span class='line'>typedef struct tWAVEFORMATEX
</span><span class='line'>{
</span><span class='line'>    short wFormatTag;         /* format type */
</span><span class='line'>    short nChannels;          /* number of channels (i.e. mono, stereo...) */
</span><span class='line'>    unsigned int nSamplesPerSec;     /* sample rate */
</span><span class='line'>    unsigned int nAvgBytesPerSec;    /* for buffer estimation */
</span><span class='line'>    short nBlockAlign;        /* block size of data */
</span><span class='line'>    short wBitsPerSample;     /* number of bits per sample of mono data */
</span><span class='line'>    short cbSize;             /* the count in bytes of the size of */
</span><span class='line'>                                    /* extra information (after cbSize) */
</span><span class='line'>} WAVEFORMATEX, *PWAVEFORMATEX;
</span><span class='line'>
</span><span class='line'>char* wavread(char *fname, WAVEFORMATEX *wf);
</span><span class='line'>
</span><span class='line'>int main(){
</span><span class='line'>  char fname = "test.wav";
</span><span class='line'>  char *speech;
</span><span class='line'>  WAVEFORMATEX wf;
</span><span class='line'>  
</span><span class='line'>  speech = wavread(fname, &wf);
</span><span class='line'>  // afterward processing...
</span><span class='line'>  
</span><span class='line'>  return 0;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>// read wave file
</span><span class='line'>char* wavread(char *fname, WAVEFORMATEX *wf){
</span><span class='line'>  FILE* fp;
</span><span class='line'>  char str[32];
</span><span class='line'>  char *speech;
</span><span class='line'>  unsigned int subchunk1size; // head size
</span><span class='line'>  unsigned int subchunk2size; // speech data size
</span><span class='line'>
</span><span class='line'>  // check format type
</span><span class='line'>  fp = fopen(fname,"r");
</span><span class='line'>  if(!fp){
</span><span class='line'>      fprintf(stderr,"Can not open the wave file: %s.\n",fname);
</span><span class='line'>      return NULL;
</span><span class='line'>  }
</span><span class='line'>  fseek(fp, 8, SEEK_SET);
</span><span class='line'>  fread(str, sizeof(char), 7, fp);
</span><span class='line'>  str[7] = '\0';
</span><span class='line'>  if(strcmp(str,"WAVEfmt")){
</span><span class='line'>      fprintf(stderr,"The file is not in WAVE format!\n");
</span><span class='line'>      return NULL;
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  // read format header
</span><span class='line'>  fseek(fp, 16, SEEK_SET);
</span><span class='line'>  fread((unsigned int*)(&subchunk1size),4,1,fp);
</span><span class='line'>  fseek(fp, 20, SEEK_SET);
</span><span class='line'>  fread(wf, subchunk1size, 1, fp);
</span><span class='line'>  
</span><span class='line'>  // read wave data
</span><span class='line'>  fseek(fp, 20+subchunk1size, SEEK_SET);
</span><span class='line'>  fread(str, 1, 4, fp);
</span><span class='line'>  str[4] = '\0';
</span><span class='line'>  if(strcmp(str,"data")){
</span><span class='line'>      fprintf(stderr,"Locating data start point failed!\n");
</span><span class='line'>      return NULL;
</span><span class='line'>  }
</span><span class='line'>  fseek(fp, 20+subchunk1size+4, SEEK_SET);
</span><span class='line'>  fread((unsigned int*)(&subchunk2size), 4, 1, fp);
</span><span class='line'>  speech = (char*)malloc(sizeof(char)*subchunk2size);
</span><span class='line'>  if(!speech){
</span><span class='line'>      fprintf(stderr, "Memory alloc failed!\n");
</span><span class='line'>      return NULL;
</span><span class='line'>  }
</span><span class='line'>  fseek(fp, 20+subchunk1size+8, SEEK_SET);
</span><span class='line'>  fread(speech, 1, subchunk2size, fp);
</span><span class='line'>
</span><span class='line'>  fclose(fp);
</span><span class='line'>  return speech;
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>




<h2>参考</h2>


<p>
[1]WAVE PCM soundfile format: https://ccrma.stanford.edu/courses/422/projects/WaveFormat/ </br>
[2]Resource Interchange File Format: http://en.wikipedia.org/wiki/Resource_Interchange_File_Format </br>
[3]基于Visual C++6.0的声音文件操作: http://www.yesky.com/20030414/1663116_1.shtml
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[给Octpress博客添加返回顶部按钮]]></title>
    <link href="http://ibillxia.github.io/blog/2013/06/30/add-a-back-to-top-button-on-ur-octpress-blog/"/>
    <updated>2013-06-30T10:47:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/06/30/add-a-back-to-top-button-on-ur-octpress-blog</id>
    <content type="html"><![CDATA[<p>有时候，博客文章太长，需要返回顶部时，需要用鼠标拖着滚动条向上好半天，这里提供一个用jQuery来实现的动态上滚的示例。
这个示例完全参考和翻译自webdesignerwall的blog：<a href="http://webdesignerwall.com/tutorials/animated-scroll-to-top">http://webdesignerwall.com/tutorials/animated-scroll-to-top</a>，
其中有部分删改，并在本人的blog上实现。</p>




<p>主要包含HTML和CSS的设计，基于jQuery的JS的设计。另外还有一点小trick</p>




<h2>Design & CSS</h2>


<p>相关的HTML代码很简单，在source/_include/custom/footer.html中添加如下代码：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;p id = "back-top">
</span><span class='line'>      &lt;a href="#top">&lt;span>&lt;/span>Back to Top&lt;/a>
</span><span class='line'>  &lt;/p></span></code></pre></td></tr></table></div></figure>

</p>




<!--more-->




<p>对应的CSS样式的设置如下：（这段代码同样的放在source/_include/custom/footer.html文件中）

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;style type="text/css">
</span><span class='line'>#back-top {
</span><span class='line'>  position: fixed;
</span><span class='line'>  bottom: 50px;
</span><span class='line'>  right: 100px;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>#back-top a {
</span><span class='line'>  width: 80px;
</span><span class='line'>  display: block;
</span><span class='line'>  text-align: center;
</span><span class='line'>  font: 11px/100% Arial, Helvetica, sans-serif;
</span><span class='line'>  text-transform: uppercase;
</span><span class='line'>  text-decoration: none;
</span><span class='line'>  color: #bbb;
</span><span class='line'>
</span><span class='line'>  /* transition */
</span><span class='line'>  -webkit-transition: 1s;
</span><span class='line'>  -moz-transition: 1s;
</span><span class='line'>  transition: 1s;
</span><span class='line'>}
</span><span class='line'>#back-top a:hover {
</span><span class='line'>  color: #000;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>/* arrow icon (span tag) */
</span><span class='line'>#back-top span {
</span><span class='line'>  width: 80px;
</span><span class='line'>  height: 80px;
</span><span class='line'>  display: block;
</span><span class='line'>  margin-bottom: 7px;
</span><span class='line'>  background: #ddd url(../../images/up-arrow.png) no-repeat center center;
</span><span class='line'>
</span><span class='line'>  /* rounded corners */
</span><span class='line'>  -webkit-border-radius: 15px;
</span><span class='line'>  -moz-border-radius: 15px;
</span><span class='line'>  border-radius: 15px;
</span><span class='line'>
</span><span class='line'>  /* transition */
</span><span class='line'>  -webkit-transition: 1s;
</span><span class='line'>  -moz-transition: 1s;
</span><span class='line'>  transition: 1s;
</span><span class='line'>}
</span><span class='line'>/*
</span><span class='line'>#back-top a:hover span {
</span><span class='line'>  background-color: #888;
</span><span class='line'>}
</span><span class='line'>*/
</span><span class='line'>&lt;/style></span></code></pre></td></tr></table></div></figure>

</p>




<p>上面的css中用到了一张图片up-arrow.png，放在source/images/目录下，图片如下：
<center><img src="http://ibillxia.github.io/images/up-arrow.png"></center>
这是从google image里面随便找的一个，你也可以找一个自己喜欢的图片。
</p>




<h2>jQuery部分</h2>


<p>HTML和CSS样式设置好了之后，最后就是添加JavaScript事件响应代码了，这里是基于jQuery实现的。代码如下：（这段代码还是放在source/_include/custom/footer.html文件中）

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.4.3/jquery.min.js">&lt;/script>
</span><span class='line'>&lt;script type="text/javascript">
</span><span class='line'>$(document).ready(function(){
</span><span class='line'>
</span><span class='line'>  // hide #back-top first
</span><span class='line'>  $("#back-top").hide();
</span><span class='line'>  
</span><span class='line'>  // fade in #back-top
</span><span class='line'>  $(function () {
</span><span class='line'>      $(window).scroll(function () {
</span><span class='line'>          if ($(this).scrollTop() > 100) {
</span><span class='line'>              $('#back-top').fadeIn();
</span><span class='line'>          } else {
</span><span class='line'>              $('#back-top').fadeOut();
</span><span class='line'>          }
</span><span class='line'>      });
</span><span class='line'>
</span><span class='line'>      // scroll body to 0px on click
</span><span class='line'>      $('#back-top a').click(function () {
</span><span class='line'>          $('body,html').animate({
</span><span class='line'>              scrollTop: 0
</span><span class='line'>          }, 800);
</span><span class='line'>          return false;
</span><span class='line'>      });
</span><span class='line'>  });
</span><span class='line'>
</span><span class='line'>});
</span><span class='line'>&lt;/script></span></code></pre></td></tr></table></div></figure>

</p>




<h2>一个Trick</h2>


<p>
在上面的HTML代码中，我们将一个链接添加到了ID为#top的里面，这个#top标签是<body>标签的ID，这样即使浏览器不支持相关的JS，
通过这个link也实现了返回顶部的功能。
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[蛙泳、自由泳、仰泳、蝶泳图解动画教你游泳]]></title>
    <link href="http://ibillxia.github.io/blog/2013/06/28/swimming-tutorial-with-gif-images/"/>
    <updated>2013-06-28T21:37:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/06/28/swimming-tutorial-with-gif-images</id>
    <content type="html"><![CDATA[<p>最近在室友的带领下，开始去cjl游泳馆学游泳，这里转载一篇游泳教程，分享给初学游泳的网友们</p>




<h2>蛙泳</h2>


<p>蛙泳配合有一个顺口溜，在讲解蛙泳动作要领之前先介绍给大家：“划手腿不动，收手再收腿，先伸胳膊后蹬腿，并拢伸直漂一会儿。”
从顺口溜中可以看到，手的动作是先于腿的动作。一定要在收手后再收腿，伸手后再蹬腿。</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062801.gif"></center>




<p>臂部动作：</br>
1、外划。双手前伸，手掌倾斜大约45度(小拇指朝上)。双手同时向外、后方划，继而屈臂向后、向下方划。</br>
2、内划。掌心由外转向内，手带动小臂加速内划，手由下向上并在胸前并拢(手高肘低、肘在肩下)，前伸。</br>
3、前伸。双手向前伸(肘关节伸直)。要提醒大家注意的是：外划是放松的，内划是用力的、加速完成的、前伸是积极的。
</p>




<!--more-->




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062802.gif"></center>




<p>腿部动作：</br>
1、收腿：屈膝收腿，脚跟向臀部靠拢，小腿要躲在大腿后面慢收腿，这样可以减少阻力。收腿结束时，两膝与肩同宽，小腿与水面垂直，脚牚在水面附近。</br>
2、翻脚：两脚距离大于两膝距离，两脚外翻，脚尖朝外，脚牚朝天，小腿和脚内侧对准水，像英文字母“W”。</br>
3、夹蹬水：实际上是腿伸直的过程(屈髋、伸膝)，由腰腹和大腿同时发力，以小腿和脚内侧同时蹬夹水，先是向外、向后、向下，然后是向内、向上方蹬水，
就像画半个圆圈。向外蹬水和向内夹水是连续完成的，也就是连蹬带夹。蹬夹水完成时双腿并拢伸直，双脚内转，脚尖相对。蹬水的速度不要过猛，要由
慢到快地加速蹬水，两条腿将近伸直并拢的时候蹬水速度最快。</br>
4、停：双腿并拢伸直后在一个短暂的滑行(1-2秒)。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062803.gif"></center>




<p>蛙泳的完整配合动作：双手外划时抬头换气，双手内划时收腿低头稍憋气，双手前伸过头时蹬腿吐气。</p>




<h2>自由泳</h2>


<p>游泳是全身运动，任何一个部位的活动都离不开全身的协调配合。从表面上看，自由泳依靠划水和打腿产生推进力，实际上，躯干的作用也不能忽视。首先，
躯干应保持一定的紧张度，腰部如果松软，整个人就像一摊泥一样。其次，身体的转动能够有效地发挥躯干部大肌肉群的力量，减少阻力，提高工作效果。</p>




<p>自由泳的完整配合有多种形式。一般常见的是每划水2次，打水6次，呼吸1次。</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062804.gif"></center>




<p>1、手的入水点在肩的延长线和身体中线之间，以大拇指领先，斜插入水。</br>
2、入水后，手、肘、肩继续前伸，使手臂伸展。随着身体的转动，屈腕、屈肘，手臂向外、后方抓水;手下划到最低点后，旋转手臂向内、上、后方划水，
保持高肘屈臂的划水姿势。</br>
3、手臂与水平面垂直时，经手领先，加速推水，手臂转为向外、向上和身后划水直到大腿侧，提肘出水。</br>
4、出水后，手臂自然、放松地经空中向前移臂，保持高肘姿势。然后手在肩前领先入水，开始下一个动作。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062805.gif"></center>




<p>1、手臂在水下成曲线划水路线，从侧面看，手相对于身体的划水轨迹为“S”形。</br>
2、自由泳两臂配合有前交叉配合、中交叉配合、和后交叉配合3种基本形式。本图为前交叉形式，为初学者比较容易掌握的方式。
</p>


<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062806.gif"></center>




<p>单臂打水划臂动作是初学者应该重点练习的动作。如此图，左臂划水，那么可以右臂扶板。一般腿打水10次左右，手臂划水一次。
掌握到一定程度的时候可以加上呼吸练习。</p>




<p>腿部鞭状打水：</br>
1、打腿动作从髋部开始发力，大腿带动小腿，做鞭状打水动作。</br>
2、向上打水腿从直到弯。以直腿开始向上打，脚接近水面时屈膝，小腿上抬，使脚牚露出水面后向下打水。开始可直腿打水，但腿略放松，不要僵硬，
在水的压力下腿会自然弯曲。向下打水前膝关节弯曲角度约130-160度，打水幅度约为30-40厘米。打水时要绷脚(芭蕾脚)，不要勾脚。
</p>




<h2>仰泳</h2>


<p>1、臂划水时，出水以大拇指领先，移臂时手臂与水面垂直，上臂贴近耳朵。移臂过程中手臂旋转，入水时小拇指领先插入水中。</br>
2、如果以头的位置为钟表12点，两手的入水点在11点和1点的位置。手入水后先直臂下划。</br>
3、两臂划水应与身体转动协调配合，两肩不断形成位置差。</br>
4、两臂划水配合采用中交叉方式，即两臂始终处于相反的位置，一臂划水时，另一臂移臂。</br>
5、头部保持稳定没有左右摆动。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062807.gif"></center>




<p>1、呼吸虽然不受限制，但最好采用有节奏的呼吸方式，或以固定在一臂移臂时吸气。毕竟划水以及身体在水中行进时会有波浪及水花。随意呼吸易呛水。</br>
2、保持水平的身体姿势，躯干和肩随手臂动作围绕纵轴转动，始终有一肩不露出水面。</br>
3、一般每划水2次，腿打水6次，呼吸1次。</br>
4、两腿交替做鞭状上下打水。向上打水要快而有力，脚略内旋并绷直，向下打水时腿和脚自然放松。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062808.gif"></center>




<p>1、移臂时手臂紧贴身体不能太宽。</br>
2、移臂时如果手臂易弯曲，则可暂时用小拇指领先出水，养成直臂出水的习惯以后再用大拇指领先出水。</br>
3、身体始终保持伸展、正直、几乎水平地仰卧于水面，好像平躺在床上，头下有一只矮枕头。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062809.gif"></center>




<p>建议：仰泳腿要体会大腿用力，上抬与下压都要有，体会大腿带动小腿的感觉。</p>




<h2>蝶泳</h2>


<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062810.gif"></center>


<p>1、蝶泳的手入水点在两肩的延长线上，以大拇指领先，斜插入水。</br>
2、入水后，肩、肘前伸，两手沿曲线向外、后、下方抓水。两手分开到肩宽时，屈肘，加速划水。</br>
3、两手分开到达到最大宽度后，手臂转为向内、向上和向后划水，手臂上抬时保持高肘屈臂。两手在胸下或腹下时，手之间的距离最近。</br>
4、呼吸与划水的配合也是蝶泳技术的关键。手臂结束向内划水时，头露出水面吸气，移臂时头还原入水。记住两个“之前”，即头在手出水前出水，在手入水前入水。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062811.gif"></center>




<p>1、蝶泳双手划水两手距离接近最近时，手臂划水的方向再一次改变，转为向外、向上和向后划水，直至出水。</br>
2、划水出水后，手臂在肩的带动下经空中向前移臂，准备入水、移臂一般以低、平、放松的姿势从两侧前移。</br>
3、蝶泳的身体姿势掌握比较难，同时鞭状打水也不易掌握。在蝶泳学习的时候，我们会有专门的分解练习让您逐步掌握运作。
</p>




<center><img src="http://ibillxia.github.io/images/2013/IMAG2013062812.gif"></center>




<p>1、蝶泳的划水路线一般为“钥匙孔”形，指两手在胸下或腹下时的距离最近，这种前后划水路线比较均匀。</br>
2、注意蝶泳的四肢动作是双臂、双腿同时协调发力。
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Windows API实现一个简单的录音程序]]></title>
    <link href="http://ibillxia.github.io/blog/2013/06/04/a-simple-code-for-wave-recording-using-windows-api/"/>
    <updated>2013-06-04T23:59:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/06/04/a-simple-code-for-wave-recording-using-windows-api</id>
    <content type="html"><![CDATA[<p>本文介绍如何使用Windows API来录制语音信号兵保存到wave文件中，主要用到三个结构体和几个wave开头的API函数（在Winmm.lib文件中）。其中三个结构体是WAVEFORMATEX、WAVEHDR、MMTIME，其详细定义都在MMSystem.h中定义，
可以转到定义看其详细内容及每一项的英文注释。用到的API函数的详细用法可以参见MSDN： http://msdn.microsoft.com/en-us/library/windows/desktop/dd743847(v=vs.85).aspx
详细的使用过程请看下文的源代码，这是一个Win32 Application，需要手动添加Winmm.lib的依赖。</p>




<!--more-->




<p>实例程序</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>// ******************* FileName: WinMain.cpp *****************************
</span><span class='line'>// 该源程序需要加入到 VC6 的 Win32 Application 的 empty Project 中
</span><span class='line'>// 对于工程的 Link 选项，至少要包含以下库: msvcrt.lib Winmm.lib
</span><span class='line'>
</span><span class='line'>#include &lt;stdio.h>
</span><span class='line'>#include &lt;atlstr.h>
</span><span class='line'>#include &lt;windows.h>
</span><span class='line'>#include &lt;Mmsystem.h>
</span><span class='line'>
</span><span class='line'>#pragma comment(lib,"Winmm.lib")
</span><span class='line'>
</span><span class='line'>char lpTemp[256];
</span><span class='line'>
</span><span class='line'>DWORD FCC(LPSTR lpStr)
</span><span class='line'>{
</span><span class='line'>  DWORD Number = lpStr[0] + lpStr[1] *0x100 + lpStr[2] *0x10000 + lpStr[3] *0x1000000 ;
</span><span class='line'>  return Number;
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>int WINAPI WinMain( HINSTANCE hInstance, HINSTANCE hPrevInstance, LPSTR lpCmdLine, int nCmdShow )
</span><span class='line'>{
</span><span class='line'>  DWORD datasize = 48000;
</span><span class='line'>    
</span><span class='line'>  // 设置录音采样参数
</span><span class='line'>  WAVEFORMATEX waveformat;
</span><span class='line'>  waveformat.wFormatTag=WAVE_FORMAT_PCM; // 指定录音格式
</span><span class='line'>  waveformat.nChannels=1;
</span><span class='line'>  waveformat.nSamplesPerSec=8000;
</span><span class='line'>  waveformat.nBlockAlign=1;
</span><span class='line'>  waveformat.wBitsPerSample=8;
</span><span class='line'>  waveformat.cbSize=0;
</span><span class='line'>  waveformat.nAvgBytesPerSec=waveformat.nChannels*waveformat.nSamplesPerSec*waveformat.wBitsPerSample/8;
</span><span class='line'>  
</span><span class='line'>  sprintf(lpTemp,"WAVEFORMATEX size = %lu", sizeof(WAVEFORMATEX));
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  HWAVEIN m_hWaveIn;
</span><span class='line'>  if ( !waveInGetNumDevs() )
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("没有可以使用的 WaveIn 通道"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  // 打开录音设备
</span><span class='line'>  int res = waveInOpen(&m_hWaveIn,WAVE_MAPPER, &waveformat, (DWORD)NULL,0L,CALLBACK_WINDOW); 
</span><span class='line'>  if ( res != MMSYSERR_NOERROR )
</span><span class='line'>  {
</span><span class='line'>     sprintf(lpTemp, "打开 waveIn 通道失败，Error_Code = 0x%x", res );
</span><span class='line'>     MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>     return 0;
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  WAVEHDR m_pWaveHdr;
</span><span class='line'>  m_pWaveHdr.lpData = (char *)GlobalLock( GlobalAlloc(GMEM_MOVEABLE|GMEM_SHARE, datasize) );
</span><span class='line'>  memset(m_pWaveHdr.lpData, 0, datasize );
</span><span class='line'>  m_pWaveHdr.dwBufferLength = datasize;
</span><span class='line'>  m_pWaveHdr.dwBytesRecorded = 0;
</span><span class='line'>  m_pWaveHdr.dwUser = 0;
</span><span class='line'>  m_pWaveHdr.dwFlags = 0;
</span><span class='line'>  m_pWaveHdr.dwLoops = 0;
</span><span class='line'>  sprintf( lpTemp, "WAVEHDR size = %lu", sizeof(WAVEHDR) );
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  // 准备内存块录音
</span><span class='line'>  int resPrepare = waveInPrepareHeader( m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR) ); 
</span><span class='line'>  if ( resPrepare != MMSYSERR_NOERROR) 
</span><span class='line'>  {
</span><span class='line'>      sprintf(lpTemp, "不能开辟录音头文件，Error_Code = 0x%03X", resPrepare );
</span><span class='line'>      MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  resPrepare = waveInAddBuffer( m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR) );
</span><span class='line'>  if ( resPrepare != MMSYSERR_NOERROR) 
</span><span class='line'>  {
</span><span class='line'>      sprintf(lpTemp, "不能开辟录音用缓冲，Error_Code = 0x%03X", resPrepare );
</span><span class='line'>      MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'> 
</span><span class='line'>  if (! waveInStart(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("开始录音"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("开始录音失败"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>  Sleep(30000);
</span><span class='line'>
</span><span class='line'>  MMTIME mmt;
</span><span class='line'>  mmt.wType = TIME_BYTES;
</span><span class='line'>  sprintf( lpTemp, "sizeof(MMTIME) = %d, sizeof(UINT) = %d", sizeof(MMTIME), sizeof(UINT) );
</span><span class='line'>  MessageBox(NULL,CString(lpTemp),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  if (! waveInGetPosition(m_hWaveIn, &mmt, sizeof(MMTIME)) )
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("取得现在音频位置"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("不能取得音频长度"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (mmt.wType != TIME_BYTES) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("指定的 TIME_BYTES 格式音频长度不支持"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (! waveInStop(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("停止录音"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else  
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("停止录音失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  
</span><span class='line'>  if ( waveInReset(m_hWaveIn) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("重置内存区失败"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  m_pWaveHdr.dwBytesRecorded = mmt.u.cb;
</span><span class='line'>  DWORD NumToWrite=0;
</span><span class='line'>  DWORD dwNumber = 0;
</span><span class='line'>  HANDLE FileHandle = CreateFile( CString("myTest.wav"), GENERIC_WRITE, 
</span><span class='line'>      FILE_SHARE_READ, NULL, CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);
</span><span class='line'>
</span><span class='line'>  // memset(m_pWaveHdr.lpData, 0, datasize);
</span><span class='line'>  dwNumber = FCC("RIFF");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = m_pWaveHdr.dwBytesRecorded + 18 + 20;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("WAVE");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("fmt ");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = 18L;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  WriteFile(FileHandle, &waveformat, sizeof(WAVEFORMATEX), &NumToWrite, NULL);
</span><span class='line'>  dwNumber = FCC("data");
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  dwNumber = m_pWaveHdr.dwBytesRecorded;
</span><span class='line'>  WriteFile(FileHandle, &dwNumber, 4, &NumToWrite, NULL);
</span><span class='line'>  WriteFile(FileHandle, m_pWaveHdr.lpData, m_pWaveHdr.dwBytesRecorded, &NumToWrite, NULL);
</span><span class='line'>  SetEndOfFile(FileHandle);
</span><span class='line'>  CloseHandle( FileHandle );  
</span><span class='line'>  FileHandle = INVALID_HANDLE_VALUE; // 收尾关闭句柄
</span><span class='line'>  MessageBox(NULL,CString("应该已生成 myTest.wav 文件"),CString("提示"),MB_OK);
</span><span class='line'>
</span><span class='line'>  if ( waveInUnprepareHeader(m_hWaveIn, &m_pWaveHdr, sizeof(WAVEHDR)) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Un_Prepare Header 失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Un_Prepare Header 成功"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if ( GlobalFree(GlobalHandle( m_pWaveHdr.lpData )) ) 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Global Free 失败"),CString("提示"),MB_OK);
</span><span class='line'>  }
</span><span class='line'>  else 
</span><span class='line'>  {
</span><span class='line'>      MessageBox(NULL,CString("Global Free 成功"),CString("提示"),MB_OK);
</span><span class='line'>      return 0;
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  if (res == MMSYSERR_NOERROR ) // 关闭录音设备
</span><span class='line'>  {
</span><span class='line'>      if (waveInClose(m_hWaveIn)==MMSYSERR_NOERROR)
</span><span class='line'>      {
</span><span class='line'>          MessageBox(NULL,CString("正常关闭录音设备"),CString("提示"),MB_OK);
</span><span class='line'>      }
</span><span class='line'>      else
</span><span class='line'>      {
</span><span class='line'>          MessageBox(NULL,CString("非正常关闭录音设备"),CString("提示"),MB_OK);
</span><span class='line'>          return 0;
</span><span class='line'>      }
</span><span class='line'>  }
</span><span class='line'>
</span><span class='line'>  return 0;
</span><span class='line'>}
</span><span class='line'>// ******************* End of File ************************</span></code></pre></td></tr></table></div></figure>




<p>这里提供的代码有点杂乱，现已整理成一个小的接口，并提供了一个简单的示例，放在GitHub上：https://github.com/ibillxia/Demo/tree/master/DemoSpeechRecord</p>




<p>参考：</br>
[1]MSDN: http://msdn.microsoft.com/en-us/library/windows/desktop/dd743586(v=vs.85).aspx</br>
[2]基于API的录音机程序: http://www.vckbase.com/index.php/wv/664
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高追踪及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization/"/>
    <updated>2013-05-29T21:37:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述</h2>


<p>在<a href="http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/">音高及其Python实现</a>一文
中，我们使用了简单的“观察法”来计算音高，这并不太难，但这并不有好而且费时费力，那么我们就想，如何通过分析和计算，使用算法来自动计算音高呢？
用算法让计算机自动抓取音高的过程，称为<b>音高追踪</b>(Pitch Tracking)。所得到的音高信息有如下一些应用：</br>
·旋律识别(Melody Recognition)：或称为“哼唱选歌”，也就是如何由使用者的哼唱，找出音乐资料库中间对应的歌。</br>
·汉语声调识别(Tone Recognition)：辨识使用者讲一句话时，每一个字的声调（一声、二声、三声、四声等）。</br>
·语音合成韵律分析(Prosody Analysis)中的音高分析：如何在合成语音时，使用最自然的音高曲线。</br>
·语音评分中的音调评分(Intonation Assessment)：如何评估使用者说话的语音，其音高曲线是否标准。</br>
·语音识别(Speech Recognition)：我们可以使用语句的音高来提高语音辨识的正确率。</br>
总而言之，音高追踪是语音信号处理中最基本也最重要的一个环节之一。
</p>




<h2>2.音高追踪的基本流程</h2>


<p>音高追踪的基本流程如下：</br>
(1)将整段音讯讯号切成音框（Frames），相邻音框之间可以重叠。</br>
(2)算出每个音框所对应的音高。</br>
(3)排除不稳定的音高值。（可由音量来筛选，或由音高值的范围来过滤。）</br>
(4)对整段音高进行平滑化，通常是使用「中位数滤波器」（Median Filters）。</br>
</p>




<!--more-->




<p>在切音框的过程中，我们允许左右音框的重叠，因此，我们定义「音框率」（Frame Rate）是每秒钟所出现的音框个数，如果取样频率是11025，音框长度是256 点，
重叠点数是84，那么音框率就是11025/(256-84) = 64，换句话说，我们的电脑要能够每秒钟处理64 个音框，才能达到实时处理的要求。如下图所示：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052901.png"></center>
</p>




<p>我们让音框重叠的目地，只是希望相邻音框之间的变化不会太大，使抓出来的音高曲线更具有连续性。但是在实际应用时，音框的重叠也不能太大，
否则会造成计算量的过大。一般有以下考虑：</br>
·音框长度至少必须包含2 个基本周期以上，才能显示语音的特性。已知人声的音高范围大约在50 Hz 至1000 Hz 之间，因此对于一个的取样频率，我们就可以计算出
音框长度的最小值。例如，若取样频率fs = 8000 Hz，那么当音高f = 50 Hz（例如男低音的歌声）时，每个基本周期的点数是fs/f = 8000/50 = 160，因此音框必须
至少是320 点；若音高是1000 Hz（例如女高音的歌声）时，每个基本周期的点数是8000/1000 = 8，因此音框必须至少是16 点。</br>
·音框长度也不能太大，太长的音框无法抓到音讯的特性随时间而变化的细微现象，同时计算量也会变大。</br>
·音框之间的重叠完全是看计算机的运算能力来决定，若重叠多，音框率就会变大，计算量就跟着变大。若重叠少（甚至可以不重叠或跳点），音框率就会变小，
计算量也跟着变小。</p>




<h2>3.音高追踪算法</h2>


<h4>3.1概述</h4>


<p>由一个音框计算出音高的方法很多，可以分为时域和频域两大类：</br>
<b>时域（Time Domain）</b></br>
·ACF: Autocorrelation function，自相关函数</br>
·AMDF: Average magnitude difference function，平均强度差分函数</br>
·SIFT: Simple inverse filter tracking</br>
<b>频域（Frequency Domain）</b></br>
·Harmonic product spectrum method</br>
·Cepstrum method</p>




<h4>3.2 ACF自相关函数</h4>


<p>首先，我们来看看ACF(Auto-Correlation Function，自相关函数)的概念。要计算音高，就得找出波形中的周期性，自相关函数的目的就是估算语音信号当前
帧与它的下一帧的相似性，其定义如下：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau}s(i)s(i+\tau)$</center>
其中$\tau$是一个延迟的时间间隔。在某个区间使得$acf(\tau)$取得最大值的那个$\tau$值就选为pitch的起止点，如下图所示：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052902.png"></center>
也就是说，我们将原始语音信号与其平移延迟信号的重叠（时间上重叠）部分进行内积运算，从而得到ACF。下面看一个具体的实例，其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'>def ACF(frame):
</span><span class='line'>    flen = len(frame)
</span><span class='line'>    acf = np.zeros(flen)
</span><span class='line'>    for i in range(flen):
</span><span class='line'>        acf[i] = np.sum(frame[i:flen]*frame[0:flen-i])
</span><span class='line'>    return acf
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>frameSize = 512
</span><span class='line'>idx1 = 10000
</span><span class='line'>idx2 = idx1+frameSize
</span><span class='line'>index1 = idx1*1.0 / framerate
</span><span class='line'>index2 = idx2*1.0 / framerate
</span><span class='line'>acf = ACF(waveData[idx1:idx2])
</span><span class='line'>#acf[0:10] = -acf[0]
</span><span class='line'>#acfmax = np.argmax(acf)
</span><span class='line'>#print(acfmax)
</span><span class='line'>#print(framerate*1.0/acfmax)
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(frameSize),waveData[idx1:idx2],'r')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(frameSize),acf,'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("ACF")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

程序运行结果如下图所示：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052903.png"></center>
</p>




<p>很显然，ACF的最大值出现在第一点，这一点作为起点在任何情况下都是已知的，我们需要知道是第二个波峰。我们可以将开始的一些点的ACF值设为负无穷（这里我
设为-acf[0]），这样可以找到第二个波峰的index为110（这一点称为pitch point，简称pp），那么对应的pitch为framerate/110 = 16000/110 = 145.455Hz.这个过程取消
程序中第32行起的4行注释即可。这样，我们就初步自动计算出了pitch了。</p>




<p>但是，细心的读者会发现，这里还有一个问题，那就是ACF曲线中前多少个点应该被置为负无穷？为简单起见，设人的pitch的范围为[40,1000](Hz)，那么pp的值应满足为：
$40 < \frac{fs}{pp} < 1000$，从而得到pp的范围：$\frac{fs}{1000} < pp < \frac{fs}{40}$。这样可以部分解决问题，对于某些情况可能结果并不一定正确。</p>

<p>另外还有一些对ACF的改进。一个主要的改进原因是，当$\tau$值变大是，两端信号的重叠部分逐渐变小，这样计算出来的ACF当然越来越小。一种改进是增加一个权值：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau} \frac{s(i)s(i+\tau)}{n-\tau}$.</center>
这种方法虽然解决了上面提到的问题，但又引入了一个新的问题，那就是，在$\tau$值较大时，计算出来的acf和pitch的差异可能很大，也即出现了不稳定。另一种改进是，
将$\tau$限制在半帧内，也即：
<center>$acf(\tau) = \sum_{i=0}^{n/2}s(i)s(i+\tau)$.</center>
但这样得到的acf只有一帧的一半，对于音高较低的信号就不利了，这时我们就得增大帧的长度，于是计算量也相应的增加了。</p>

<h4>3.3 NSDF</h4>
<p>ACF的范围是未知的，NSDF(normalized squared difference function)将ACF规整到[-1,1]之间，其定义的表达式如下：
<center>$nsdf(\tau) = \frac{2\sum s(i)s(i+\tau)}{\sum s^{2}(i) + \sum s^{2}(i+\tau)}$.</center>
</p>

<h4>3.4 AMDF</h4>
<p>AMDF (average magnitude difference function) 的定义如下：
<center>$amdf(\tau) = \sum_{i=0}^{n-1-\tau}|s(i)-s(i+\tau)|$.</center>
与ACF相反，这里用距离而不是相似度来计算，所以这里选取pitch point(简称pp)的标准是选最小值对应的index(实际代码中，为了与ACF进行统一，我对AMDF取了相反数)。
相应的，也有一些对AMDF这个函数的修正，如加权值、只是用前半帧等，另外还可以将AMDF与ACF结合，将ACF除以AMDF，得到的结果可以更容易找到pitch point。
</p>

<h4>3.5 Pitch Tracking</h4>
<p>能够正确计算pitch了，我们就可以对一段时序的信号进行pitch tracking了。</p>

<p>PS：另外，还有一些频域的音高追踪方法，将在后续文章中介绍。</p>

<h2>4.参考资料</h2>
<p>[1]Audio Signal Processing and Recognition, Chap 7: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/index.asp
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大白鼠听人话]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/24/cctv-news-rat-understand-what-human-says/"/>
    <updated>2013-05-24T22:59:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/24/cctv-news-rat-understand-what-human-says</id>
    <content type="html"><![CDATA[<p>最近一直忙着准备给媒体展示的音控大鼠机器人一不小心上了CCTV了，虽然自己感觉没什么了不起的，也不知道网络上是什么评论。
但既然上了CCTV，还是发博纪念一下吧</p>




<p>央视新闻视频链接：<a href="http://tv.cntv.cn/vodplay/e58c8785e00a4ead9b83dfae5b53f12a/860010-1102010100">浙江杭州最新科研成果：大白鼠听人话</a>
真没想到自己居然正面出境这么长时间。</p>




<p>杭州日报的记者写的新闻还挺生动的：<a href="http://hzdaily.hangzhou.com.cn/hzrb/html/2013-05/24/content_1501396.htm">“嫁接”了机器视觉的大白鼠在沙盘迷宫中寻觅阿汤哥的照片</a></p>




<p>PS：感谢CCTV，感谢杭州日报，感谢ZJU，感谢CCNT，感谢各位老师和同学，感谢生仪的给大鼠做开颅手术的两位mm，感谢各位在微博帮忙宣传和转发的各位同学！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-端点检测及Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/"/>
    <updated>2013-05-22T22:22:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection</id>
    <content type="html"><![CDATA[<h2>端点检测</h2>


<p>端点检测（End-Point Detection，EPD）的目标是要决定信号的语音开始和结束的位置，所以又可以称为Speech Detection或Voice Activity Detection（VAD）。
端点检测在语音预处理中扮演着一个非常重要的角色。</p>




<p>常见的端点检测方法大致可以分为如下两类：</br>
（1）时域（Time Domain）的方法：计算量比较小，因此比较容易移植到计算能力较差的嵌入式平台</br>
（a）音量：只使用音量来进行端检，是最简单的方法，但是容易对清音造成误判。另外，不同的音量计算方法得到的结果也不尽相同，至于那种方法更好也没有定论。</br>
（b）音量和过零率：以音量为主，过零率为辅，可以对清音进行较精密的检测。</br>
（2）频域（Frequency Domain）的方法：计算量相对较大。</br>
（a）频谱的变化性（Variance）：有声音的频谱变化较规律，可以作为一个判断标准。</br>
（b）频谱的Entropy：有规律的频谱的Entropy一般较小，这也可以作为一个端检的判断标准。
</p>




<p>下面我们分别从这两个方面来探讨端检的具体方法和过程。</p>




<!--more-->




<h2>时域的端检方法</h2>


<p>时域的端检方法分为只用音量的方法和同时使用音量和过零率的方法。只使用音量的方法最简单计算量也最小，我们只需要设定一个音量阈值，任何音量小于该阈值的帧
被认为是静音（silence）。这种方法的关键在于如何选取这个阈值，一种常用的方法是使用一些带标签的数据来训练得到一个阈值，使得误差最小。</p>




<p>下面我们来看看最简单的、不需要训练的方法，其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'>def findIndex(vol,thres):
</span><span class='line'>    l = len(vol)
</span><span class='line'>    ii = 0
</span><span class='line'>    index = np.zeros(4,dtype=np.int16)
</span><span class='line'>    for i in range(l-1):
</span><span class='line'>        if((vol[i]-thres)*(vol[i+1]-thres)&lt;0):
</span><span class='line'>            index[ii]=i
</span><span class='line'>            ii = ii+1
</span><span class='line'>    return index[[0,-1]]
</span><span class='line'>
</span><span class='line'>fw = wave.open('sunday.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>vol = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>threshold1 = max(vol)*0.10
</span><span class='line'>threshold2 = min(vol)*10.0
</span><span class='line'>threshold3 = max(vol)*0.05+min(vol)*5.0
</span><span class='line'>
</span><span class='line'>time = np.arange(0,nframes) * (1.0/framerate)
</span><span class='line'>frame = np.arange(0,len(vol)) * (nframes*1.0/len(vol)/framerate)
</span><span class='line'>index1 = findIndex(vol,threshold1)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index2 = findIndex(vol,threshold2)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>index3 = findIndex(vol,threshold3)*(nframes*1.0/len(vol)/framerate)
</span><span class='line'>end = nframes * (1.0/framerate)
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(time,waveData,color="black")
</span><span class='line'>plt.plot([index1,index1],[-1,1],'-r')
</span><span class='line'>plt.plot([index2,index2],[-1,1],'-g')
</span><span class='line'>plt.plot([index3,index3],[-1,1],'-b')
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.plot(frame,vol,color="black")
</span><span class='line'>plt.plot([0,end],[threshold1,threshold1],'-r', label="threshold 1")
</span><span class='line'>plt.plot([0,end],[threshold2,threshold2],'-g', label="threshold 2")
</span><span class='line'>plt.plot([0,end],[threshold3,threshold3],'-b', label="threshold 3")
</span><span class='line'>plt.legend()
</span><span class='line'>plt.ylabel('Volume(absSum)')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

其中计算音量的函数calVolume参见<a href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">
音量及其Python实现</a>一文。程序的运行结果如下图：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052201.png"></center>
</p>




<p>这里采用了三种设置阈值的方法，但这几种设置方法对所有的输入都是相同的，对于一些特定的语音数据可能得不到很好的结果，比如杂音较强、清音较多或音量
变化较大等语音信号，此时单一阈值的方法的效果就不太好了，虽然我们可以通过增加帧与帧之间的重叠部分，但相对而言计算量会比较大。下面我们利用一些更多的
特征来进行端点加测，例如使用过零率等信息，其过程如下：</br>
（1）以较高音量阈值($\tau _{u}$)为标准，找到初步的端点；</br>
（2）将端点前后延伸到低音量阈值($\tau _{l}$)处；</br>
（3）再将端点前后延伸到过零率阈值($\tau _{zc}$)处，以包含语音中清音的部分。</br>
这种方法需要确定三个阈值($\tau _{u}$,$\tau _{l}$,$\tau _{zc}$)，可以用各种搜寻方法来调整这三个参数。其示意图(参考[1])如下：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052202.png"></center>
我们在同一个图中绘制出音量和过零率的阈值图如下：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013052203.png"></center>
可以看到我们可以通过过零率的阈值来把错分的清音加入到语音部分来。上图使用到的阈值还是和音量的阈值选取方法相同，比较简单直接。
</p>




<p>另外，我们还可以连续对波形进行微分，再计算音量，这样就可以凸显清音的部分，从而将其正确划分出来，详见参考[1]。</p>




<h2>频域的端检方法</h2>


<p>有声音的信号在频谱上会有重复的谐波结构，因此我们也可以使用频谱的变化性（Variation）或Entropy来进行端点检测，可以参见如下链接：
http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/paper/endPointDetection/</p>




<p>总之，端点检测是语音预处理的重头戏，其实现方法也是五花八门，本文只给出了最简单最原始也最好理解的几种方法，这些方法要真正做到实用，还需要针对一些
特殊的情况在做一些精细的设置和处理，但对于一般的应用场景应该还是基本够用的。</p>




<h2>参考（References）</h2>


<p>
[1]EPD in Time Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdTimeDomain.asp?title=6-2%20EPD%20in%20Time%20Domain</br>
[2]EPD in Frequency Domain: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/epdFreqDomain.asp?title=6-3%20EPD%20in%20Frequency%20Domain
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音色及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/"/>
    <updated>2013-05-18T21:57:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization</id>
    <content type="html"><![CDATA[<h2>音色（Timbre）</h2>


<p>音色是一个很模糊的概念，它泛指语音的内容，例如“天书”这两个字的发音，虽然都是一声（即他们的音高应该是相同或接近的），
但由于音色不同，我们可以分辨这两个音。直觉而言，音色的不同，意味着基本波形的不同，因此我们可以用基本周期的波形来代表音色。
</p>




<p>若要从基本周期的波形来直接分析音色是一件很困难的事情。通常我们的做法是将每一个帧进行频谱分析（Spectral Analysis），算出一个
帧如何分解为不同频率的分量，然后才能进行对比或分析。在频谱分析中，最常用的方法就是快速傅里叶变换（Fast Fourier Transform，FFT），
这是一个相当常用的方法，可以讲在时域（Time Domain）的信号转换成频域（Frequency Domain）的信号，并进而知道每个频率的信号强度。</p>




<p>语谱图（Spectrogram）就是语音频谱图，一般是通过处理接收的时域信号得到频谱图，因此只要有足够时间长度的时域信号就可以(时间长度
为保证频率分辨率)。专业点讲，语谱图就是频谱分析视图，如果针对语音数据的话，叫语谱图。语谱图的横坐标是时间，纵坐标是频率，坐标点
值为语音数据能量，因而语谱图很好的表达了语音的音色随时间变化的趋势。有些经验丰富的人能够通过看语谱图而知道对应的语音信号的内容，
这种技术成为Spectrogram Reading。</p>




<!--more-->




<h2>Python绘制语谱图</h2>


<p>如果是用Matlab，绘制语谱图并不难，网上资料也一堆一堆的。但是，如果要想用Python来绘制呢？网上相关资料很少很少，万幸中找到了参考[4]，
但是，[4]中提供的程序是不能运行的，还需要安装几个库，特别是Audiolab这个，折腾了我好半天，最终安装了，但运行时发现这个audiolab根本无法
import进来，因为ms与numpy的版本有冲突，出现了什么“numpy.dtype does not appear to be the correct type object”，弄了好半天也没有解决，
后来才发现其实不需要audiolab也可以的，因为其实audiolab只是读取不同格式（扩展名）的语音文件的一个lib而已，并不涉及到绘制语谱图的东西。</p>




<p>
闲话少说了，上代码吧，其实看看这代码也挺简单的，就调一个matplotlib.pyplot.specgram()就可以了。

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>soundInfo = fw.readframes(-1)
</span><span class='line'>soundInfo = np.fromstring(soundInfo,np.int16)
</span><span class='line'>f = fw.getframerate()
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'>plt.subplot(211)
</span><span class='line'>plt.plot(soundInfo)
</span><span class='line'>plt.ylabel('Amplitude')
</span><span class='line'>plt.title('Wave from and spectrogram of aeiou.wav')
</span><span class='line'>
</span><span class='line'>plt.subplot(212)
</span><span class='line'>plt.specgram(soundInfo,Fs = f, scale_by_freq = True, sides = 'default')
</span><span class='line'>plt.ylabel('Frequency')
</span><span class='line'>plt.xlabel('time(seconds)')
</span><span class='line'>plt.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>程序运行的效果如下图：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013051801.png"></center>
虽然程序简单，但还有一些小bug，比如subplot(212)的xlabel和ylabel无法显示，这个问题暂时还没有解决。（更新：这个问题已解决，把mpp.show()放到
最后一行就可以了，顺便图也更新了）</p>




<p>另外，就是关于这个语谱图具体是如何绘制的，这一点涉及到FFT和短时能量的计算，短时能量在<a href="">前文中</a>
已经讲过了，这里不再赘述。关于FFT将在后续文章中讨论。</p>




<h2>参考（References）</h2>


<p>
[1]Timbre (音色): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureTimber.asp?title=5-5</br>
[2]Wiki - 音色: http://zh.wikipedia.org/wiki/音色</br>
[3]语谱图： http://blog.csdn.net/wuxiaoer717/article/details/6941339</br>
[4]How to plot spectrogram with Python：http://jaganadhg.freeflux.net/blog/archive/2009/07/23/how-to-plot-spectrogram-with-python.html
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.plot([index3,index3],[-1,1],'g')
</span><span class='line'>pl.plot([index4,index4],[-1,1],'g')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),waveData[10000:10512],'r')
</span><span class='line'>pl.plot([59,59],[-1,1],'b')
</span><span class='line'>pl.plot([169,169],[-1,1],'b')
</span><span class='line'>print(1/( (169-59)*1.0/framerate ))
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),waveData[15000:15512],'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观。例如这里的基频为：1/( (169-59)*1.0/framerate )=145.45Hz），
然后还可以计算半音（semitone，可以参见[2]），进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
·男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
·女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-过零率及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/"/>
    <updated>2013-05-15T21:44:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization</id>
    <content type="html"><![CDATA[<h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




<!--more-->




<h2>ZCR的Python实现</h2>


<p>ZCR的Python实现如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'>def ZeroCR(waveData,frameSize,overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = math.ceil(wlen/step)
</span><span class='line'>    zcr = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        #To avoid DC bias, usually we need to perform mean subtraction on each frame
</span><span class='line'>        #ref: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        zcr[i] = sum(curFrame[0:-1]*curFrame[1::]&lt;=0)
</span><span class='line'>    return zcr</span></code></pre></td></tr></table></div></figure>

</p>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算ZCR的代码如下：

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>str_data = fw.readframes(nframes)
</span><span class='line'>wave_data = np.fromstring(str_data, dtype=np.short)
</span><span class='line'>wave_data.shape = -1, 1
</span><span class='line'>#wave_data = wave_data.T
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate Zero Cross Rate
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 0
</span><span class='line'>zcr = ZeroCR(wave_data,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(wave_data)) * (1.0 / framerate)
</span><span class='line'>time2 = np.arange(0, len(zcr)) * (len(wave_data)/len(zcr) / framerate)
</span><span class='line'>pl.subplot(211)
</span><span class='line'>pl.plot(time, wave_data)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(212)
</span><span class='line'>pl.plot(time2, zcr)
</span><span class='line'>pl.ylabel("ZCR")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>

</p>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013051502.png"></center>
</p>




<h2>参考（References）</h2>


<p>
[1]Zero Crossing Rate (過零率): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureZeroCrossingRate.asp?title=5-3%20Zero%20Crossing%20Rate%20(%B9L%B9s%B2v)&language=english</br>
[2]Wiki: http://zh.wikipedia.org/zh/过零率
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音量及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/"/>
    <updated>2013-05-15T19:36:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


<!--more-->


<p>音量计算的Python实现如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import math
</span><span class='line'>import numpy as np
</span><span class='line'>
</span><span class='line'># method 1: absSum
</span><span class='line'>def calVolume(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.median(curFrame) # zero-justified
</span><span class='line'>        volume[i] = np.sum(np.abs(curFrame))
</span><span class='line'>    return volume
</span><span class='line'>
</span><span class='line'># method 2: 10 times log10 of square sum
</span><span class='line'>def calVolumeDB(waveData, frameSize, overLap):
</span><span class='line'>    wlen = len(waveData)
</span><span class='line'>    step = frameSize - overLap
</span><span class='line'>    frameNum = int(math.ceil(wlen*1.0/step))
</span><span class='line'>    volume = np.zeros((frameNum,1))
</span><span class='line'>    for i in range(frameNum):
</span><span class='line'>        curFrame = waveData[np.arange(i*step,min(i*step+frameSize,wlen))]
</span><span class='line'>        curFrame = curFrame - np.mean(curFrame) # zero-justified
</span><span class='line'>        volume[i] = 10*np.log10(np.sum(curFrame*curFrame))
</span><span class='line'>    return volume</span></code></pre></td></tr></table></div></figure>




<p>对于给定语音文件aeiou.wav，利用上面的函数计算音量曲线的代码如下：</p>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import pylab as pl
</span><span class='line'>import numpy as np
</span><span class='line'>import Volume as vp
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('aeiou.wav','r')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># calculate volume
</span><span class='line'>frameSize = 256
</span><span class='line'>overLap = 128
</span><span class='line'>volume11 = vp.calVolume(waveData,frameSize,overLap)
</span><span class='line'>volume12 = vp.calVolumeDB(waveData,frameSize,overLap)
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, nframes)*(1.0/framerate)
</span><span class='line'>time2 = np.arange(0, len(volume11))*(frameSize-overLap)*1.0/framerate
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(time2, volume11)
</span><span class='line'>pl.ylabel("absSum")
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(time2, volume12, c="g")
</span><span class='line'>pl.ylabel("Decibel(dB)")
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure>




<p>运行以上程序得到下图：
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013051501.png"></center>
</p>




<h2>参考（References）</h2>


<p>[1]Volume (音量):http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeatureVolume.asp?title=5-2%20Volume%20(%AD%B5%B6q)</br>
[2]用Python做科学计算-声音的输入输出:http://hyry.dip.jp:8000/pydoc/wave_pyaudio.html</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理基础学习笔记之时域处理]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/08/speech-processing-in-time-domain/"/>
    <updated>2013-05-08T23:13:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/08/speech-processing-in-time-domain</id>
    <content type="html"><![CDATA[<p>语音信号的分析分为时域、频域、倒谱域等，时域分析简单、运算量小、物理意义明确，但对于语音识别而言，
更为有效的是频域的分析方法，那么为什么还有进行时域的分析呢？</p>




<p>语音信号具有时变特性，但在短时内可以看做是平稳的，所以语音的时域分析是建立在“短时”的条件下的，经研究统计，
语音信号在帧长为10ms~30ms内是相对平稳的。</p>




<p>语音信号是模拟信号，在进行处理之前，要进行数字化，模拟信号数字化的一般方法是采样，按照Nyquist采样定理进行
采样（一般在8K~10KHz）后，在进行量化（一般用8bit，也有16bit等）和编码，变为数字信号。</p>




<p>在语音信号数字化之后，就可以开始对其进行处理了，首先是预处理，由于语音信号的平均功率谱受声门激励和口鼻辐射的影响，
高频端大约在800Hz以上按6dB/倍频程跌落，为此要在预处理中进行预加重。预加重的目的是提升高频部分，是信号变得平坦，
以便于进行频谱分析或声道参数分析。预加重可以用具有6dB/倍频程的提升高频特性的预加重数字滤波器实现。预处理的另一
方面工作是分帧和加窗：分帧的帧长一般在10ms~30ms，分帧既可以是连续的，也可以是有部分over-lap；短时分析的实质是
对信号加窗，一般采用Hamming窗，其他的还有矩形窗、汉宁窗等，如下图所示。
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013050801.png"></center>
</p>




<!--more-->




<p>好了，经过预处理之后就可以真正开始进行时域分析了，这里的时域分析主要包含短时平均能量、短时过零分析、短时自相
关分析以及高阶统计量分析等。</p>




<p>短时平均能量（Short Time Average Energy）可以理解为先计算信号格采样值的平方，然后用一个移动窗h(n-m)选取出一个个
短时平方序列，并将各段的平方值求和，从而得到短时能量序列。短时平均能量（En）可以用来从清音中区分浊音（浊音的En比
清音大得多），可以用来确定声母和韵母、无声与有声、连字等的分界，还可以作为一种超音段信息用于语音识别。但短时平均
能量En对于高电平信号可能产生溢出，此时可以采用短时平均幅度（Short Time Average Magnitude）来度量语音信号幅度的变化。</p>




<p>信号的幅度值从正值到负值要经过零点，从负值到正值也要经过零点，称为过零，统计信号在单位时间（如1s）内过零的次数，
就成为过零率。如果信号按段分割，就成为短时，把各段信号的过零率做统计平均，就是短时平均过零率（Short Time Average Cross 
Zero Ratio）。短时平均过零率（Zn）可以作为“频率”来理解。过零率可以用来定量的分析清音/浊音，特别是在背景噪声电平较大时
更为有效（相比短时平均能量而言），有时还可以同时结合Zn和En来进行判定。</p>




<p>如果说短时平均过零率是描述复杂波形“频率”特征的一个参数，那么短时平均上升过零间隔（Short Time Rise Zero-Crossing Inteval）
就是描述复杂波形“周期”特性的参数。研究表明：在一定噪声背景下，该参数具有很好的稳健性，对不同的语音具有很好的差异性。</p>




<p>自相关函数是偶函数，语音信号的短时自相关函数（Short Time Autocorrelation Function）可以理解为序列[x(n)x(n-k)]通过一个
冲激响应为hk(n)的数字滤波器的输出，即有Rn(k) = [x(n)x(n-k)]*hk(n)。短时自相关函数是语音信号时域分析中的一个重要参量，但是
运算量很大。短时平均幅度差函数AMDF（Short Time Average Magnitude Difference Function）与自相关函数有类似的功效，但运算量
可降低许多，所以在语音信号处理中应用广泛。</p>




<p>最后是高阶统计量了。近来高阶统计量在语音信号处理中应用也越来越多，高阶统计量一般指高阶矩(Moment)、高阶累积量(Cumulant)以及
他们的谱——高阶矩谱和高阶累积量谱。首先定义了随机变量x的（第一）特征函数（也称为矩生成函数），实际为它的密度函数f(x)的傅里叶变换。
然后定义了第二特征函数（也称为累积量生成函数），它是第一特征函数的对数。还有随机变量x的k阶矩（mk）的定义，它是x的k次幂与f(x)的
乘积在x∈R上的积分。类似的还有k阶中心矩（μk）的定义，都与概率论中的定义差不多。现在，可以对第一、二特征函数进行泰勒展开，可以得
到ck（x的k阶累积量）和mk之间的一些关系，可以发现k<4时，ck=μk，此时ck的物理意义与μk的物理意义相同，而k>=4时，则不相等。对于c3，
描述了概率分布的对称性，通过定义一个新的概念——偏度（Skewness，也称为偏态系数）来衡量。对于c4，文中为了简化，假设了x的均值为0，
然后定义了一个称为峰态（也称峰度，Kurtosis）的概念，以表示分布相对于正太分布的尖锐或平坦程度。后面两小节分别对此进行了从单个
随机变量到多个随机变量的推广的分析和随机变量服从高斯分布（正态分布）的特殊情形做了分析。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[五一登高远足]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/01/go-hiking-International-Labour-Day/"/>
    <updated>2013-05-01T22:01:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/01/go-hiking-International-Labour-Day</id>
    <content type="html"><![CDATA[<p>五一天晴气爽，登高望远，强身健体！只可惜“不畏浮云遮望眼，只缘身在最高层” 这句诗在空气严重污染的今天已不适用了！</p>




<p><img src="http://ibillxia.github.io/images/2013/IMAG2013050101.jpg">

<!-- more -->

<img src="http://ibillxia.github.io/images/2013/IMAG2013050102.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050103.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050104.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050105.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050106.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050107.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050108.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050109.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050110.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050111.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050112.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050113.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050114.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050115.jpg">

<img src="http://ibillxia.github.io/images/2013/IMAG2013050116.jpg">
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Alize等工具构建说话人识别平台]]></title>
    <link href="http://ibillxia.github.io/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/"/>
    <updated>2013-04-26T22:07:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc</id>
    <content type="html"><![CDATA[<p>前段时间有好几位同学询问如何用Alize实现说话人识别的问题，由于寒假前赶Paper，来不及详细解答，更没时间写Demo。
开学后不久抽时间写了一个Demo，并上传到了GitHub：https://github.com/ibillxia/VoicePrintReco/tree/master/Demo</p>




<p>下面将利用Alize+SPro进行简单的GMM-Based的说话人识别的基本流程总结如下：</br>
1.Features extraction 特征提取</br>
sfbcep.exe（MFCC）或slpcep.exe（LPCC）</br>

2.Silence removal 静音检测和去除</br>
NormFeat.exe 先能量规整</br>
EnergyDetector.exe 基于能量检测的静音去除</br>

3.Features Normalization 特征规整</br>
NormFeat.exe 再使用这个工具进行特征规整</br>

4.World model training</br>
TrainWorld.exe 训练UBM</br>

5.Target model training</br>
TrainWorld.exe 在训练好UBM的基础上训练training set和testing set的GMM</br>

6.Testing</br>
ComputeTest.exe 将testing set 的GMM在training set的GMM上进行测试和打分</br>

7.Score Normalization</br>
ComputeNorm.exe 将得分进行规整</br>

8. Compute EER 计算等错误率</br>
你可以查查计算EER的matlab代码，NIST SRE的官网上有下载（http://www.itl.nist.gov/iad/mig//tools/DETware_v2.1.targz.htm）。</br>
</p>




<!--more-->




<p>关于各步骤中参数的问题，可以在命令行“工具 -help”来查看该工具个参数的具体含义，另外还可参考Alize源码中各个工具的test目录中提供的实例，
而关于每个工具的作用及理论知识则需要查看相关论文。</p>




<p>常见问题及解答: http://mistral.univ-avignon.fr/mediawiki/index.php/Frequently_asked_questions</p>




<p>更多问题请在Google论坛（https://groups.google.com/forum/?fromgroups=&hl=zh-CN#!forum/alize&#8212;voice-print-recognition）提出，大家一起讨论！</p>




<h3>推荐资料</h3>


<p>
[1] ALIZE - User Manual: http://mistral.univ-avignon.fr/doc/userguide_alize.001.pdf</br>
[2] LIA_SPKDET Package documentation: http://mistral.univ-avignon.fr/doc/userguide_LIA_SpkDet.002.pdf</br>
[3] Reference System based on speech modality ALIZE/LIA RAL: http://www-clips.imag.fr/geod/User/laurent.besacier/NEW-TPs/TP-Biometrie/tools/CommentsLBInstall/doc.pdf</br>
[4] Jean-Francois Bonastre, etc. ALIZE/SpkDet: a state-of-the-art open source software for speaker recognition</br>
[5] TOMMIE GANNERT. A Speaker Veri?cation System Under The Scope: Alize</br>
[6] Alize Wiki: http://mistral.univ-avignon.fr/mediawiki/index.php/Main_Page
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VALSE2013]]></title>
    <link href="http://ibillxia.github.io/blog/2013/04/22/VALSE2013/"/>
    <updated>2013-04-22T22:28:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/04/22/VALSE2013</id>
    <content type="html"><![CDATA[<h3>学术研讨</h3>


<p>VALSE是Vision And Learning SEminar的缩写，它主要目的是为计算机视觉、图像处理、模式识别与机器学习研究领域内的中国青年学者（以70后研发
人员为主）提供一个深层次学术交流的舞台。虽然参与会议和做报告的人主要是做视觉的，但很多问题是机器学习和模式识别当中的一般性问题，所以我这
个搞语音的也去打酱油了^_^。</p>




<p>今年的VALSE在南京东南大学召开，参加会议的人数超出预期，会场爆满，仅学校的老师和公司的研究人员就占了会场大半，学生沦落到只能座最后两排，
或者座分会场（这个太不科学了-_-!）。会程安排也很紧凑，中午几乎没有休息时间，吃饭都很赶，而下午也很晚（6点半左右）才结束。这次会议有好几个
perfect的报告，但也有些不太感兴趣的，有的甚至感觉很2。除了一些报告，还有两个主题讨论会，印象中主要包括三个论题：学术界与工业界的Gap及衔接
问题，深度学习是否是计算机视觉的终极解决方案，计算机视觉要不要从生物视觉机理中受启发等。</p>




<p>闲话少说，言归正传，数萝卜下窖的讲讲这两天的经历吧。
第一天上午，第一个做报告的是MSRA的张磊，主要讲了计算机视觉的一些基本问题，从AI的历史将起，提到了Turing Test，是人工智能
的Benchmark。而CV的一个基本问题是Object Recognition，人们的研究经历了从之前的Model Based到如今的Data Driven及Big Data的过程，各种模型和方法可谓
层出不穷，然而对于真正解决问题、真正达到人类一般的视觉智能，还相差甚远。接着他讲了关于在路灯下找钥匙的故事（详询http://tongyanyan.blog.edu.cn/2006/427512.html），
听了这个故事后，感觉那个找钥匙的人很滑稽可笑，然而再想想我们自己正在做的研究，是不是在某种程度上和故事中的这个人一样呢。通过这个故事，他引出自己
的观点：要想解决Object Recognition这个问题或者说要解决CV的问题，就需要More Effective Representation & Match。接下来讲在Representation方面一些研究
人员提出的一些人工设计的Feature，而在Match方面则从Point、Line、Plane、Volume（点线面体）进行了详尽的讲述。最后还提了一下Deep Neural Network在CV中的
应用，可以discover hidden patterns。虽然对CV中的很多概念和模型方法不太了解，但感觉还是挺有收获的。</p>




<p>上午的后两个报告都是讲Sparse的，虽然之前看过关于Sparse Coding的东西，但当他们在上面讲的，主要偏重与Sparse这个问题的优化求解方法及其变形，
涉及到很多数学公式和推导，感觉很枯燥，加之晚睡早起，有点犯困，所以基本没有听进去。贾佳亚的报告还似懂非懂，而陈欢欢的Sparse Bayesian Learning
表示完全没听懂。个人感觉Sparse还是很重要的，所以在弄完Deep Learning这个专题后，我想有必要对这两个报告及其相关论文再做深入的学习和研究。</p>


<!--more-->


<p>中午3个东南大学的同学请我们实验室的在他们学校食堂吃饭，虽然不太记得他们名字了，真心感谢他们！</p>




<p>下午第一个报告是高新波的IQA&VALSE，主要讲了图像质量评价的一些东西，虽然也提到了一些生物视觉方面的东西，感觉很没趣，基本没怎么听，打了个盹。
第二个报告是俞洪波的关于生物视觉方面的东西，很感兴趣，他从深层复杂网络结构、神经元、突触、离子通道、蛋白等多个层面上讲了视觉系统的信息处理流程，
后面还提到了视觉功能柱，指出了视觉神经元具有很强的选择性，不同部位的神经元对不同方向、距离的视觉信息具有选择性的激活增强，最后还讲了一些模拟
视觉系统的计算模型，并描述了一些实验，虽然对报告题目中的Self Organization Model到底是什么还不是很清楚，但对生物视觉系统有了更进一步的了解，
而且知道了他们是怎么获取神经元激活区域的。</p>




<p>下午的第2个Session的第一个报告是颜水成的Fashion Recommendation，包括Hair Style，Makeup，Clothing，Shoes等的Recommendation，不太感冒，只是对他重复提到
的关于华人做研究的一个问题深表同感，他说华人做研究其实很不错的，能在很多TOP会议期刊发Paper甚至Best Paper，但原创性的问题却很少，我们都在提高别人的Citation，
所以华人还需要在发现问题方面多下功夫，而不是仅仅在解决问题方面。后面两个报告一个是王亦洲的General Purpose Vision，表示没听懂。最后一个报告是王晓刚的Crowd 
Video Surveillance，主要是讲在Video中识别人并跟踪人的移动，或者统计视场中人的数量之类的，只是感性的了解了一下，印象里报告中好像没有提到什么具体的CV技术，
只是举了一个人体位置跟踪的例子，还有一个用在足球视屏中运动员跟踪的例子。</p>




<p>第二天上午第一个Session是两个报告，一个是陈小武的Image/Video/3D Scene Understanding and Editing，主要分以下四个方面：Illumination Learning and Synthesis，
Labeling and Lavering and Editing，Estimating 3D Model from a single Image，Video Event Representation and Inference，总体感觉讲的内容涉及到很多东西，甚至他的
学生不仅懂CV，还要懂美术、剪纸等，而且他们每年都会发CVPR、ICCV、ECCV之类的，感觉还挺NB的。另外一个报告是非常期待的于凯的关于深度学习和大数据的报告，但听了之后，
感觉有些Depressed，因为他的报告中没有涉及Deep Learning的一些细节的东西，诸如RBM的原理及其训练等，基本上只是泛泛而谈，之前对Deep Learning做了深入的调研和学习，
自我感觉Deep Learning也没什么神秘的，虽然对Gibbs采样和CD算法的理论还没有完全理解清楚，但我觉得Deep Learning更多的是一种思想方法，在Deep Architecture中，Knowledge
通过一层一层抽象和提取后，对于Classification、Clustering等任务具有更有效Representation，而且在Training Error非常小的情况下，还是可以再Testing中获得理想的Error Ratio，
相比Shallow Architecture，不存在模型Over Fitting的问题。另外，有人提到Deep Architecture中Layer数目的确定的问题，于凯的回答是，在Neural Networks中加一层后，进行Deep 
Learning的过程，如果相对于没加该层得到的Test Error更小，并且是非常有效的性能提升，那么就加进这一层。然后同样的，再加一层，再进行Deep Learning，以此类推。</p>




<p>上午的后一个Session是关于CV在Industry中的Application，先是来在Industry中的一些研究开发人员对他们目前的工作做一些简短的介绍，感觉某些公司有严重的广告嫌疑，很是讨厌。
然后是讨论阶段，各自就CV在学术界和工业界之间的Gap发表意见，总结起来主要有以下观点：一方面学术界与工业界的Gap是必要的，学术必须要超前，这样工业界才可能将其成熟的应用；
另一方面，学术界与工业界的Gap可以通过在工业界设置研究院（比如MSRA、百度最近在硅谷设置深度学习研究院之类），这样可以加快学术成果应用于工业界的进程，学术最终的目的就是
在工业界中发挥巨大作用，服务广大民众，给社会带来价值。</p>




<p>上午的Panel严重超时了，直到快1点了才结束，去餐馆吃饭，我们跟老板说我们下午要考试，让快点上菜，结果上菜速度果然飞快，而我们吃得也很快，基本上一盘菜一会儿就吃光，
真是高效啊，哈哈！</p>




<p>下午首先是一个Panel，讨论（更确切的说是辩论）了两个主题，一个是关于计算机视觉是否要借鉴吸收生物神经视觉的结果，另一个是Deep Learning是否是CV的终极解决方案，这两个辩论
都非常精彩，笑点不断。Panel开始之前，首先是两位报告者发言，首先上台的是 @老师木（袁进辉），他自我介绍了一番，然后讲了讲生物视觉与计算机视觉的紧密联系，认为计算机视觉要想
取得重大突破，就必须借鉴生物视觉的研究的发现。另外一位是李学龙老师，很有个性，只写了一张PPT，但发言时却如滔滔江水绵绵不绝，可以听得出，他对生物视觉也非常了解，也认为计算机
视觉必须借鉴生物视觉的一些研究成果。后面的讨论非常精彩，将学术娱乐化了。这两个论题本身就很具争议性，正反两方各执其词，要辩论出个是非来，还真需要真才实学。</p>




<p>Panel完之后是两个报告，一个是吴建鑫的Approximating Additive Kernel for Large Scale Vision Tasks，没怎么听懂。另一个是张敏灵的Multi-label Learning，感觉很没趣，主要是
觉得这并不是一个新问题，但在图像标注方面确实是一个很重要的问题。</p>




<p>最后，还可以从一些微博内容中获取更多关于VALSE2013的信息，可以搜索主题\#VALSE\# 或\#VALSE2013\#，或者关注 @潘布衣（会议Chair潘刚）、@张磊MSRA、@余凯_西二旗民工、
@老师木等等。。。</p>




<h3>游玩休闲</h3>


<p>我们周五下午6点多到，下雨了。坐地铁然后走到旅馆，吃晚饭就8点了，但还好雨也听了，我们就去附近的夫子庙、秦淮河逛了逛。
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042201.jpg"></center>
第二天晚上去阅江楼逛了逛，到哪儿才发现晚上关门，坑爹啊！不过在外面远眺夜晚的阅江楼也不错，然后走了一个把小时到南京长江大桥。
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042202.jpg"></center>
第三天晚上就待在住处。因为订的第四天下午6点多的票，所以白天就可以尽情的去玩玩了。第一站来到了中山园陵
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042203.jpg"></center>
只可惜周一不开放，被挡在“天下为公”的门外。原打算接下来要去的雨花台、大屠杀纪念馆也不开放，坑爹啊！
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042204.jpg"></center>
木办法，就在里面找了一个开放的十朝历史博物馆去了
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042205.jpg"></center>
然后去总统府了，就在外面看了看
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042206.jpg"></center>
再然后去玄武门，一到哪儿就又下雨了
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042207.jpg"></center>
进里面看了看玄武湖，然后就直接回旅馆了
<center><img src="http://ibillxia.github.io/images/2013/IMAG2013042208.jpg"></center>
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习及其在语音方面的应用]]></title>
    <link href="http://ibillxia.github.io/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/"/>
    <updated>2013-04-17T22:43:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing</id>
    <content type="html"><![CDATA[<p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="http://ibillxia.github.io/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

]]></content>
  </entry>
  
</feed>
