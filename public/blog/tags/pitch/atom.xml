<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Pitch | Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/blog/tags/pitch/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2014-03-02T15:06:41+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高追踪及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization/"/>
    <updated>2013-05-29T21:37:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/29/audio-signal-processing-time-domain-pitch-tracking-and-python-realization</id>
    <content type="html"><![CDATA[<h2>1.概述</h2>


<p>在<a href="http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/">音高及其Python实现</a>一文
中，我们使用了简单的“观察法”来计算音高，这并不太难，但这并不有好而且费时费力，那么我们就想，如何通过分析和计算，使用算法来自动计算音高呢？
用算法让计算机自动抓取音高的过程，称为<b>音高追踪</b>(Pitch Tracking)。所得到的音高信息有如下一些应用：</br>
·旋律识别(Melody Recognition)：或称为“哼唱选歌”，也就是如何由使用者的哼唱，找出音乐资料库中间对应的歌。</br>
·汉语声调识别(Tone Recognition)：辨识使用者讲一句话时，每一个字的声调（一声、二声、三声、四声等）。</br>
·语音合成韵律分析(Prosody Analysis)中的音高分析：如何在合成语音时，使用最自然的音高曲线。</br>
·语音评分中的音调评分(Intonation Assessment)：如何评估使用者说话的语音，其音高曲线是否标准。</br>
·语音识别(Speech Recognition)：我们可以使用语句的音高来提高语音辨识的正确率。</br>
总而言之，音高追踪是语音信号处理中最基本也最重要的一个环节之一。
</p>




<h2>2.音高追踪的基本流程</h2>


<p>音高追踪的基本流程如下：</br>
(1)将整段音讯讯号切成音框（Frames），相邻音框之间可以重叠。</br>
(2)算出每个音框所对应的音高。</br>
(3)排除不稳定的音高值。（可由音量来筛选，或由音高值的范围来过滤。）</br>
(4)对整段音高进行平滑化，通常是使用「中位数滤波器」（Median Filters）。</br>
</p>




<!--more-->




<p>在切音框的过程中，我们允许左右音框的重叠，因此，我们定义「音框率」（Frame Rate）是每秒钟所出现的音框个数，如果取样频率是11025，音框长度是256 点，
重叠点数是84，那么音框率就是11025/(256-84) = 64，换句话说，我们的电脑要能够每秒钟处理64 个音框，才能达到实时处理的要求。如下图所示：
<center><img src="/images/2013/IMAG2013052901.png"></center>
</p>




<p>我们让音框重叠的目地，只是希望相邻音框之间的变化不会太大，使抓出来的音高曲线更具有连续性。但是在实际应用时，音框的重叠也不能太大，
否则会造成计算量的过大。一般有以下考虑：</br>
·音框长度至少必须包含2 个基本周期以上，才能显示语音的特性。已知人声的音高范围大约在50 Hz 至1000 Hz 之间，因此对于一个的取样频率，我们就可以计算出
音框长度的最小值。例如，若取样频率fs = 8000 Hz，那么当音高f = 50 Hz（例如男低音的歌声）时，每个基本周期的点数是fs/f = 8000/50 = 160，因此音框必须
至少是320 点；若音高是1000 Hz（例如女高音的歌声）时，每个基本周期的点数是8000/1000 = 8，因此音框必须至少是16 点。</br>
·音框长度也不能太大，太长的音框无法抓到音讯的特性随时间而变化的细微现象，同时计算量也会变大。</br>
·音框之间的重叠完全是看计算机的运算能力来决定，若重叠多，音框率就会变大，计算量就跟着变大。若重叠少（甚至可以不重叠或跳点），音框率就会变小，
计算量也跟着变小。</p>




<h2>3.音高追踪算法</h2>


<h4>3.1概述</h4>


<p>由一个音框计算出音高的方法很多，可以分为时域和频域两大类：</br>
<b>时域（Time Domain）</b></br>
·ACF: Autocorrelation function，自相关函数</br>
·AMDF: Average magnitude difference function，平均强度差分函数</br>
·SIFT: Simple inverse filter tracking</br>
<b>频域（Frequency Domain）</b></br>
·Harmonic product spectrum method</br>
·Cepstrum method</p>




<h4>3.2 ACF自相关函数</h4>


<p>首先，我们来看看ACF(Auto-Correlation Function，自相关函数)的概念。要计算音高，就得找出波形中的周期性，自相关函数的目的就是估算语音信号当前
帧与它的下一帧的相似性，其定义如下：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau}s(i)s(i+\tau)$</center>
其中$\tau$是一个延迟的时间间隔。在某个区间使得$acf(\tau)$取得最大值的那个$\tau$值就选为pitch的起止点，如下图所示：
<center><img src="/images/2013/IMAG2013052902.png"></center>
也就是说，我们将原始语音信号与其平移延迟信号的重叠（时间上重叠）部分进行内积运算，从而得到ACF。下面看一个具体的实例，其代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'>def ACF(frame):
</span><span class='line'>    flen = len(frame)
</span><span class='line'>    acf = np.zeros(flen)
</span><span class='line'>    for i in range(flen):
</span><span class='line'>        acf[i] = np.sum(frame[i:flen]*frame[0:flen-i])
</span><span class='line'>    return acf
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>frameSize = 512
</span><span class='line'>idx1 = 10000
</span><span class='line'>idx2 = idx1+frameSize
</span><span class='line'>index1 = idx1*1.0 / framerate
</span><span class='line'>index2 = idx2*1.0 / framerate
</span><span class='line'>acf = ACF(waveData[idx1:idx2])
</span><span class='line'>#acf[0:10] = -acf[0]
</span><span class='line'>#acfmax = np.argmax(acf)
</span><span class='line'>#print(acfmax)
</span><span class='line'>#print(framerate*1.0/acfmax)
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(frameSize),waveData[idx1:idx2],'r')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(frameSize),acf,'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("ACF")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
程序运行结果如下图所示：
<center><img src="/images/2013/IMAG2013052903.png"></center>
</p>




<p>很显然，ACF的最大值出现在第一点，这一点作为起点在任何情况下都是已知的，我们需要知道是第二个波峰。我们可以将开始的一些点的ACF值设为负无穷（这里我
设为-acf[0]），这样可以找到第二个波峰的index为110（这一点称为pitch point，简称pp），那么对应的pitch为framerate/110 = 16000/110 = 145.455Hz.这个过程取消
程序中第32行起的4行注释即可。这样，我们就初步自动计算出了pitch了。</p>




<p>但是，细心的读者会发现，这里还有一个问题，那就是ACF曲线中前多少个点应该被置为负无穷？为简单起见，设人的pitch的范围为[40,1000](Hz)，那么pp的值应满足为：
$40 < \frac{fs}{pp} < 1000$，从而得到pp的范围：$\frac{fs}{1000} < pp < \frac{fs}{40}$。这样可以部分解决问题，对于某些情况可能结果并不一定正确。</p>

<p>另外还有一些对ACF的改进。一个主要的改进原因是，当$\tau$值变大是，两端信号的重叠部分逐渐变小，这样计算出来的ACF当然越来越小。一种改进是增加一个权值：
<center>$acf(\tau) = \sum_{i=0}^{n-1-\tau} \frac{s(i)s(i+\tau)}{n-\tau}$.</center>
这种方法虽然解决了上面提到的问题，但又引入了一个新的问题，那就是，在$\tau$值较大时，计算出来的acf和pitch的差异可能很大，也即出现了不稳定。另一种改进是，
将$\tau$限制在半帧内，也即：
<center>$acf(\tau) = \sum_{i=0}^{n/2}s(i)s(i+\tau)$.</center>
但这样得到的acf只有一帧的一半，对于音高较低的信号就不利了，这时我们就得增大帧的长度，于是计算量也相应的增加了。</p>

<h4>3.3 NSDF</h4>
<p>ACF的范围是未知的，NSDF(normalized squared difference function)将ACF规整到[-1,1]之间，其定义的表达式如下：
<center>$nsdf(\tau) = \frac{2\sum s(i)s(i+\tau)}{\sum s^{2}(i) + \sum s^{2}(i+\tau)}$.</center>
</p>

<h4>3.4 AMDF</h4>
<p>AMDF (average magnitude difference function) 的定义如下：
<center>$amdf(\tau) = \sum_{i=0}^{n-1-\tau}|s(i)-s(i+\tau)|$.</center>
与ACF相反，这里用距离而不是相似度来计算，所以这里选取pitch point(简称pp)的标准是选最小值对应的index(实际代码中，为了与ACF进行统一，我对AMDF取了相反数)。
相应的，也有一些对AMDF这个函数的修正，如加权值、只是用前半帧等，另外还可以将AMDF与ACF结合，将ACF除以AMDF，得到的结果可以更容易找到pitch point。
</p>

<h4>3.5 Pitch Tracking</h4>
<p>能够正确计算pitch了，我们就可以对一段时序的信号进行pitch tracking了。</p>

<p>PS：另外，还有一些频域的音高追踪方法，将在后续文章中介绍。</p>

<h2>4.参考资料</h2>
<p>[1]Audio Signal Processing and Recognition, Chap 7: http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/index.asp
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[语音信号处理之时域分析-音高及其Python实现]]></title>
    <link href="http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/"/>
    <updated>2013-05-16T23:10:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization</id>
    <content type="html"><![CDATA[<h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


<!--more-->


<p>其代码如下：
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import wave
</span><span class='line'>import numpy as np
</span><span class='line'>import pylab as pl
</span><span class='line'>
</span><span class='line'># ============ test the algorithm =============
</span><span class='line'># read wave file and get parameters.
</span><span class='line'>fw = wave.open('a.wav','rb')
</span><span class='line'>params = fw.getparams()
</span><span class='line'>print(params)
</span><span class='line'>nchannels, sampwidth, framerate, nframes = params[:4]
</span><span class='line'>strData = fw.readframes(nframes)
</span><span class='line'>waveData = np.fromstring(strData, dtype=np.int16)
</span><span class='line'>waveData = waveData*1.0/max(abs(waveData))  # normalization
</span><span class='line'>fw.close()
</span><span class='line'>
</span><span class='line'># plot the wave
</span><span class='line'>time = np.arange(0, len(waveData)) * (1.0 / framerate)
</span><span class='line'>
</span><span class='line'>index1 = 10000.0 / framerate
</span><span class='line'>index2 = 10512.0 / framerate
</span><span class='line'>index3 = 15000.0 / framerate
</span><span class='line'>index4 = 15512.0 / framerate
</span><span class='line'>
</span><span class='line'>pl.subplot(311)
</span><span class='line'>pl.plot(time, waveData)
</span><span class='line'>pl.plot([index1,index1],[-1,1],'r')
</span><span class='line'>pl.plot([index2,index2],[-1,1],'r')
</span><span class='line'>pl.plot([index3,index3],[-1,1],'g')
</span><span class='line'>pl.plot([index4,index4],[-1,1],'g')
</span><span class='line'>pl.xlabel("time (seconds)")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(312)
</span><span class='line'>pl.plot(np.arange(512),waveData[10000:10512],'r')
</span><span class='line'>pl.plot([59,59],[-1,1],'b')
</span><span class='line'>pl.plot([169,169],[-1,1],'b')
</span><span class='line'>print(1/( (169-59)*1.0/framerate ))
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>
</span><span class='line'>pl.subplot(313)
</span><span class='line'>pl.plot(np.arange(512),waveData[15000:15512],'g')
</span><span class='line'>pl.xlabel("index in 1 frame")
</span><span class='line'>pl.ylabel("Amplitude")
</span><span class='line'>pl.show()</span></code></pre></td></tr></table></div></figure></notextile></div>
</p>




<p>根据参考[1]，可以通过观察一帧的波形图来计算基音频率（感觉这种方法有点奇葩，不过很直观。例如这里的基频为：1/( (169-59)*1.0/framerate )=145.45Hz），
然后还可以计算半音（semitone，可以参见[2]），进而得到pitch与semitone的关系。[1]中还提到了钢琴的半音差，DS表示完全看不懂啊，有木有！！！</p>




<p>参考[2]中还简单介绍了如何改变音高、扩展音域，以及如何改变乐器的振动的弦的音高（通过改变弦长、张力、密度等），感兴趣的可以看看。</p>




<p>另外，由于生理结构的差异，男女性的音高范围不尽相同，一般而言：</br>
·男性的音高范围是35~72半音，对应的频率范围是62~523Hz；</br>
·女性的音高范围是45~83半音，对应的频率范围是110~1000Hz。</br>
然而，我们分辨男女的声音并不是只根据音高，还要根据音色（也即共振峰，下一篇文章中将详细介绍）。
</p>




<p>关于音高的计算，目前有很多种算法，具体将会在后续文章中详细介绍。</p>




<h2>参考（References）</h2>


<p>
[1]Pitch (音高): http://neural.cs.nthu.edu.tw/jang/books/audiosignalprocessing/basicFeaturePitch.asp</br>
[2]Wiki： http://zh.wikipedia.org/wiki/音高
</p>

]]></content>
  </entry>
  
</feed>
