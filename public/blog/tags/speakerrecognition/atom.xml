<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: SpeakerRecognition | Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/blog/tags/speakerrecognition/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2014-05-11T23:47:42+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[GMM-SVM-NAP Method in Speaker Recognition]]></title>
    <link href="http://ibillxia.github.io/blog/2012/08/13/GMM-SVM-NAP-Method-in-Speaker-Recognition/"/>
    <updated>2012-08-13T22:38:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/08/13/GMM-SVM-NAP-Method-in-Speaker-Recognition</id>
    <content type="html"><![CDATA[<h2>1.Gaussian Mixture Models(GMM)</h2>


<p>For a D-dimensional feature vector, x, the mixture density used for the likelihood function is deﬁned as</br>
<center>$p(x|\lambda) = \sum_{i=1}^{M}w_{i}p_{i}(x)$.</center></br>
The density is a weighted linear combination of $M$ unimodal Gaussian densities , $p_{i}(x)$</br>
<center>$p_{i}(x) = \frac{1}{(2\pi)^{D/2}|\sum _{i}|^{1/2}} exp[-\frac{1}{2}(x-\mu_{i})^{T}(\sum _{i})^{-1}(x-\mu_{i})]$.</center></br>
</p>




<h2>2.Support Vector Machines(SVM)</h2>


<p>An SVM is a two-class classifier constructed from sums of a kernel function $K(• , •)$</br>
<center>$f(x) = \sum_{i=1}{N}\alpha_{i} t_{i} K(x,x_{i}) + d$.</center></br>
$t_{i}$ are the ideal outputs,  $\sum_{i=1}{N}\alpha_{i} t_{i}=0$ and $\alpha_{i} > 0$,$x_{i}$ are support vectors and obtained 
from the training set by an optimization process.</br>
$K(. , .)$ is constrained to have certain properties (the Mercer condition), so that it can be expressed as</br>
<center>$K(x,y) = b(x)^{T} b(y)$.</center></br>
Kernel function examples: <a href="http://www.shamoxia.com/html/y2010/2292.html">http://www.shamoxia.com/html/y2010/2292.html</a>
</p>




<!-- more -->


<h2>3.GMM-SVM</h2>


<p>GMM Supervectors：given a speaker utterance, GMM-UBM training is performed by MAP adaptation of the means $m_{i}$, 
and we form a GMM supervector $m = [m_{i}]$</br>
<img src="/images/2012/IMAG2012081301.jpg"></br>
GMM Supervectors Linear Kernel: a natural distance between the two utterances is the KL divergence, while it does n't satisfy 
the Mercer condition, we consider the following approximation</br>
<center>$\begin{align}
D(g_{a}\parallel g_{b}) = &\int_{R^{n}}g_{a}xlog\left(\frac{g_{a}(x)}{g_{b}(x)} \right)dx \\
\leq & \sum_{i=1}^{N} \lambda _{i}D(N(.;m_{i}^{a},\Sigma _{i})\parallel N(.;m_{i}^{b},\Sigma _{i}))\\
= &\frac{1}{2}\sum_{i=1}^{N}\lambda_{i}(m_{i}^{a}-m_{i}^{b})\Sigma ^{-1}(m_{i}^{a}-m_{i}^{b})
\end{align}$</center></br>
we can define a new distance formula,</br>
<center>$d(m^{a},m^{b}) = \frac{1}{2} \sum_{i=1}^{N}\lambda_{i}(m_{i}^{a}-m_{i}^{b})\Sigma_{i}^{-1}(m_{i}^{a}-m_{i}^{b})$</center></br>
From the distance, we can find the corresponding inner product which is the kernel function.
</p>




<h2>4.SVM-NAP</h2>


<p>Nuisance Attribute Projection(NAP) attempts to remove the unwanted within-class variation of the observed feature vectors. 
This is achieved by applying the transform</br>
<center>$y' = P_{n}y = (I-V_{n}V_{n}^{T})y$.</center></br>
where Vn is the principal directions of within-class variability.
</p>




<p>The SVM NAP constructs a new kernel,
<center>$\begin{align}
K(m^{a},m^{b}) =& [Pb(m^{a})]^{T}[Pb(m^{b})]\\
=& b(m^{a})^{t}Pb(m^{b})\\
=& b(m^{a})^{t}(I-vv^{t})b(m^{b})
\end{align}$.</center></br>
where $P$ is a projection ($P^{2} = P$) , $v$ is the direction being removed from the SVM expansion 
space. $b(.)$ is the SVM expansion, and $\parallel v\parallel ^{2} = 1$.
</p>




<h2>5.Experiments on NIST SRE 2010</h2>


<p>In the experiments, I use the spro4 to extract MFCC and LPCC features, and use Alize to 
build GMM-UBM and all of the remained tasks.</p>




<h2>References</h2>


<p>
[1]Douglas A. Reynolds, T. F. Quatieri, and R. Dunn, Speaker verication using adapted Gaussian mixture models, Digital Signal Processing, vol. 10, no. 1-3, pp. 1941, 2000.</br>
[2]W. Campbell, “Generalized linear discriminant sequence kernels for speaker recognition,” in IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, 2002, pp. 161–164.</br>
[3]W. M. Campbell, D. E. Sturim, D. A. Reynolds, A. Solomonoff, SVM Based Speaker Verification Using a GMM Supervector Kernel and NAP Variability Compensation.</br>
[4]Robbie Vogt, Sachin Kajarekar, Sridha Sridharan, Discriminant NAP for SVM Speaker Recognition.
</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于主成分分析分类器的说话人识别]]></title>
    <link href="http://ibillxia.github.io/blog/2012/07/20/PCA-based-speaker-recognition/"/>
    <updated>2012-07-20T22:08:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/07/20/PCA-based-speaker-recognition</id>
    <content type="html"><![CDATA[<h2>1.说话人分类常用算法</h2>


<p>说话人分类是说话人识别中的一大主题，目前已经发展了很多方法，这些方法大致可以分为两类：</br>
(1)基于概率分布的高复杂度的精确分类算法，如高斯混合模型（Gaussian Mixture Model，GMM）、隐马尔科夫
模型（Hidden Markov Model，HMM）和支持向量机（Support Vector Machine，SVM）模型；</br>
(2)基于模板的低复杂度的分类算法，如向量量化方法（Vector Quantization，VQ）。</br>
第一类说话人分类算法能取得不错的效果，但他们的高复杂度使得进行说话人识别需要很长时间，不适合于实时应用。
而第二类说话人匪类算法虽然复杂度低，但当系统中说话人的人数增加时，识别性能下降得很快。</p>




<p>Wu Junwen，Zhang Xuegong提出了一种基于PCA的分类器（即主成分子空间（Principal Component Subspace，PCS）分类器），
作为一种线性分类器，其复杂度低于以上第一类算法，又表现出很好的分类能力。另外，我们还可以根据截断误差子空间来构造分类器，
称为TES（Truncation Error Subspace）分类器。这两种分类器反映了主成分分析方法的两个方面，结合两种分类器可得到混合分类器，
称其为P&T分类器。</p>




<!-- more -->


<h2>2.主成分分析的分类依据</h2>


<p>对语音信号进行特征提取后，将得到的数据进行主成分分析，经过训练确定主成分的个数m和经过主成分变换的特征数据（即主成分
子空间），同时也可得到截断误差子空间，主成分子空间或截断误差子空间即可作为描述说话人特征的模型。</p>




<p><strong>PCS分类器的基本思路</strong>是，对于一个待分类样本$X$，将它向数据集中每个说话人模型（即PCS）投影，即
计算$U_{k}^{T}(X-\mu _{k})$，其中$U_{k}$为数据集中第$k$个说话人的模型，$\mu _{k})$为每个模型的中心向量，定义投影
量所保留的原向量的方差为：</br>
<center>$f(k)=\parallel U_{k}^{T}(X-\mu _{k})\parallel ^{2}，k=1，2，…，n$.</center></br>
这个值即为投影量保留的方差之和。如果X投影到正确类的PCS，即同一说话人模型，那么该值将达到最大，而投影到其他说话人
模型上所保留的方差之和都比该值小。即有：</br>
<center>$x \in model${ $k|max(f(k)),1 < k \leq n $ }.</center></p>

<p><strong>TES分类器</strong>与PCS分类器类似，只是说话人模型是用TES来表示的，对应的说话人模型相似度的度量标准为：</br>
<center>$g(k)=\parallel V_{k}^{T}(X-\mu _{k})\parallel ^{2}, k=1, 2, ..., n$.</center></br>
其中$V_{k}$表示第$k$个说话人模型。而决策规则为：
<center>$x\in model$ { $k|min(g(k)),1 < k \leq n $ }.</center></p>

</p><strong>P&T分类器</strong>是将以上两种分类器结合起来进行决策，最简单的结合方法是线性组合，可以设PCS分类器的
权重为$\lambda （0\leq \lambda \leq 1）$，而TES分类器的权重为1-λ，则衡量说话人模型相似度的标准为：</br>
<center>$h(k)=\lambda f(k) - (1-\lambda)g(k), k=1, 2, ..., n$.</center></br>
而对应的决策规则为：</br>
<center>$x\in model$ { $k|min(h(k)),1 < k \leq n $ }.</center></p>

<p>在使用P&T分类器时要注意，在计算f(k)，g(k)之前，应先对PCS和TES进行归一化，以免某一个值太大而影响了另一个值的作用。</p>

<h2>3.主成分分类器应用于说话人识别</h2>
<p>根据说话人识别任务的特点，基于PCA的分类器需要解决两个问题：</br>
(1)训练说话人模型；</br>
(2)对测试音进行鉴别。</p>

<p>说话人鉴别的<strong>训练过程</strong>，就是训练每个说话人的模型参数，在此以PCS分类器为例进行说明。PCS分类器的模型参数由两部分组成，
即主成分主方向数m和主成分子空间$PCS=[U1，U2，…，Um]$。关于主成分的计算在前面的文章中讲过，就不在赘述，这里重点说说主方向
数确定的方法。为了使得PCS分类器的分类效果最好，我们可以选用少量的说话人以及他们的少量的测试语音，组成确定阈值的“验证
集（validation set）”，变动主方向数$m$，选取使得识别率最好时的主方向数$m$作为参数。为简单起见，我们令所有说话人的主成分
方向数取相同的值。</p>

<p>测试音鉴别：设数据库中共有M个说话人模型，${X(n)|n=1,2,…,N}$为某个测试语音上提取的特征向量序列，则模型相似度度量标准为：</br>
<center>$F(k) = \sum_{n=1}^{N} \parallel U_{k}^{T}(X(n)-\mu _{k}) \parallel ^{2}, k = 1,2,...,M$.</center></br>
那么，使得这个值最大的说话人即为识别结果。
</p>

<h2>4.实验结果</h2>
<p>分别在无噪语料库YOHO和有噪语料库PHONE做实验，得到如下结果：</br>
<center><img src="/images/2012/IMAG2012072001.jpg"></center></br>
<center><img src="/images/2012/IMAG2012072002.jpg"></center>
</p>

<h2>5.复杂度分析</h2>
<p>P&T分类器的复杂度</br>
模型训练阶段：</br>
$PTtrain = N*d+N*d+d*d*N+4/3d^{3}+O(d^{2})+O(dlogd) 
= O(N*d^{2})+O(d^{3})$
模型测试阶段：
$PTtest=M*Ntest*P*d+M*N*P+O(1)
=O(M*Ntest*P*d)$
</p>

<h2>6.小结</h2>
<p>由实验结果及复杂度分析可知，基于PCA分类器的说话人识别不仅具有很好的识别率，而且时间复杂度较低，
是比较理想的说话人识别分类器。</p>

<h2>参考文献</h2>
<p>[1]《说话人识别模型与方法》吴朝辉 杨迎春 著 清华大学出版社</br>
[2]基于PCA与LDA的说话人识别研究 章万峰 浙江大学硕士论文</p>

]]></content>
  </entry>
  
</feed>