<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Convex | Bill's Blog]]></title>
  <link href="http://ibillxia.github.io/blog/tags/convex/atom.xml" rel="self"/>
  <link href="http://ibillxia.github.io/"/>
  <updated>2014-05-11T23:58:41+08:00</updated>
  <id>http://ibillxia.github.io/</id>
  <author>
    <name><![CDATA[Bill Xia]]></name>
    <email><![CDATA[ibillxia@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[凸优化简介]]></title>
    <link href="http://ibillxia.github.io/blog/2012/09/26/convex-optimization-overview/"/>
    <updated>2012-09-26T23:03:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/09/26/convex-optimization-overview</id>
    <content type="html"><![CDATA[<p>在machine learning 的很多问题中，我们最终往往要求解某个函数的最优值。用数学术语表示就是，
给定一个函数 $f: R<sup>{n}</sup> \rightarrow R$，求 $ x \in R<sup>{n}</sup> $使得$f(x)$ 取得最小（大）值。例如least-square,
logistic regression, linear regression, svm, etc. 这类问题统称为优化问题。</p>

<h2>1.引言</h2>


<p>在一般情况下，求解任意一个函数的全局最优值是很困难的。但是对于一种特定类型的函数——凸函数（convex function），
我们可以很有效的求解其全局最优值。这里的“有效”是指在实际问题求解中，能在多项式复杂度的时间里求解。
人们将这类函数的最值问题称为凸优化问题（Convex Optimal Problem）。下面我从凸集和凸函数讲起，然后
介绍凸优化的一般描述和典型问题举例。</p>




<h2>2.凸集及其实例</h2>


<p><strong>凸集的定义</strong>：一个集合$C$是凸集，当且仅当对任意$x,y\in C$和$\theta \in R$且$0\leq \theta \leq 1$，都有</br>
<center>$\theta x + (1-\theta)y \in C$.</center>
其几何意义在于，在集合C中任取两个点，连接两点的直线段上的任一点也在集合C中。下图是凸集和非凸集的例子：</br>
<img src="/images/201209026/IMAG20120902601.jpg">
</p>




<!-- more -->


<p><strong>凸集的实例</strong>：
以下列举几个简单的凸集实例</br>
（1）所有Rn。很显然，对任意给定的$x,y\in R^{n}$ 都有 $\theta x + (1-\theta)y \in R^{n}$。
（2）非负卦限，R^{n}_{+}。很显然也符合定义。</br>
（3）单位球。设$\parallel . \parallel$为$R^{n}$上的模（例如欧几里得空间的模为$\parallel x\parallel = sqrt(sum(x_{i}^{2}))$.）,
那么集合${x|\parallel x\parallel \leq 1}$是一个凸集。
</p>




<h2>3.凸函数及判定和Jensen不等式</h2>


<p>凸优化中的一个核心概念就是凸函数。</br>
<strong>凸函数定义</strong>：一个函数$f: R^{n} → R$是凸函数当且仅当其定义域（设为$D(f)$）是凸集，
且对任意的$x,y\in D(f)$和$\theta \in R$且$0\leq \theta \leq 1$，都有</br>
<center>$f(\theta x + (1-\theta)y \leq \theta f(x) + (1-\theta) f(y))$.</center>
设$f(x)$为一元函数，那么上式的几何意义在于，曲线上任意两点处的割线在函数曲线的上方，如下图所示：</br>
<img src="/images/2012/IMAG20120902602.jpg">
</p>




<p>常见的凸函数有指数函数（$f(x) = a^{x}，a>1$）、负对数函数（$f(x)=-log_{a}x，a>1，x>0$）、开口向上的二次函数等。</p>




<p><strong>凸函数第一判定定理</strong>：设函数$f: R^{n} → R$是一阶可导的，那么$f$是凸函数当且仅当对任意的$x,y \in D(f)$都有：
<center>$f(y) \geq f(x) + f'(x)(y-x)$</center>
其中$f(x) + f'(x)(y-x)$称为$f(x)$在$x$处的一阶近似。</p>




<p><strong>凸函数第二判定定理</strong>：设函数$f: R^{n} → R$是二阶可导的，那么$f$是凸函数当且仅当对任意的$x\in D(f)$都有：</p>


<center>$f''(x)  \succeq 0$.</center>


<p>其中，当$f''(x)$是矩阵时，符号‘$ \succeq $’表示半正定，而非一个个的不等式（在一维的情况下，相当于'$\geq$'；
在二维情况下，不是表示对所有的$i$和$j$都有$X_{ij} \geq 0$，而是表示$X$为半正定矩阵）。
</p></p>

<p><strong>Jensen不等式</strong>：我们先看凸函数的定义中的不等式:</br>
<center>$f(\theta x+(1-\theta y)) \leq \theta f(x) + (1-\theta)f(y), for 0\leq \theta \leq 1$.</center>
类似的可以将其推广到多个点的情况：</br>
<center>$f(\sum_{i=1}^{k}\theta_{i}x_{i}) \leq \sum_{i=1}{k}\theta_{i}f(x_{i}), for \sum_{i=1}^{k}\theta_{i} = 1, \theta_{i} \geq 0 \forall i$.</center>
因为上式中的和为1，可以将其看作为是概率密度，则上式又可写为：</br>
<center>$f(E[x]) \leq E[f(x)]$.</center>
这个不等式称为Jensen不等式。
</p>




<h2>4.凸优化问题举例</h2>


<p>有了凸集和凸函数的定义，现在我们重点讨论凸优化问题的求解方法。凸优化的一般描述为：</br>
<center>$minimize f(x)$,</center>
<center>$subject to x \in C$.</center>
其中$f(x)$为凸函数，$C$是一个凸集，这是不带约束条件的情况下的凸优化问题。对于带约束条件的问题而言，其一般描述为：</br>
<center>$minimize f(x)$,</center>
<center>$subject to g_{i}(x) \leq 0, i=1,2,...,m; h_{j}(x) = 0, j=1,2,...,p$.</center>
其中$f(x)$为凸函数，$g_{i}(x)$对所有的$i$均为凸函数，$h_{j}(x)$均为仿射函数。注意$g_{i}(x)$不等式中不等号的方向。
</p>




<p>凸问题中的全局优化：首先要分清楚什么是局部最优，什么是全局最优。局部最优是指在该最优值附近的点对应的函数值
都比该最优值大，而全局最优是指对可行域里所有点，其函数值都比该最优值大。对于凸优化问题，它具有一个很重要的特性，
那就是所有的局部最优值都是全局最优的（关于其证明这里就不讲了，感兴趣的可以自行查查资料或后文中的参考文献）。</p>




<h4>几类特殊的凸优化问题：</h4>


<p>（1）线性规划（Linear Programing, LP）: 目标函数和约束条件函数都是线性函数的情况，一般形式如下：</br>
<center>$minimize c^{T}x +d$,</center>
<center>$subject to Gx \preceq h; Ax = b$.</center>
</p>




<p>（2）二次规划（Quadratic Programing, QP）: 目标函数为二次函数，约束条件为线性函数，一般形式为：</br>
<center>$minimize 1/2 x^{T}Px + c^{T}x +d$,</center>
<center>$subject to Gx \preceq h; Ax = b$.</center>
LP可以看做是QP的特例，QP包含LP。
</p>




<p>（3）二次约束的二次规划（Quadratically Constrained Quadratic Programming, QCQP）: 目标函数和约束条件均为
二次函数的情况，一般形式为：</br>
<center>$minimize 1/2 x^{T}Px + c^{T}x +d$,</center>
<center>$subject to 1/2 x^{T}Qx + r^{T}x + s \preceq h, i=1,2,...,m; Ax = b$.</center>
QP可以看做是QCQP的特例，QCQP包含QP。
</p>




<p>半正定规划（Semideﬁnite Programming，SDP）: 其一般形式为：</br>
<center>$minimize tr(CX)$,</center>
<center>$subject to tr(A_{i}X) = b_{i},i=1,2,...,p; 0 \preceq X$.</center>
其中对称矩阵$X\inS^{n}$为决策变量，矩阵$C$，$A_{i}$均为对称矩阵，条件$0 \preceq X$的作用为约束$X$为半正定的。
QCQP可以看做是SDP的特例，SDP包含QCQP。SDP在machine learning中有非常广泛的应用。
</p>




<h2>5.凸优化应用举例</h2>


<p>下面我们来看几个实例。</br>
（1）支持向量机（Support Vector Machines，SVM）：凸优化在machine learning中的一个典型的应用就是基于支持向量机分类器，
它可以用如下优化问题表示：</br>
<center>$minimize 1/2 \parallel x \parallel ^{2} + C \xi _{i}$,</center>
<center>$subject to y^{(i)}(w^{T}x^{(i)}+b) \geq 1 - \xi _{i}, \xi_{i} \geq 0, i = 1,2,...,m$.</center>
其中决策变量$w\in R^{n}, \xi \in R^{n}, b \in R$. $C\in R, x(i), y(i), i=1,2, ... , m$由
具体问题定义。可以看出，这是一个典型的QP问题。
</p>




<p>（2）带约束的least squares问题：其一般描述为</br>
<center>$minimize 1/2\parallel Ax-b \parallel ^{2} $,</center>
<center>$subject to l \preceq x \preceq u$.</center>
这也是一个很典型的QP问题。
</p>




<p>（3）Maximum Likelihood for Logistic Regression：该问题的目标函数为:</br>
<center>$l (\theta) = \sum_{i=1}^{n}[y^{(i)}ln g(\theta^{T}x^{(i)}) + (1-y^{(i)})ln(1-g(\theta^{T}x^{(i)}))]$</center>
其中$g(z)$为Sigmoid函数，关于Logistic Regression请参见Andrew Ng的Machine Learning的第6讲。
</p>




<h2>6.凸优化问题求解简介</h2>


<p>上文中提到了几类特殊的凸优化问题，并举了几个应用实例，但并没有给出解法。对于凸优化问题，目前没有一个通用的解析式的
解决方案，但是我们仍然可以用非解析的方法来有效的求解很多问题。内点法（后续文章中会详细介绍）被证明是很好的解决方案，
特别具有实用性，在某些问题中，能够在多项式时间复杂度下，将解精确到指定精度。</p>




<p>我们将会看到，经过10到100次的迭代，内点法可以解决一般的凸优化问题，其中每次迭代的时间复杂度为</br>
<center>$max{n^{3},n^{2}m,F}$.</center>
其中$F$为计算目标函数和约束条件函数的一阶、二阶导数的总时间代价。如同用解析法求解线性规划问题，内点法也是非常可靠的，
在一般的PC机上，它可以在几十秒的时间内求解含有上百个变量、上千个约束条件的凸优化问题。对于一些特殊的结构（如稀疏的），
内点法可以求解包含上千个变量和约束条件的凸优化问题。
</p>




<p>对于一般的凸优化问题的求解，还没有像求解最小二乘和线性规划那么成熟的技术，基于内点法的一般凸优化问题求解依然是
当前的一个研究热点。虽然目前还没有公认的最好的解决方案，但我们有理由相信，在不久的将来，内点法求解一般的凸优化问题
是一项技术。事实上，对于一些特定的凸优化问题，如二次锥规划和几何规划问题（后续文章将会介绍），内点法在向一项技术迈进。</p>




<h2>参考文献</h2>


<p>
[1]Book: Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004</br>
[2]Slide: Zico Kolter (updated by Honglak Lee). Convex Optimization Overview. October 17, 2008</br>
[3]Standford Convex Optimization Course I：http://www.stanford.edu/class/ee364a/lectures.html
</p>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Log Sum Inequality]]></title>
    <link href="http://ibillxia.github.io/blog/2012/08/14/the-log-sum-inequanlity/"/>
    <updated>2012-08-14T21:42:00+08:00</updated>
    <id>http://ibillxia.github.io/blog/2012/08/14/the-log-sum-inequanlity</id>
    <content type="html"><![CDATA[<p>In  mathematics, the log sum inequality is very useful for proving several theorems in information theory.</p>




<h2>1.Statements</h2>


<p>Suppose ai and bi are nonnegative numbers. The log sum inequality states that</br>
<center>$\sum_{i=1}^{n}a_{i}log \frac{a_{i}}{b_{i}} \seq a log \frac{a}{b}$,</center>
where $a=\sum a_{i}$ and $b=\sum b_{i}$. The equality established if and only if $a_{i}/b{i}$ are equal for all $i$.
</p>




<h2>2.Proof</h2>


<h3>(1)Convex functioin</h3>


<p>In mathematics, a real-valued function F(x) defined on an interval is called convex (or convex downward or concave 
upward) if the graph of the function lies below the line segment joining any two points of the graph. Equivalently, a 
function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set.</p>




<!-- more -->


<p>The mathematical expressions are as follows,</br>
A real valued function $f: X → R$ defined on a convex set $X$ in a vector space is called convex if, for any two points 
$x_{1}$ and $x_{2}$ in $X$ and any $t \in [0,1]$,</br>
<center>$f(tx_{1}+(1-t)x_{2}) \leq tf(x_{1}) + (1-t)f(x_{2})$.</center>
The function is called strictly convex if remove the equal in the expression.</p>




<p>Examples of convex functions are the quadratic function $F(x)=x_{2}$ and the exponential function $F(x)=e^{x}$ for 
any real number $x$. For more examples and properties of this type of function, please refer to them on Wikipedia.</p>




<h3>(2)Jensen's inequality</h3>


<p>Jensen's inequality generalizes the statement that the secant line of a convex function lies above the graph of 
the function. There are also converses of the Jensen's inequality, which estimate the upper bound of the integral 
of the convex function.</p>




<p>In the context of probability theory, it is generally stated in the following form: if $X$ is a random variable   
and $\phi$ is a convex function, then</br>
<center>$\phi(E[X]) \leq E[\phi(X)]$.</center>
For a real convex function $\phi$ , numbers $x1, x2, ..., xn$ in its domain, and positive 
weights $ai$, Jensen's inequality can be stated as:</br>
<center>$\phi(\frac{\sum a_{i} x_{i}}{\sum a_{j}}) \leq \frac{a_{i}\phi (x_{i})}{\sum a_{j}}$.</center>

<h3>(3)The Log Sum Inequality</h3>
Notice that after setting $f(x) = xlog x$ we have</br>
<center>$\begin{align}
\sum_{i=1}^{n}a_{i}llog\frac{a_{i}}{b_{i}} =& \sum_{i=1}^{n}b_{i}f(\frac{a_{i}}{b_{i}}) = b\sum_{i=1}^{n}\frac{b_{i}}{b}f(\frac{a_{i}}{b_{i}}) \\
\geq & bf(\sum_{i=1}^{n}\frac{b_{i}}{b}\frac{a_{i}}{b_{i}}) = bf(\frac{1}{b}\sum_{i=1}^{n}a_{i}) = bf(\frac{a}{b})\\
= & alog \frac{a}{b}
\end{align}$.</center>

where the inequality follows from Jensen's inequality since  $\frac{b_{i}}{b} \geq 0, \sum _{i} \frac{b_{i}}{b} = 1$, 
and $f$ is convex.

<h2>3.Application</h2>
<p>The log sum inequality can be used to prove several inequalities in information theory such as Gibbs' inequality or 
the convexity of Kullback-Leibler divergence.</p>

<p>For example to prove Gibbs' inequality it is enough to substitute $p_{i}$s for $a_{i}$s, and $q_{i}$s for $b_{i}$s to get</br>
<center>$D_{KL}(P\parallel Q) \equiv \sum_{i=1}^{n}p_{i}log_{2}\frac{p_{i}}{q_{i}} \geq 1log\frac{1}{1} = 0$.</center>
</p>

<h2>References</h2>
<p>[1]Convex function, http://en.wikipedia.org/wiki/Convex_function</br>
[2]Jensen's inequality, http://en.wikipedia.org/wiki/Jensen%27s_inequality</br>
[3]The Log Sum Inequality, http://en.wikipedia.org/wiki/Log_sum_inequality
</p>

]]></content>
  </entry>
  
</feed>