
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Bill's Blog</title>
  <meta name="author" content="Bill Xia">

  
  <meta name="description" content="Yesterday is History, Tomorrow a Mystery, Today is a Gift, Thats why it's called the Present！">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ibillxia.github.io/blog/page/3">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/style.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Bill's Blog" type="application/atom+xml">
  <script type="text/javascript">
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}

$(document).bind('DOMNodeInserted', function(event) {
  addBlankTargetForLinks();
});
</script>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script type="text/javascript">
$(document).ready(function(){

	// hide #back-top first
	$("#back-top").hide();
	
	// fade in #back-top
	$(function () {
		$(window).scroll(function () {
			if ($(this).scrollTop() > 100) {
				$('#back-top').fadeIn();
			} else {
				$('#back-top').fadeOut();
			}
		});

		// scroll body to 0px on click
		$('#back-top a').click(function () {
			$('body,html').animate({
				scrollTop: 0
			}, 800);
			return false;
		});
	});

});
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39460228-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">Bill's Blog</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/tags">Tags</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://www.google.com/" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:ibillxia.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/">语音信号处理之时域分析-端点检测及Python实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-22T22:22:00+08:00" pubdate data-updated="true">May 22<span>nd</span>, 2013</time>
        
         | <a href="/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>端点检测</h2>


<p>端点检测（End-Point Detection，EPD）的目标是要决定信号的语音开始和结束的位置，所以又可以称为Speech Detection或Voice Activity Detection（VAD）。
端点检测在语音预处理中扮演着一个非常重要的角色。</p>




<p>常见的端点检测方法大致可以分为如下两类：</br>
（1）时域（Time Domain）的方法：计算量比较小，因此比较容易移植到计算能力较差的嵌入式平台</br>
（a）音量：只使用音量来进行端检，是最简单的方法，但是容易对清音造成误判。另外，不同的音量计算方法得到的结果也不尽相同，至于那种方法更好也没有定论。</br>
（b）音量和过零率：以音量为主，过零率为辅，可以对清音进行较精密的检测。</br>
（2）频域（Frequency Domain）的方法：计算量相对较大。</br>
（a）频谱的变化性（Variance）：有声音的频谱变化较规律，可以作为一个判断标准。</br>
（b）频谱的Entropy：有规律的频谱的Entropy一般较小，这也可以作为一个端检的判断标准。
</p>




<p>下面我们分别从这两个方面来探讨端检的具体方法和过程。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/">语音信号处理之时域分析-音色及其Python实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-18T21:57:00+08:00" pubdate data-updated="true">May 18<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>音色（Timbre）</h2>


<p>音色是一个很模糊的概念，它泛指语音的内容，例如“天书”这两个字的发音，虽然都是一声（即他们的音高应该是相同或接近的），
但由于音色不同，我们可以分辨这两个音。直觉而言，音色的不同，意味着基本波形的不同，因此我们可以用基本周期的波形来代表音色。
</p>




<p>若要从基本周期的波形来直接分析音色是一件很困难的事情。通常我们的做法是将每一个帧进行频谱分析（Spectral Analysis），算出一个
帧如何分解为不同频率的分量，然后才能进行对比或分析。在频谱分析中，最常用的方法就是快速傅里叶变换（Fast Fourier Transform，FFT），
这是一个相当常用的方法，可以讲在时域（Time Domain）的信号转换成频域（Frequency Domain）的信号，并进而知道每个频率的信号强度。</p>




<p>语谱图（Spectrogram）就是语音频谱图，一般是通过处理接收的时域信号得到频谱图，因此只要有足够时间长度的时域信号就可以(时间长度
为保证频率分辨率)。专业点讲，语谱图就是频谱分析视图，如果针对语音数据的话，叫语谱图。语谱图的横坐标是时间，纵坐标是频率，坐标点
值为语音数据能量，因而语谱图很好的表达了语音的音色随时间变化的趋势。有些经验丰富的人能够通过看语谱图而知道对应的语音信号的内容，
这种技术成为Spectrogram Reading。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/18/audio-signal-processing-time-domain-timbre-python-realization/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/">语音信号处理之时域分析-音高及其Python实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-16T23:10:00+08:00" pubdate data-updated="true">May 16<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>音高（Pitch）</h2>


<p>概念：音高（Pitch）是语音信号的一个很重要的特征，直觉上而言它表示声音频率的高低，这个频率是指基本频率（基频），也即基本周期的倒数。
若直接观察语音的波形，只要语音信号稳定，我们可以很容易的看出基本周期的存在。例如我们取一个包含256个采样点的帧，单独绘制波形图，就可以明显的
看到它的基本周期。如下图所示：
<center><img src="/images/2013/IMAG2013051601.png"></center>
其中最上面的波形为|a|的发音，中间的为上图中红色双竖线（位于语音区）所对应的帧的具体波形，而最下面的是上图中绿色双竖线（位于静音区）所
对应的帧的具体波形。很容易看到中间的波形具有明显的周期性。
</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/16/audio-signal-processing-time-domain-pitch-python-realization/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/">语音信号处理之时域分析-过零率及其Python实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-15T21:44:00+08:00" pubdate data-updated="true">May 15<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>过零率（Zero Crossing Rate）</h2>


<p>概念：过零率（Zero Crossing Rate，ZCR）是指在每帧中，语音信号通过零点（从正变为负或从负变为正）的次数。
这个特征已在语音识别和音乐信息检索领域得到广泛使用，是对敲击的声音的分类的关键特征。</p>




<p>ZCR的数学形式化定义为：
<center>$zcr = \frac{1}{T-1}\sum_{t=1}^{T-1}\pi\{s_{t}s_{t-1}<0\}$.</center>
其中$s$是采样点的值，$T$为帧长，函数$\pi\{A\}$在A为真是值为1，否则为0.
</p>




<p>特性：</br>
(1).一般而言，清音（unvoiced sound）和环境噪音的ZCR都大于浊音（voiced sound）；</br>
(2).由于清音和环境噪音的ZCR大小相近，因而不能够通过ZCR来区分它们；</br>
(3).在实际当中，过零率经常与短时能量特性相结合来进行端点检测，尤其是ZCR用来检测清音的起止点；</br>
(4).有时也可以用ZCR来进行粗略的基频估算，但这是非常不可靠的，除非有后续的修正（refine）处理过程。
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/15/audio-signal-processing-time-domain-ZeroCR-python-realization/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">语音信号处理之时域分析-音量及其Python实现</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-15T19:36:00+08:00" pubdate data-updated="true">May 15<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>1.概述（Introduction）</h2>


<p>本系列文主要介绍语音信号时域的4个基本特征及其Python实现，这4个基本特征是：</br>
(1)音量（Volume）；</br>
(2)过零率（Zero-Crossing-Rate）；</br>
(3)音高（Pitch）；</br>
(4)音色（Timbre）。
</p>




<h2>2.音量（Volume）</h2>


<p>音量代表声音的强度，可由一个窗口或一帧内信号振幅的大小来衡量，一般有两种度量方法：</br>
（1）每个帧的振幅的绝对值的总和：
<center>$volume = \sum_{i=1}^{n}|s_{i}|$.</center>
其中$s_{i}$为第该帧的$i$个采样点，$n$为该帧总的采样点数。这种度量方法的计算量小，但不太符合人的听觉感受。</br>
（2）幅值平方和的常数对数的10倍：
<center>$volume = 10 * log_{10}\sum_{i=1}^{n}s_{i}^{2}$.</center>
它的单位是分贝（Decibels），是一个对数强度值，比较符合人耳对声音大小的感觉，但计算量稍复杂。
</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/15/audio-signal-process-time-domain-volume-python-realization/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/08/speech-processing-in-time-domain/">语音信号处理基础学习笔记之时域处理</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-08T23:13:00+08:00" pubdate data-updated="true">May 8<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/05/08/speech-processing-in-time-domain/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>语音信号的分析分为时域、频域、倒谱域等，时域分析简单、运算量小、物理意义明确，但对于语音识别而言，
更为有效的是频域的分析方法，那么为什么还有进行时域的分析呢？</p>




<p>语音信号具有时变特性，但在短时内可以看做是平稳的，所以语音的时域分析是建立在“短时”的条件下的，经研究统计，
语音信号在帧长为10ms~30ms内是相对平稳的。</p>




<p>语音信号是模拟信号，在进行处理之前，要进行数字化，模拟信号数字化的一般方法是采样，按照Nyquist采样定理进行
采样（一般在8K~10KHz）后，在进行量化（一般用8bit，也有16bit等）和编码，变为数字信号。</p>




<p>在语音信号数字化之后，就可以开始对其进行处理了，首先是预处理，由于语音信号的平均功率谱受声门激励和口鼻辐射的影响，
高频端大约在800Hz以上按6dB/倍频程跌落，为此要在预处理中进行预加重。预加重的目的是提升高频部分，是信号变得平坦，
以便于进行频谱分析或声道参数分析。预加重可以用具有6dB/倍频程的提升高频特性的预加重数字滤波器实现。预处理的另一
方面工作是分帧和加窗：分帧的帧长一般在10ms~30ms，分帧既可以是连续的，也可以是有部分over-lap；短时分析的实质是
对信号加窗，一般采用Hamming窗，其他的还有矩形窗、汉宁窗等，如下图所示。
<center><img src="/images/2013/IMAG2013050801.png"></center>
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/08/speech-processing-in-time-domain/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/05/01/go-hiking-International-Labour-Day/">五一登高远足</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-01T22:01:00+08:00" pubdate data-updated="true">May 1<span>st</span>, 2013</time>
        
         | <a href="/blog/2013/05/01/go-hiking-International-Labour-Day/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>五一天晴气爽，登高望远，强身健体！只可惜“不畏浮云遮望眼，只缘身在最高层” 这句诗在空气严重污染的今天已不适用了！</p>




<p><img src="/images/2013/IMAG2013050101.jpg">

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/05/01/go-hiking-International-Labour-Day/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/">使用Alize等工具构建说话人识别平台</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-26T22:07:00+08:00" pubdate data-updated="true">Apr 26<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前段时间有好几位同学询问如何用Alize实现说话人识别的问题，由于寒假前赶Paper，来不及详细解答，更没时间写Demo。
开学后不久抽时间写了一个Demo，并上传到了GitHub：https://github.com/ibillxia/VoicePrintReco/tree/master/Demo</p>




<p>下面将利用Alize+SPro进行简单的GMM-Based的说话人识别的基本流程总结如下：</br>
1.Features extraction 特征提取</br>
sfbcep.exe（MFCC）或slpcep.exe（LPCC）</br>

2.Silence removal 静音检测和去除</br>
NormFeat.exe 先能量规整</br>
EnergyDetector.exe 基于能量检测的静音去除</br>

3.Features Normalization 特征规整</br>
NormFeat.exe 再使用这个工具进行特征规整</br>

4.World model training</br>
TrainWorld.exe 训练UBM</br>

5.Target model training</br>
TrainWorld.exe 在训练好UBM的基础上训练training set和testing set的GMM</br>

6.Testing</br>
ComputeTest.exe 将testing set 的GMM在training set的GMM上进行测试和打分</br>

7.Score Normalization</br>
ComputeNorm.exe 将得分进行规整</br>

8. Compute EER 计算等错误率</br>
你可以查查计算EER的matlab代码，NIST SRE的官网上有下载（http://www.itl.nist.gov/iad/mig//tools/DETware_v2.1.targz.htm）。</br>
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/26/building-speaker-recognition-system-using-alize-etc/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/22/VALSE2013/">VALSE2013</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-22T22:28:00+08:00" pubdate data-updated="true">Apr 22<span>nd</span>, 2013</time>
        
         | <a href="/blog/2013/04/22/VALSE2013/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>学术研讨</h3>


<p>VALSE是Vision And Learning SEminar的缩写，它主要目的是为计算机视觉、图像处理、模式识别与机器学习研究领域内的中国青年学者（以70后研发
人员为主）提供一个深层次学术交流的舞台。虽然参与会议和做报告的人主要是做视觉的，但很多问题是机器学习和模式识别当中的一般性问题，所以我这
个搞语音的也去打酱油了^_^。</p>




<p>今年的VALSE在南京东南大学召开，参加会议的人数超出预期，会场爆满，仅学校的老师和公司的研究人员就占了会场大半，学生沦落到只能座最后两排，
或者座分会场（这个太不科学了-_-!）。会程安排也很紧凑，中午几乎没有休息时间，吃饭都很赶，而下午也很晚（6点半左右）才结束。这次会议有好几个
perfect的报告，但也有些不太感兴趣的，有的甚至感觉很2。除了一些报告，还有两个主题讨论会，印象中主要包括三个论题：学术界与工业界的Gap及衔接
问题，深度学习是否是计算机视觉的终极解决方案，计算机视觉要不要从生物视觉机理中受启发等。</p>




<p>闲话少说，言归正传，数萝卜下窖的讲讲这两天的经历吧。
第一天上午，第一个做报告的是MSRA的张磊，主要讲了计算机视觉的一些基本问题，从AI的历史将起，提到了Turing Test，是人工智能
的Benchmark。而CV的一个基本问题是Object Recognition，人们的研究经历了从之前的Model Based到如今的Data Driven及Big Data的过程，各种模型和方法可谓
层出不穷，然而对于真正解决问题、真正达到人类一般的视觉智能，还相差甚远。接着他讲了关于在路灯下找钥匙的故事（详询http://tongyanyan.blog.edu.cn/2006/427512.html），
听了这个故事后，感觉那个找钥匙的人很滑稽可笑，然而再想想我们自己正在做的研究，是不是在某种程度上和故事中的这个人一样呢。通过这个故事，他引出自己
的观点：要想解决Object Recognition这个问题或者说要解决CV的问题，就需要More Effective Representation & Match。接下来讲在Representation方面一些研究
人员提出的一些人工设计的Feature，而在Match方面则从Point、Line、Plane、Volume（点线面体）进行了详尽的讲述。最后还提了一下Deep Neural Network在CV中的
应用，可以discover hidden patterns。虽然对CV中的很多概念和模型方法不太了解，但感觉还是挺有收获的。</p>




<p>上午的后两个报告都是讲Sparse的，虽然之前看过关于Sparse Coding的东西，但当他们在上面讲的，主要偏重与Sparse这个问题的优化求解方法及其变形，
涉及到很多数学公式和推导，感觉很枯燥，加之晚睡早起，有点犯困，所以基本没有听进去。贾佳亚的报告还似懂非懂，而陈欢欢的Sparse Bayesian Learning
表示完全没听懂。个人感觉Sparse还是很重要的，所以在弄完Deep Learning这个专题后，我想有必要对这两个报告及其相关论文再做深入的学习和研究。</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/22/VALSE2013/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/">深度学习及其在语音方面的应用</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-17T22:43:00+08:00" pubdate data-updated="true">Apr 17<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/4/">&larr; Older</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/2/">Newer &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section>
  <h2>Categories</h2>
    <ul id="category-list">
		<li><a href='/blog/categories/assp'>ASSP (15)</a></li><li><a href='/blog/categories/engineering'>Engineering (4)</a></li><li><a href='/blog/categories/intelligence'>Intelligence (4)</a></li><li><a href='/blog/categories/life'>Life (11)</a></li><li><a href='/blog/categories/linux'>Linux (3)</a></li><li><a href='/blog/categories/math'>Math (5)</a></li><li><a href='/blog/categories/prml'>PRML (12)</a></li><li><a href='/blog/categories/program'>Program (32)</a></li><li><a href='/blog/categories/technics'>Technics (7)</a></li><li><a href='/blog/categories/view'>View (7)</a></li>
	</ul>
</section>
<section>
  <h2>Tags</h2>
  <ul class="tag-cloud">
	<a style="font-size: 93%" href="/blog/tags/alize/">Alize</a>
<a style="font-size: 137%" href="/blog/tags/audio/">Audio</a>
<a style="font-size: 153%" href="/blog/tags/c/">C</a>
<a style="font-size: 93%" href="/blog/tags/convex/">Convex</a>
<a style="font-size: 112%" href="/blog/tags/deeplearning/">DeepLearning</a>
<a style="font-size: 93%" href="/blog/tags/htk/">HTK</a>
<a style="font-size: 146%" href="/blog/tags/life/">Life</a>
<a style="font-size: 93%" href="/blog/tags/love/">Love</a>
<a style="font-size: 93%" href="/blog/tags/machinelearning/">MachineLearning</a>
<a style="font-size: 159%" href="/blog/tags/neuralnetworks/">NeuralNetworks</a>
<a style="font-size: 93%" href="/blog/tags/nuance/">Nuance</a>
<a style="font-size: 93%" href="/blog/tags/optimization/">Optimization</a>
<a style="font-size: 153%" href="/blog/tags/php/">PHP</a>
<a style="font-size: 112%" href="/blog/tags/prim/">Prim</a>
<a style="font-size: 153%" href="/blog/tags/programtest/">ProgramTest</a>
<a style="font-size: 93%" href="/blog/tags/programing/">Programing</a>
<a style="font-size: 146%" href="/blog/tags/python/">Python</a>
<a style="font-size: 126%" href="/blog/tags/quicksort/">QuickSort</a>
<a style="font-size: 93%" href="/blog/tags/rbm/">RBM</a>
<a style="font-size: 93%" href="/blog/tags/speakerrecognition/">SpeakerRecognition</a>
<a style="font-size: 112%" href="/blog/tags/speech/">Speech</a>
<a style="font-size: 93%" href="/blog/tags/uml/">UML</a>
<a style="font-size: 93%" href="/blog/tags/vpr/">VPR</a>
<a style="font-size: 112%" href="/blog/tags/web/">Web</a>
<a style="font-size: 165%" href="/blog/tags/zju/">ZJU</a>

  </ul>
</section><section>
  <h2>Recent Comments</h2>
  <script type="text/javascript" src="http://ibillxia.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=0&avatar_size=32&excerpt_length=22"></script>
  <a href="http://disqus.com/">Powered by Disqus</a>
</section>

<section>
  <h2>Sina Weibo</h2>
  <ul id="weibo">
	<iframe 
		width="100%" 
		height="550" 
		class="share_self"  
		frameborder="0" 
		scrolling="no" 
		src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=2704795533&verifier=9551ab13&dpc=1">
	</iframe>
  </ul>
</section>

<section>
	<h2>Reading List</h2>
	<ul>
		<script type="text/javascript" src="http://www.douban.com/service/badge/65527470/?show=collection&amp;n=12&amp;columns=3" ></script>
	</ul>
</section>
<section>
  <h2>Copyleft</h2>
  <p align="center"><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png"></a></p>
  <p>Except where otherwise noted, content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a></p>
</section>
  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><p id = "back-top">
	<a href="#top"><span></span>Back to Top</a>
</p>
<hr>
<p>
  Copyright &copy; 2009 - 2014 - <a href="http://about.me/ibillxia">Bill Xia</a> -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> - Theme by <a href="http://twitter.github.com/bootstrap/">Twitter Bootstrap</a> </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ibillxia';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
