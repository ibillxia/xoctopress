
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Bill's Blog</title>
  <meta name="author" content="Bill Xia">

  
  <meta name="description" content="Yesterday is History, Tomorrow a Mystery, Today is a Gift, Thats why it's called the Present！">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ibillxia.github.io/blog/page/4">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/style.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Bill's Blog" type="application/atom+xml">
  <script type="text/javascript">
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}

$(document).bind('DOMNodeInserted', function(event) {
  addBlankTargetForLinks();
});
</script>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script type="text/javascript">
$(document).ready(function(){

	// hide #back-top first
	$("#back-top").hide();
	
	// fade in #back-top
	$(function () {
		$(window).scroll(function () {
			if ($(this).scrollTop() > 100) {
				$('#back-top').fadeIn();
			} else {
				$('#back-top').fadeOut();
			}
		});

		// scroll body to 0px on click
		$('#back-top a').click(function () {
			$('body,html').animate({
				scrollTop: 0
			}, 800);
			return false;
		});
	});

});
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39460228-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">Bill's Blog</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/tags">Tags</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://www.google.com/" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:ibillxia.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/06/Convolutional-Neural-Networks/">卷积神经网络（CNN）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-06T23:34:00+08:00" pubdate data-updated="true">Apr 6<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/06/Convolutional-Neural-Networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>1. 概述</h2>


<p>卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是<strong>非全连接</strong>的，
另一方面同一层中某些神经元之间的连接的<strong>权重是共享的</strong>（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物
神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。</p>




<p>卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他
形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野(receptive field)的概念，1984年日本学者Fukushima
基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。</p>




<p>神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有
位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima
将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面
得到了广泛的应用。</p>




<h2>2. CNN的结构</h2>


<p>卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：</br>
1 特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取<strong>局部特征</strong>。一旦一个特征被提取出来，
只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。</br>
2 特征映射。网络的每一个计算层都是由<strong>多个特征映射组</strong>成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下<strong>共享
相同的突触权值</strong>集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。</br>
3.子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他
形式的变形的敏感度下降的作用。</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/06/Convolutional-Neural-Networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/04/why-do-Fourier-transformation/">为什么要进行傅立叶变换</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-04T10:42:00+08:00" pubdate data-updated="true">Apr 4<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/04/why-do-Fourier-transformation/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>一、傅立叶变换的由来</h2>


<p>关于傅立叶变换，无论是书本还是在网上可以很容易找到关于傅立叶变换的描述，但是大都是些故弄玄虚的文章，太过抽象，
尽是一些让人看了就望而生畏的公式的罗列，让人很难能够从感性上得到理解，最近，我偶尔从网上看到一个关于数字信号处理
的电子书籍，是一个叫Steven W. Smith, Ph.D.外国人写的，写得非常浅显，里面有七章由浅入深地专门讲述关于离散信号的傅
立叶变换，虽然是英文文档，我还是硬着头皮看完了有关傅立叶变换的有关内容，看了有茅塞顿开的感觉，在此把我从中得到的
理解拿出来跟大家分享，希望很多被傅立叶变换迷惑的朋友能够得到一点启发，这电子书籍是免费的，有兴趣的朋友也可以从网
上下载下来看一下，URL地址是：http://www.dspguide.com/pdfbook.htm </p>




<p>要理解傅立叶变换，确实需要一定的耐心，别一下子想着傅立叶变换是怎么变换的，当然，也需要一定的高等数学基础，最基本
的是级数变换，其中傅立叶级数变换是傅立叶变换的基础公式。</p>




<h2>二、傅立叶变换的提出</h2>


<p>让我们先看看为什么会有傅立叶变换？傅立叶是一位法国数学家和物理学家的名字，英语原名是Jean Baptiste Joseph Fourier(1768-1830), 
Fourier对热传递很感兴趣，于1807年在法国科学学会上发表了一篇论文，运用正弦曲线来描述温度分布，论文里有个在当时具有争议性的决断：
任何连续周期信号可以由一组适当的正弦曲线组合而成。当时审查这个论文的人，其中有两位是历史上著名的数学家拉格朗日(Joseph Louis 
Lagrange, 1736-1813)和拉普拉斯(Pierre Simon de Laplace, 1749-1827)，当拉普拉斯和其它审查者投票通过并要发表这个论文时，拉格朗日
坚决反对，在近50年的时间里，拉格朗日坚持认为傅立叶的方法无法表示带有棱角的信号，如在方波中出现非连续变化斜率。法国科学学会屈服
于拉格朗日的威望，拒绝了傅立叶的工作，幸运的是，傅立叶还有其它事情可忙，他参加了政治运动，随拿破仑远征埃及，法国大革命后因会被
推上断头台而一直在逃避。直到拉格朗日死后15年这个论文才被发表出来。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/04/why-do-Fourier-transformation/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/30/back-propagation-neural-networks/">反向传播(BP)神经网络</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-30T21:37:00+08:00" pubdate data-updated="true">Mar 30<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/30/back-propagation-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前面几篇文章中对神经网络和深度学习进行一些简介，包括神经网络的发展历史、基本概念和常见的几种神经网络以及神经网络的学习方法等，
本文具体来介绍一下一种非常常见的神经网络模型——反向传播(Back Propagation)神经网络。</p>




<h2>1.概述</h2>


<p>BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文
<em><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Learning representations by back-propagating errors</a></em>
值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。
</p>




<p>BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的
输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断
调整网络的权值和阈值，使网络的误差平方和最小。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/30/back-propagation-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/27/learning-process-of-neural-networks/">神经网络的学习方法概述</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-27T23:51:00+08:00" pubdate data-updated="true">Mar 27<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/27/learning-process-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要讨论一下神经网络的一般学习方法，主要有error-correction learning，memory-based learning， Hebbian learning，competitive learning，
Boltzmann learning等。然后介绍一些学习的方式，如监督学习、非监督学习、强化学习等。最后是一些具体的应用领域和实际问题。</p>




<h2>1.Knowledge Representation</h2>


<p>好的学习方法固然重要，但知识的表示，直接影响到feature的表示，也是非常重要的，因此在正式讨论学习方法之前，我们首先谈谈知识的表示。
首先一个问题是，什么是知识？在PRML中我们有如下定义：</br>
<blockquote><p>Knowledge refers to stored information or models used by a person or machine to interpret, predict, and appropriately respond to the outside world.</p><footer><strong>Fischler and Firschein</strong> <cite>Intelligence: The Eye，the Brain and the Computer</cite></footer></blockquote>
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/27/learning-process-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/24/classes-of-neural-networks/">神经网络模型分类</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-24T23:07:00+08:00" pubdate data-updated="true">Mar 24<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/24/classes-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要介绍一下几种不同类型的神经网络模型，主要有前馈神经网络，反馈神经网络，自组织神经网络，随机神经网络</p>




<h2>1.前馈神经网络</h2>


<h4>1)自适应线性神经网络(Adaline)</h4>


<p>自适应线性神经网络（Adaptive Linear，简称Adaline) 是由威德罗（Widrow）和霍夫（Hoff）首先提出的。它与感知器的主要不同之处在于
其神经元有一个线性激活函数，这允许输出可以是任意值，而不仅仅只是像感知器中那样只能取0或1。它采用的是W—H学习法则，也称最小均方差(LMS)
规则对权值进行训练。自适应线性元件的主要用途是线性逼近一个函数式而进行模式联想。</p>




<h4>2)单层感知器</h4>


<p>单层感知器（Perceptron）是由美国计算机科学家罗森布拉特（F.Roseblatt）于1957年提出的。它是一个具有单层神经元的网络，由线性阈值
逻辑单元所组成。它的输入可以是非离散量，而且可以通过学习而得到，这使单层感知器在神经网络研究中有着重要的意义和地位：它提出了自组织、
自学习的思想，对能够解决的问题，有一个收敛的算法，并从数学上给出了严格的证明。</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/24/classes-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/20/basics-of-neural-networks/">神经网络简介</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-20T23:21:00+08:00" pubdate data-updated="true">Mar 20<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/20/basics-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Deep Learning的本质是多层的神经网络，因此在深入学习Deep Learning之前，有必要了解一些神经网络的基本知识。
本文首先对神经网络的发展历史进行简要的介绍，然后给出神经元模型的形式化描述，接着是神经网络模型的定义、特性，
最后是一些最新的进展等。关于神经网络的分类、学习方法、应用场景等将在后续文章中介绍。</p>




<h2>1.发展简史</h2>


<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts建立了神经网络和数学模型，称为MP模型。他们通过MP模型提出
了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。</br>
1945年，Von Neumann在成功的试制了存储程序式电子计算机后，他也对人脑的结构与存储式计算机进行的根本区别的比较，还提出了以简单神经元构成的自再生自动机网络结构。</br>
1949年，心理学家D.O.Heb提出了突触联系强度可变的设想，并据此提出神经元的学习准则——Hebb规则，为神经网络的学习算法奠定了基础。</br>
1958年，F.Rosenblatt提出了感知模型，该模型是由阈值神经元组成的，它试图模拟动物和人的感知和学习能力。</br>
1962年Widrow提出了自适应线性元件，这是一种连续的取值的线性网络，主要用于自适应信号处理和自适应控制。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/20/basics-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/16/introduction-to-deep-learning/">深度学习简介</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-16T21:36:00+08:00" pubdate data-updated="true">Mar 16<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/16/introduction-to-deep-learning/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>0.概述</h2>

<p>
以下是Wiki上对深度学习的下的定义：</br>
Deep learning refers to a sub-field of machine learning that is based on learning several levels of representations, 
corresponding to a hierarchy of features or factors or concepts, where higher-level concepts are defined from lower-level ones, 
and the same lower-level concepts can help to define many higher-level concepts.
</p>


<p>
深度学习就是学习多个级别的表示和抽象，帮助理解数据，如图像、声音和文本。深度学习的概念源于人工神经网络的研究，
含多隐层的多层感知器就是一种深度学习结构。那些涉及从输入产生输出的计算,我们可以用流程图来表示，
流程图的一个特殊的概念就是它的深度: 从输入到输出的路径的最长长度。传统的前馈神经网络可以理解为
深度等于层数(隐层数+1)的网络。深度学习通过组合低层特征形成更加抽象的高层表示（属性类别或特征），
以发现数据的分布式特征表示。
</p>


<h2>1.深度学习产生的背景</h2>

<h3>1.1深度不够的缺陷</h3>

<p>
在很多情况下，深度为2就已足以在给定精度范围内表示任何函数了，例如逻辑门、正常
神经元、sigmoid-神经元、SVM中的RBF(Radial Basis Function)等，但这样也有一个代价：
那就是图中需要的节点数会很多，这也就意味着当我们学习目标函数时，需要更多的计算
单元和更多的参数。理论结果显示，对于某一类函数，需要的参数的个数与输入的大小是
成指数关系的，逻辑门、正常神经元、RBF单元就属于这类。后来Hastad发现，当深度为d时，
这类函数可以用O(n)个节点（输入为n个）的神经网络有效表示，但当深度被限制为d-1时，
则需要有O(n2)个节点来表示。
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/16/introduction-to-deep-learning/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/01/04/2013-first-snow/">2013年第一场雪</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-01-04T21:13:00+08:00" pubdate data-updated="true">Jan 4<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/01/04/2013-first-snow/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>元旦刚过就下了新年的第一场雪，这场雪下得还确实有点大，从昨天下午开始，今天下了一天了，现在积雪都有四、五寸厚了吧！
在南方持续这么长时间下这么大的雪很少见了，所以大家都很兴奋。下午上完了两节课后，大概3点的样子，我们实验室的六、七号人就
准备去爬宝石山，瞻保俶塔，观断桥残雪美景。</p>




<p>出发了，在校门口拍了几张照。
下面这张是正大门的全景：</br>
<img src="/images/2013/IMAG2013010401.jpg"></p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/01/04/2013-first-snow/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/01/02/ai-top-conferences/">AI 顶级会议列表</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-01-02T19:17:00+08:00" pubdate data-updated="true">Jan 2<span>nd</span>, 2013</time>
        
         | <a href="/blog/2013/01/02/ai-top-conferences/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>注: 本文为小百合BBS的daniel所写，稍有删改。</p>


<p>tier-1的列得较全, tier-2的不太全, tier-3的很不全.同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令人
尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的。</p>




<h3>The First Class</h3>


<p>今天先谈谈AI里面tier-1的conferences, 其实基本上就是AI里面大家比较公认的top conference. 下面同分的按字母序排列.</p>




<p>IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI实在太大, 所以虽然每届基本上能录100多篇（现在已经
到200多篇了），但分到每个领域就没几篇 了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难度很大. 
不过从录用率上来看倒不太低,基本上20%左右, 因为内行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会议
的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了PC的工作效
率（囧o(╯□╰)o）. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了减少
被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司的&#8221;IJCAI Inc.&#8221;主办的(当然实际上并不是公司, 实际上是
个基金会), 每次会议上要发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer& Thoughts Award, 前者是终身
成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年
科学家,每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外, IJCAI 的 PC member相当于其他会议的area chair, 权力很大, 
因为是由PC member去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇
文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位. （PS：一个非常好的消息是IJCAI-2013要来
中国Beijing了，非常感谢王飞跃老师等的辛勤的申办！但愿能够亲临现场！）。IJCAI-2013的Important dates：</br>
 Abstract submission: January 26, 2013 (11:59PM, UTC-12).</br>
 Paper submission: January 31, 2013 (11:59PM, UTC- 12).</br>
 Author feedback: March 4-6, 2013 (11:59PM, UTC-12).</br>
 Notification of acceptance/rejection: April 2, 2013.</br>
 Camera-ready copy due: Apr 23, 2013.</br>
 Technical sessions: August 3-9, 2013.</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/01/02/ai-top-conferences/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2012/12/21/human-audio-system-neural-parts/">人的听觉系统生理结构（2）——中枢部分</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-21T21:55:00+08:00" pubdate data-updated="true">Dec 21<span>st</span>, 2012</time>
        
         | <a href="/blog/2012/12/21/human-audio-system-neural-parts/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>0.概述</h2>


<p>大脑中与听觉相关的部分称为听觉中枢，它纵跨脑干、中脑、丘脑的大脑皮层，是感觉系统中最长的中枢通路之一。自下向上，
主要环节包括：蜗神经核、上橄榄核、外侧丘系核、下丘核、丘脑的内侧膝状体、大脑皮层颞叶的听觉皮层等，图1所示为听觉中枢
的传导通路。由中枢系统的多层传导过程，可以很自然的联想到近两年很热门的Deep Learning的机器学习方法。
<center><img src="/images/2012/IMAG2012122106.jpg"></center>
<center>图 1 听觉中枢传导通路</center>
</p>




<h2>1蜗神经核</h2>


<p>听神经纤维全部终止于蜗神经核，每条神经纤维可分为三个分支，分别支配耳蜗核的三个亚核，即背核、后腹核与前腹核。用微电极记录
单细胞电活动的方法证实，每个亚核都有各自的声音频率代表区（或称音调定位组合），高频分布在各亚核的背侧，即耳蜗底部投射在各亚核
的背上部；低频区分布在各亚核的腹侧，即耳蜗顶部投射在各亚核的腹下区。前腹侧核中的神经元主要是类本原神经元，它能够保存听觉神经
纤维中的时间-位置编码；后腹侧核中主要是建立和振荡反应类型的神经元，它们能够保存听觉神经纤维中的发放率-位置编码；背侧核中主要
是休止和累积反应类型的神经元，它们表现为非单调的发放率-强度关系。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2012/12/21/human-audio-system-neural-parts/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/5/">&larr; Older</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/3/">Newer &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section>
  <h2>Categories</h2>
    <ul id="category-list">
		<li><a href='/blog/categories/assp'>ASSP (15)</a></li><li><a href='/blog/categories/engineering'>Engineering (4)</a></li><li><a href='/blog/categories/intelligence'>Intelligence (4)</a></li><li><a href='/blog/categories/life'>Life (11)</a></li><li><a href='/blog/categories/linux'>Linux (3)</a></li><li><a href='/blog/categories/math'>Math (5)</a></li><li><a href='/blog/categories/prml'>PRML (12)</a></li><li><a href='/blog/categories/program'>Program (30)</a></li><li><a href='/blog/categories/technics'>Technics (7)</a></li><li><a href='/blog/categories/view'>View (7)</a></li>
	</ul>
</section>
<section>
  <h2>Tags</h2>
  <ul class="tag-cloud">
	<a style="font-size: 93%" href="/blog/tags/alize/">Alize</a>
<a style="font-size: 137%" href="/blog/tags/audio/">Audio</a>
<a style="font-size: 146%" href="/blog/tags/c/">C</a>
<a style="font-size: 93%" href="/blog/tags/convex/">Convex</a>
<a style="font-size: 112%" href="/blog/tags/deeplearning/">DeepLearning</a>
<a style="font-size: 93%" href="/blog/tags/htk/">HTK</a>
<a style="font-size: 146%" href="/blog/tags/life/">Life</a>
<a style="font-size: 93%" href="/blog/tags/love/">Love</a>
<a style="font-size: 93%" href="/blog/tags/machinelearning/">MachineLearning</a>
<a style="font-size: 159%" href="/blog/tags/neuralnetworks/">NeuralNetworks</a>
<a style="font-size: 93%" href="/blog/tags/nuance/">Nuance</a>
<a style="font-size: 153%" href="/blog/tags/php/">PHP</a>
<a style="font-size: 93%" href="/blog/tags/perceptron/">Perceptron</a>
<a style="font-size: 112%" href="/blog/tags/prim/">Prim</a>
<a style="font-size: 153%" href="/blog/tags/programtest/">ProgramTest</a>
<a style="font-size: 93%" href="/blog/tags/programing/">Programing</a>
<a style="font-size: 146%" href="/blog/tags/python/">Python</a>
<a style="font-size: 126%" href="/blog/tags/quicksort/">QuickSort</a>
<a style="font-size: 93%" href="/blog/tags/rbm/">RBM</a>
<a style="font-size: 93%" href="/blog/tags/speakerrecognition/">SpeakerRecognition</a>
<a style="font-size: 112%" href="/blog/tags/speech/">Speech</a>
<a style="font-size: 93%" href="/blog/tags/uml/">UML</a>
<a style="font-size: 93%" href="/blog/tags/vpr/">VPR</a>
<a style="font-size: 112%" href="/blog/tags/web/">Web</a>
<a style="font-size: 165%" href="/blog/tags/zju/">ZJU</a>

  </ul>
</section><section>
  <h2>Recent Comments</h2>
  <script type="text/javascript" src="http://ibillxia.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=0&avatar_size=32&excerpt_length=22"></script>
  <a href="http://disqus.com/">Powered by Disqus</a>
</section>

<section>
  <h2>Sina Weibo</h2>
  <ul id="weibo">
	<iframe 
		width="100%" 
		height="550" 
		class="share_self"  
		frameborder="0" 
		scrolling="no" 
		src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=2704795533&verifier=9551ab13&dpc=1">
	</iframe>
  </ul>
</section>

<section>
	<h2>Reading List</h2>
	<ul>
		<script type="text/javascript" src="http://www.douban.com/service/badge/65527470/?show=collection&amp;n=12&amp;columns=3" ></script>
	</ul>
</section>
<section>
  <h2>Copyleft</h2>
  <p align="center"><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png"></a></p>
  <p>Except where otherwise noted, content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a></p>
</section>
  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><p id = "back-top">
	<a href="#top"><span></span>Back to Top</a>
</p>
<hr>
<p>
  Copyright &copy; 2009 - 2014 - <a href="http://about.me/ibillxia">Bill Xia</a> -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> - Theme by <a href="http://twitter.github.com/bootstrap/">Twitter Bootstrap</a> </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ibillxia';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
