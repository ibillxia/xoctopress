
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Bill's Blog</title>
  <meta name="author" content="Bill Xia">

  
  <meta name="description" content="Yesterday is History, Tomorrow a Mystery, Today is a Gift, Thats why it's called the Present！">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://ibillxia.github.io/blog/page/4">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/style.css" media="screen, projection" rel="stylesheet" type="text/css">
  <style type="text/css">
    body {
      padding-bottom: 40px;
    }
    h1 {
      margin-bottom: 15px;
    }
    img {
      max-width: 100%;
    }
    .sharing, .meta, .pager {
      margin: 20px 0px 20px 0px;
    }
    .page-footer p {
      text-align: center;
    }
  </style>
  <script src="/javascripts/libs/jquery.js"></script>
  <script src="/javascripts/libs/modernizr-2.0.js"></script>
  <script src="/javascripts/libs/bootstrap.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Bill's Blog" type="application/atom+xml">
  <script type="text/javascript">
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}

$(document).bind('DOMNodeInserted', function(event) {
  addBlankTargetForLinks();
});
</script>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<script type="text/javascript">
$(document).ready(function(){

	// hide #back-top first
	$("#back-top").hide();
	
	// fade in #back-top
	$(function () {
		$(window).scroll(function () {
			if ($(this).scrollTop() > 100) {
				$('#back-top').fadeIn();
			} else {
				$('#back-top').fadeOut();
			}
		});

		// scroll body to 0px on click
		$('#back-top a').click(function () {
			$('body,html').animate({
				scrollTop: 0
			}, 800);
			return false;
		});
	});

});
</script>


  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39460228-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <nav role="navigation"><div class="navbar navbar-inverse">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">Bill's Blog</a>

      <div class="nav-collapse">
        <ul class="nav">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/tags">Tags</a></li>
  <li><a href="/about">About</a></li>
</ul>


        <ul class="nav pull-right" data-subscription="rss">
          <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
          
        </ul>

        
          <form class="pull-right navbar-search" action="http://www.google.com/" method="get">
            <fieldset role="search">
              <input type="hidden" name="q" value="site:ibillxia.github.io" />
              <input class="search-query" type="text" name="q" results="0" placeholder="Search"/>
            </fieldset>
          </form>
        
      </div>
    </div>
  </div>
</div>
</nav>
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
  
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/">深度学习及其在语音方面的应用</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-17T22:43:00+08:00" pubdate data-updated="true">Apr 17<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/17/Deep-Learning-and-its-application-in-audio-and-speech-processing/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>以下是今天在组会上讲的内容，与大家分享一下。有些地方我也没有完全理解，欢迎大家一起来讨论。</p>


<p><center>
<embed width="780"
    height="574"
    name="plugin"
    src="/upload/Deep Learning - Bill Xia.pdf"
    type="application/pdf"
/>
</center></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines/">基于能量的模型和波尔兹曼机</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-12T22:12:00+08:00" pubdate data-updated="true">Apr 12<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>由于深度置信网络（Deep Belief Networks，DBN）是基于限制性玻尔兹曼机（Restricted Boltzmann Machines，RBM）的深层网络结构，
所以本文重点讨论一下玻尔兹曼机（BM），以及它的学习算法——对比散度（Contrastive Divergence，CD）算法。在介绍BM前，我们首先介绍一下
基于能量的模型（Energy Based Model，EBM），因为BM是一种特殊的EBM。</p>




<h2>1. 基于能量的模型(EBM)</h2>


<p>基于能量的模型是一种具有普适意义的模型，可以说它是一种模型框架，在它的框架下囊括传统的判别模型和生成模型，图变换网络(Graph-transformer 
Networks)，条件随机场，最大化边界马尔科夫网络以及一些流形学习的方法等。EBM通过对变量的每个配置施加一个有范围限制的能量来捕获变量之间的依赖
关系。EBM有两个主要的任务，一个是推断(Inference)，它主要是在给定观察变量的情况，找到使能量值最小的那些隐变量的配置；另一个是学习(Learning)，
它主要是寻找一个恰当的能量函数，使得观察变量的能量比隐变量的能量低。</p>




<p>基于能量的概率模型通过一个能量函数来定义概率分布，
<center>$p(x) = \frac{e^{E(x)}}{Z}.$ &#8230; ①</center>
其中Z为规整因子，
<center>$Z = \sum _{x} e^{-E(x)}.$ &#8230; ②</center>
基于能量的模型可以利用使用梯度下降或随机梯度下降的方法来学习，具体而言，就是以训练集的负对数作为损失函数，
<center>$l(\theta,D) = -L(\theta,D) = - \frac{1}{N}\sum_{x^{(i)}\in D} log p(x^{(i)}).$ &#8230; ③</center>
其中$\theta$为模型的参数，将损失函数对$\theta$求偏导，
<center>$\Delta = \frac{\partial l(\theta,D)}{\partial \theta} = - \frac{1}{N} \frac{\partial \sum log p(x^{(i)})}{\partial \theta}.$ &#8230; ④</center>
即得到损失函数下降最快的方向。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/12/Energy-Based-Models-and-Boltzmann-Machines/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation/">2013IDF声龙语音识别技术演示</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-10T12:57:00+08:00" pubdate data-updated="true">Apr 10<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/10/Intel-Developer-Forum-2013-Nuance-Dragon-Presentation/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>2013英特尔信息技术峰会(Intel Developer Forum, IDF)上，来自Nuance的声龙语音合成和识别技术的演示，中文语音识别不给力，
笑点频出啊，哈哈</p>




<p><iframe height=560 width=780 src="http://player.youku.com/embed/XNTQwNjQ0MjUy" frameborder=0 allowfullscreen></iframe></p>



</div>
  
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/06/Convolutional-Neural-Networks/">卷积神经网络（CNN）</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-06T23:34:00+08:00" pubdate data-updated="true">Apr 6<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/06/Convolutional-Neural-Networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>1. 概述</h2>


<p>卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是<strong>非全连接</strong>的，
另一方面同一层中某些神经元之间的连接的<strong>权重是共享的</strong>（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物
神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。</p>




<p>卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他
形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野(receptive field)的概念，1984年日本学者Fukushima
基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。</p>




<p>神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有
位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima
将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面
得到了广泛的应用。</p>




<h2>2. CNN的结构</h2>


<p>卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。
这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：</br>
1 特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取<strong>局部特征</strong>。一旦一个特征被提取出来，
只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。</br>
2 特征映射。网络的每一个计算层都是由<strong>多个特征映射组</strong>成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下<strong>共享
相同的突触权值</strong>集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。</br>
3.子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他
形式的变形的敏感度下降的作用。</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/06/Convolutional-Neural-Networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/04/04/why-do-Fourier-transformation/">为什么要进行傅立叶变换</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-04T10:42:00+08:00" pubdate data-updated="true">Apr 4<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/04/04/why-do-Fourier-transformation/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>一、傅立叶变换的由来</h2>


<p>关于傅立叶变换，无论是书本还是在网上可以很容易找到关于傅立叶变换的描述，但是大都是些故弄玄虚的文章，太过抽象，
尽是一些让人看了就望而生畏的公式的罗列，让人很难能够从感性上得到理解，最近，我偶尔从网上看到一个关于数字信号处理
的电子书籍，是一个叫Steven W. Smith, Ph.D.外国人写的，写得非常浅显，里面有七章由浅入深地专门讲述关于离散信号的傅
立叶变换，虽然是英文文档，我还是硬着头皮看完了有关傅立叶变换的有关内容，看了有茅塞顿开的感觉，在此把我从中得到的
理解拿出来跟大家分享，希望很多被傅立叶变换迷惑的朋友能够得到一点启发，这电子书籍是免费的，有兴趣的朋友也可以从网
上下载下来看一下，URL地址是：http://www.dspguide.com/pdfbook.htm </p>




<p>要理解傅立叶变换，确实需要一定的耐心，别一下子想着傅立叶变换是怎么变换的，当然，也需要一定的高等数学基础，最基本
的是级数变换，其中傅立叶级数变换是傅立叶变换的基础公式。</p>




<h2>二、傅立叶变换的提出</h2>


<p>让我们先看看为什么会有傅立叶变换？傅立叶是一位法国数学家和物理学家的名字，英语原名是Jean Baptiste Joseph Fourier(1768-1830), 
Fourier对热传递很感兴趣，于1807年在法国科学学会上发表了一篇论文，运用正弦曲线来描述温度分布，论文里有个在当时具有争议性的决断：
任何连续周期信号可以由一组适当的正弦曲线组合而成。当时审查这个论文的人，其中有两位是历史上著名的数学家拉格朗日(Joseph Louis 
Lagrange, 1736-1813)和拉普拉斯(Pierre Simon de Laplace, 1749-1827)，当拉普拉斯和其它审查者投票通过并要发表这个论文时，拉格朗日
坚决反对，在近50年的时间里，拉格朗日坚持认为傅立叶的方法无法表示带有棱角的信号，如在方波中出现非连续变化斜率。法国科学学会屈服
于拉格朗日的威望，拒绝了傅立叶的工作，幸运的是，傅立叶还有其它事情可忙，他参加了政治运动，随拿破仑远征埃及，法国大革命后因会被
推上断头台而一直在逃避。直到拉格朗日死后15年这个论文才被发表出来。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/04/why-do-Fourier-transformation/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/30/back-propagation-neural-networks/">反向传播(BP)神经网络</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-30T21:37:00+08:00" pubdate data-updated="true">Mar 30<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/30/back-propagation-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>前面几篇文章中对神经网络和深度学习进行一些简介，包括神经网络的发展历史、基本概念和常见的几种神经网络以及神经网络的学习方法等，
本文具体来介绍一下一种非常常见的神经网络模型——反向传播(Back Propagation)神经网络。</p>




<h2>1.概述</h2>


<p>BP（Back Propagation）神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文
<em><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Learning representations by back-propagating errors</a></em>
值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。
</p>




<p>BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的
输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断
调整网络的权值和阈值，使网络的误差平方和最小。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/30/back-propagation-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/27/learning-process-of-neural-networks/">神经网络的学习方法概述</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-27T23:51:00+08:00" pubdate data-updated="true">Mar 27<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/27/learning-process-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要讨论一下神经网络的一般学习方法，主要有error-correction learning，memory-based learning， Hebbian learning，competitive learning，
Boltzmann learning等。然后介绍一些学习的方式，如监督学习、非监督学习、强化学习等。最后是一些具体的应用领域和实际问题。</p>




<h2>1.Knowledge Representation</h2>


<p>好的学习方法固然重要，但知识的表示，直接影响到feature的表示，也是非常重要的，因此在正式讨论学习方法之前，我们首先谈谈知识的表示。
首先一个问题是，什么是知识？在PRML中我们有如下定义：</br>
<blockquote><p>Knowledge refers to stored information or models used by a person or machine to interpret, predict, and appropriately respond to the outside world.</p><footer><strong>Fischler and Firschein</strong> <cite>Intelligence: The Eye，the Brain and the Computer</cite></footer></blockquote>
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/27/learning-process-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/24/classes-of-neural-networks/">神经网络模型分类</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-24T23:07:00+08:00" pubdate data-updated="true">Mar 24<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/24/classes-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>本文主要介绍一下几种不同类型的神经网络模型，主要有前馈神经网络，反馈神经网络，自组织神经网络，随机神经网络</p>




<h2>1.前馈神经网络</h2>


<h4>1)自适应线性神经网络(Adaline)</h4>


<p>自适应线性神经网络（Adaptive Linear，简称Adaline) 是由威德罗（Widrow）和霍夫（Hoff）首先提出的。它与感知器的主要不同之处在于
其神经元有一个线性激活函数，这允许输出可以是任意值，而不仅仅只是像感知器中那样只能取0或1。它采用的是W—H学习法则，也称最小均方差(LMS)
规则对权值进行训练。自适应线性元件的主要用途是线性逼近一个函数式而进行模式联想。</p>




<h4>2)单层感知器</h4>


<p>单层感知器（Perceptron）是由美国计算机科学家罗森布拉特（F.Roseblatt）于1957年提出的。它是一个具有单层神经元的网络，由线性阈值
逻辑单元所组成。它的输入可以是非离散量，而且可以通过学习而得到，这使单层感知器在神经网络研究中有着重要的意义和地位：它提出了自组织、
自学习的思想，对能够解决的问题，有一个收敛的算法，并从数学上给出了严格的证明。</p>


</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/24/classes-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/20/basics-of-neural-networks/">神经网络简介</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-20T23:21:00+08:00" pubdate data-updated="true">Mar 20<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/20/basics-of-neural-networks/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Deep Learning的本质是多层的神经网络，因此在深入学习Deep Learning之前，有必要了解一些神经网络的基本知识。
本文首先对神经网络的发展历史进行简要的介绍，然后给出神经元模型的形式化描述，接着是神经网络模型的定义、特性，
最后是一些最新的进展等。关于神经网络的分类、学习方法、应用场景等将在后续文章中介绍。</p>




<h2>1.发展简史</h2>


<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts建立了神经网络和数学模型，称为MP模型。他们通过MP模型提出
了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。</br>
1945年，Von Neumann在成功的试制了存储程序式电子计算机后，他也对人脑的结构与存储式计算机进行的根本区别的比较，还提出了以简单神经元构成的自再生自动机网络结构。</br>
1949年，心理学家D.O.Heb提出了突触联系强度可变的设想，并据此提出神经元的学习准则——Hebb规则，为神经网络的学习算法奠定了基础。</br>
1958年，F.Rosenblatt提出了感知模型，该模型是由阈值神经元组成的，它试图模拟动物和人的感知和学习能力。</br>
1962年Widrow提出了自适应线性元件，这是一种连续的取值的线性网络，主要用于自适应信号处理和自适应控制。</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/20/basics-of-neural-networks/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header class="page-header">
    
      <h1 class="entry-title"><a href="/blog/2013/03/16/introduction-to-deep-learning/">深度学习简介</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-03-16T21:36:00+08:00" pubdate data-updated="true">Mar 16<span>th</span>, 2013</time>
        
         | <a href="/blog/2013/03/16/introduction-to-deep-learning/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>0.概述</h2>

<p>
以下是Wiki上对深度学习的下的定义：</br>
Deep learning refers to a sub-field of machine learning that is based on learning several levels of representations, 
corresponding to a hierarchy of features or factors or concepts, where higher-level concepts are defined from lower-level ones, 
and the same lower-level concepts can help to define many higher-level concepts.
</p>


<p>
深度学习就是学习多个级别的表示和抽象，帮助理解数据，如图像、声音和文本。深度学习的概念源于人工神经网络的研究，
含多隐层的多层感知器就是一种深度学习结构。那些涉及从输入产生输出的计算,我们可以用流程图来表示，
流程图的一个特殊的概念就是它的深度: 从输入到输出的路径的最长长度。传统的前馈神经网络可以理解为
深度等于层数(隐层数+1)的网络。深度学习通过组合低层特征形成更加抽象的高层表示（属性类别或特征），
以发现数据的分布式特征表示。
</p>


<h2>1.深度学习产生的背景</h2>

<h3>1.1深度不够的缺陷</h3>

<p>
在很多情况下，深度为2就已足以在给定精度范围内表示任何函数了，例如逻辑门、正常
神经元、sigmoid-神经元、SVM中的RBF(Radial Basis Function)等，但这样也有一个代价：
那就是图中需要的节点数会很多，这也就意味着当我们学习目标函数时，需要更多的计算
单元和更多的参数。理论结果显示，对于某一类函数，需要的参数的个数与输入的大小是
成指数关系的，逻辑门、正常神经元、RBF单元就属于这类。后来Hastad发现，当深度为d时，
这类函数可以用O(n)个节点（输入为n个）的神经网络有效表示，但当深度被限制为d-1时，
则需要有O(n2)个节点来表示。
</p>




</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/03/16/introduction-to-deep-learning/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/5/">&larr; Older</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/3/">Newer &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span3">
  
    <section>
  <h2>Categories</h2>
    <ul id="category-list">
		<li><a href='/blog/categories/assp'>ASSP (15)</a></li><li><a href='/blog/categories/engineering'>Engineering (4)</a></li><li><a href='/blog/categories/intelligence'>Intelligence (4)</a></li><li><a href='/blog/categories/life'>Life (11)</a></li><li><a href='/blog/categories/linux'>Linux (3)</a></li><li><a href='/blog/categories/math'>Math (5)</a></li><li><a href='/blog/categories/prml'>PRML (12)</a></li><li><a href='/blog/categories/program'>Program (33)</a></li><li><a href='/blog/categories/technics'>Technics (7)</a></li><li><a href='/blog/categories/view'>View (7)</a></li>
	</ul>
</section>
<section>
  <h2>Tags</h2>
  <ul class="tag-cloud">
	<a style="font-size: 137%" href="/blog/tags/audio/">Audio</a>
<a style="font-size: 93%" href="/blog/tags/bpnn/">BPNN</a>
<a style="font-size: 159%" href="/blog/tags/c/">C</a>
<a style="font-size: 93%" href="/blog/tags/convex/">Convex</a>
<a style="font-size: 112%" href="/blog/tags/deeplearning/">DeepLearning</a>
<a style="font-size: 93%" href="/blog/tags/htk/">HTK</a>
<a style="font-size: 146%" href="/blog/tags/life/">Life</a>
<a style="font-size: 93%" href="/blog/tags/love/">Love</a>
<a style="font-size: 93%" href="/blog/tags/machinelearning/">MachineLearning</a>
<a style="font-size: 159%" href="/blog/tags/neuralnetworks/">NeuralNetworks</a>
<a style="font-size: 93%" href="/blog/tags/nuance/">Nuance</a>
<a style="font-size: 93%" href="/blog/tags/optimization/">Optimization</a>
<a style="font-size: 153%" href="/blog/tags/php/">PHP</a>
<a style="font-size: 93%" href="/blog/tags/perceptron/">Perceptron</a>
<a style="font-size: 112%" href="/blog/tags/prim/">Prim</a>
<a style="font-size: 153%" href="/blog/tags/programtest/">ProgramTest</a>
<a style="font-size: 93%" href="/blog/tags/programing/">Programing</a>
<a style="font-size: 146%" href="/blog/tags/python/">Python</a>
<a style="font-size: 126%" href="/blog/tags/quicksort/">QuickSort</a>
<a style="font-size: 93%" href="/blog/tags/rbm/">RBM</a>
<a style="font-size: 112%" href="/blog/tags/speech/">Speech</a>
<a style="font-size: 93%" href="/blog/tags/uml/">UML</a>
<a style="font-size: 93%" href="/blog/tags/vpr/">VPR</a>
<a style="font-size: 112%" href="/blog/tags/web/">Web</a>
<a style="font-size: 165%" href="/blog/tags/zju/">ZJU</a>

  </ul>
</section><section>
  <h2>Recent Comments</h2>
  <script type="text/javascript" src="http://ibillxia.disqus.com/recent_comments_widget.js?num_items=5&hide_avatars=0&avatar_size=32&excerpt_length=22"></script>
  <a href="http://disqus.com/">Powered by Disqus</a>
</section>

<section>
  <h2>Sina Weibo</h2>
  <ul id="weibo">
	<iframe 
		width="100%" 
		height="550" 
		class="share_self"  
		frameborder="0" 
		scrolling="no" 
		src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=0&skin=1&isTitle=1&noborder=1&isWeibo=1&isFans=1&uid=2704795533&verifier=9551ab13&dpc=1">
	</iframe>
  </ul>
</section>

<section>
	<h2>Reading List</h2>
	<ul>
		<script type="text/javascript" src="http://www.douban.com/service/badge/65527470/?show=collection&amp;n=12&amp;columns=3" ></script>
	</ul>
</section>
<section>
  <h2>Copyleft</h2>
  <p align="center"><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png"></a></p>
  <p>Except where otherwise noted, content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_US">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License</a></p>
</section>
  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer"><p id = "back-top">
	<a href="#top"><span></span>Back to Top</a>
</p>
<hr>
<p>
  Copyright &copy; 2009 - 2014 - <a href="http://about.me/ibillxia">Bill Xia</a> -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> - Theme by <a href="http://twitter.github.com/bootstrap/">Twitter Bootstrap</a> </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ibillxia';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
